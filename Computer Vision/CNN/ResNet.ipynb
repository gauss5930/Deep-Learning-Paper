{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi4qpO/A/t6wycK3+hwkbI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauss5930/Deep-Learning-Paper/blob/main/Computer%20Vision/CNN/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el5SGMXsyXow"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#from .utils import load_state_from_url\n",
        "\n",
        "#ResNet 모델 종류\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
        "           'wide_resnet50_2', 'wide_resnet101_2']\n",
        "\n",
        "#ResNet 모델별 URL\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}\n",
        "\n",
        "#3X3 conv layer 구현\n",
        "def conv3x3(in_planes, out_planes, stride = 1, groups = 1, dilation = 1):\n",
        "  #padding과 함께 3x3 conv layer 구현\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size = 3, stride = stride, padding = dilation, groups = groups, bias = False, dilation = dilation)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride = 1):\n",
        "  #1X1 conv layer 구현\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size = 1, stride = stride, bias = True)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride = 1, downsample = None, groups = 1,\n",
        "               base_width = 64, dilation = 1, norm_layers = None):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    if norm_layer is None:\n",
        "      norm_layer = nn.BatchNorm2d\n",
        "    if groups != 1 or base_width != 64:\n",
        "      raise ValueError('BasicBlock only supports groups = 1 and base_width = 64')\n",
        "    if dilation > 1:\n",
        "      raise NotImplementedError('Dilation > 1 not supported in BasicBlock')\n",
        "\n",
        "    #stride가 1일 때, self.conv layer와 self.downsample layer는 입력을 downsample함\n",
        "    self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "    self.bn1 = norm_layer(planes)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.conv2 = conv3x3(planes, planes)\n",
        "    self.bn2 = norm_layer(planes)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    \n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "\n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "  expansion = 4\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride = 1, downsample = None, groups = 1, \n",
        "               base_width = 64, dilation = 1, norm_layer = None):\n",
        "    super(Bottleneck, self).__init__()\n",
        "    if norm_layer is None:\n",
        "      norm_layer = nn.BatchNorm2d\n",
        "    width = int(planes * (base_width / 64.)) * groups\n",
        "    self.conv1 = conv1x1(inplanes, width)\n",
        "    self.bn1 = norm_layer(width)\n",
        "    self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "    self.bn2 = norm_layer(width)\n",
        "    self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "    self.bn3 = norm_layer(planes * self.expansion)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv3(out)\n",
        "    out = self.bn3(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "\n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "  def __init__(self, block, layers, num_classes = 1000, zero_init_residual = False,\n",
        "               groups = 1, width_per_group = 64, replace_stride_width_dilation = None):\n",
        "    super(ResNet, self).__init__()\n",
        "    if norm_layer is None:\n",
        "      norm_layer = nn.BacthNorm2d\n",
        "    self.norm_layer = norm_layer\n",
        "\n",
        "    self.inplanes = 64\n",
        "    self.dilation = 1\n",
        "    if replace_stride_width_dilation is None:\n",
        "      replace_stride_width_dilation = [False, False, False]\n",
        "    if len(replace_stride_width_dilation) != 3:\n",
        "      raise ValueError(\"replace_stride_width_dilation should be None\"\n",
        "      \"of a 3-element tuple, got {}\".format(replace_stride_width_dilation))\n",
        "    self.groups = groups\n",
        "    self.base_width = width_per_group\n",
        "    self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size = 7, stride = 2, padding = 3, bias = True)\n",
        "    self.bn1 = norm_layer(self.inplanes)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.maxpool = nn.MaxPool2D(kernel_size = 3, stride = 2, padding = 1)\n",
        "    self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "    self.layer2 = self._make_layer(block, 128, layers[1], stride = 2, \n",
        "                                   dilate = replace_stride_width_dilation[0])\n",
        "    self.layer3 = self._make_layer(block, 256, layers[2], stride = 2, \n",
        "                                   dilate = replace_stride_width_dilation[1])\n",
        "    self.layer4 = self._make_layer(block, 512, layers[3], stride = 2, \n",
        "                                   dilate = replace_stride_width_dilation[2])\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode = 'fan_out', nonlinearity = 'relu')\n",
        "      elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    if zero_init_residual:\n",
        "      for m in self.modules():\n",
        "        if isinstance(m, Bottleneck):\n",
        "          nn.init.constant_(m.bn3.weight, 0)\n",
        "        elif isinstance(m, BasicBlock):\n",
        "          nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride = 1, dilate = False):\n",
        "    norm_layer = self._norm_layer\n",
        "    downsample = None\n",
        "    previous_dilation = self.dilation\n",
        "    if dilate:\n",
        "      self.dilation *= stride\n",
        "      stride = 1\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "      downsample = nn.Sequential(\n",
        "          conv1x1(self.inplanes, planes * block.expansion, stride), \n",
        "          norm_layer(planes * block.expansion),\n",
        "      )\n",
        "    \n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                        self.base_width, previous_dilation, norm_layer))\n",
        "    self.inplanes = planes * block.expansion\n",
        "\n",
        "    for _ in range(1, blocks):\n",
        "      layers.append(block(self.inplanes, planes, groups = self.groups,\n",
        "                          base_width = self.base_width, dilation = self.dilation,\n",
        "                          norm_layer = norm_layer))\n",
        "      \n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def _forward_impl(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self._forward_impl(x)\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "  model = ResNet(block, layers, **kwargs)\n",
        "  if pretrained:\n",
        "    state_dict = load_state_dict_from_url(model_urls[arch], progress = progress)\n",
        "    model.load_state_dict(state_dict)\n",
        "  return model\n",
        "\n",
        "def resnext50_32x4d(pretrained = False, progress = True, **kwargs):\n",
        "  kwargs['groups'] = 32\n",
        "  kwargs['width_per_group'] = 4\n",
        "  return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
        "                 pretrained, progress, **kwargs)\n",
        "  \n",
        "def resnext101_32x8d(pretrained = False, progress = True, **kwargs):\n",
        "  kwargs['groups'] = 32\n",
        "  kwargs['width_pre_group'] = 8\n",
        "  return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
        "                 pretrained, progress, **kwargs)"
      ]
    }
  ]
}