{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtfpaygJNZbBpUa0WxvN2p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauss5930/Deep-Learning-Paper/blob/main/Computer%20Vision/CNN/GoogLeNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f7qWOK7AQC5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.jit.annotations import Optional, Tuple\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "__all__ = ['GoogLeNet', 'googlenet', 'GoogLeNetOutputs', '_GoogLeNetOutputs']\n",
        "\n",
        "model_urls = {'googlenet': 'https://download.pytorch.org/models/googlenet-1378be20.pth'}\n",
        "\n",
        "GoogLeNetOutputs = namedtuple('GoogLeNetOutputs', ['logits', 'aux_logits2', 'aux_logits1'])\n",
        "GoogLeNetOutputs.__annotations__ = {'logits': Tensor, 'aux_logits2': Optional[Tensor], \n",
        "                                    'aux_logits1': Optional[Tensor]}\n",
        "\n",
        "#역전파를 위한 GoogLeNet outputs 설정\n",
        "_GoogLeNetOutputs = GoogLeNetOutputs\n",
        "\n",
        "\n",
        "def googlenet(pretrained = False, progress = True, **kwargs):\n",
        "  #pretraind: True면 ImageNet으로 pretrained된 모델 반환\n",
        "  #progress: True면 download bar 보여주기\n",
        "  #aux_logits: True면 두 개의 추가적인 branch 더해줌 --> 성능 향상에 도움 됌\n",
        "  #transform input: True면 입력을 preprocessing\n",
        "\n",
        "  if pretrained:\n",
        "    if 'transform_input' not in kwargs:\n",
        "      kwargs['transform_input'] = True\n",
        "    if 'aux_logits' not in kwargs:\n",
        "      kwargs['aux_logits'] = False\n",
        "    if kwargs['aux_logits']:\n",
        "      warnings.warn('auxiliary heads in the pretrained googlenet model are NOT pretrained, ')\n",
        "\n",
        "    original_aux_logits = kwargs['aux_logits']\n",
        "    kwargs['aux_logits'] = True\n",
        "    kwargs['init_weights'] = False\n",
        "    model = GoogLeNet(**kwargs)\n",
        "    state_dict = load_state_dict_from_url(model_urls['googlenet'], progress = progress)\n",
        "    model.load_state_dict(state_dict)\n",
        "    if not original_aux_logits:\n",
        "      model.aux_logits = False\n",
        "      model.aux1 = None\n",
        "      model.aux2 = None\n",
        "    return model\n",
        "\n",
        "  return GoogLeNet(**kwargs)\n",
        "\n",
        "class GoogLeNet(nn.Module):\n",
        "  __constants__ = ['aux_logits', 'transform_input']\n",
        "\n",
        "  def __init__(self, num_classes = 1000, aux_logits = True, transform_input = False,\n",
        "               init_weights = None, blocks = None):\n",
        "    super(GoogLeNet, self).__init__()\n",
        "    if blocks is None:\n",
        "      blocks = [BasicConv2d, Inception, InceptionAux]\n",
        "    if init_weights is None:\n",
        "      warnings.warn('The default weight initialization of GoogLeNet will be changed in future releases of')\n",
        "      init_weights = True\n",
        "    assert len(blocks) == 3\n",
        "    conv_block = blocks[0]\n",
        "    inception_block = blocks[1]\n",
        "    inception_aux_block = blocks[2]\n",
        "\n",
        "    self.aux_logits = aux_logits\n",
        "    self.transform_input = transform_input\n",
        "\n",
        "    self.conv1 = conv_block(3, 64, kernel_size = 7, stride = 2, padding = 3)\n",
        "    self.maxpool1 = nn.MaxPool2d(3, stride = 2, ceil_mode = True)\n",
        "    self.conv2 = conv_block(64, 64, kernel_size = 1)\n",
        "    self.conv3 = conv_block(64, 192, kernel_size = 3, padding = 1)\n",
        "    self.maxpool2 = nn.MaxPool2d(3, stride = 2, ceil_mode = True)\n",
        "\n",
        "    self.inception3a = inception_block(192, 64, 96, 128, 16, 32, 32)\n",
        "    self.inception3b = inception_block(256, 128, 128, 192, 32, 96, 64)\n",
        "    self.maxpool3 = nn.MaxPool2d(3, stride = 2, ceil_mode = True)\n",
        "\n",
        "    self.inception4a = inception_block(480, 192, 96, 208, 16, 48, 64)\n",
        "    self.inception4b = inception_block(512, 160, 112, 224, 24, 64, 64)\n",
        "    self.inception4c = inception_block(512, 128, 127, 256, 24, 64, 64)\n",
        "    self.inception4d = inception_block(512, 112, 144, 288, 32, 64, 64)\n",
        "    self.inception4e = inception_block(528, 256, 160, 320, 32, 128, 128)\n",
        "    self.maxpool4 = nn.MaxPool2d(2, stride = 2, ceil_mode = True)\n",
        "\n",
        "    self.inception5a = inception_block(832, 256, 160, 320, 32, 128, 128)\n",
        "    self.inception5b = inception_block(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "    if aux_logits:\n",
        "      self.aux1 = inception_aux_block(512, num_classes)\n",
        "      self.aux2 = inception_aux_block(528, num_classes)\n",
        "    else:\n",
        "      self.aux1 = None\n",
        "      self.aux2 = None\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    if init_weights:\n",
        "      self._initialize_weights()\n",
        "\n",
        "  def _initialize_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        import scipy.stats as stats\n",
        "        X = stats.truncnorm(-2, 2, scale = 0.01)\n",
        "        values = torch.as_tensor(X.rvs(m.weight.numel()), dtype = m.weight.dtype)\n",
        "        values = values.view(m.weight.size())\n",
        "        with torch.no_grad():\n",
        "          m.weight.copy_(values)\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  def _transform_input(self, x):\n",
        "    #(Tensor) --> Tensor\n",
        "    if self.transform_input:\n",
        "      x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
        "      x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
        "      x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
        "      x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
        "    return x\n",
        "\n",
        "  def _forward(self, x):\n",
        "    #type: (Tensor) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]\n",
        "    #N x 3 x 224 x 224\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    #N x 64 x 112 x 112\n",
        "    x = self.maxpool1(x)\n",
        "\n",
        "    #N x 64 x 56 x 56\n",
        "    x = self.conv2(x)\n",
        "\n",
        "    #N x 64 x 56 x 56\n",
        "    x = self.conv3(x)\n",
        "\n",
        "    #N x 192 x 56 x 56\n",
        "    x = self.maxpool2(x)\n",
        "\n",
        "    #N x 192 x 28 x 28\n",
        "    x = self.inception3a(x)\n",
        "\n",
        "    #N x 256 x 28 x 28\n",
        "    x = self.inception3b(x)\n",
        "\n",
        "    #N x 480 x 28 x 28\n",
        "    x = self.maxpool3(x)\n",
        "\n",
        "    #N x 480 x 14 x 14\n",
        "    x = self.inception4a(x)\n",
        "\n",
        "    # N x 512 x 14 x 14\n",
        "    aux1 = torch.hit.annotate(Optional[Tensor], None)\n",
        "    if self.aux1 is not None:\n",
        "      if self.training:\n",
        "        aux1 = self.aux1(x)\n",
        "\n",
        "    x = self.inception4b(x)\n",
        "\n",
        "    #N x 512 x 14 x 14\n",
        "    x = self.inception4c(x)\n",
        "\n",
        "    #N x 512 x 14 x 14\n",
        "    x = self.inception4d(x)\n",
        "\n",
        "    #N x 528 x 14 x 14\n",
        "    x = self.inception4e(x)\n",
        "\n",
        "    #N x 832 x 14 x 14\n",
        "    x = self.maxpool4(x)\n",
        "\n",
        "    #N x 832 x 7 x 7\n",
        "    x = self.inception5a(x)\n",
        "\n",
        "    #N x 832 x 7 x 7\n",
        "    x = self.inception5b(x)\n",
        "    #N x 1024 x 7 x 7\n",
        "\n",
        "    x = self.avgpool(x)\n",
        "    #N x 1024 x 1 x 1\n",
        "\n",
        "    x = torch.flatten(x, 1)\n",
        "    # N x 1024\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc(x)\n",
        "    #N x 1000 (num_classes)\n",
        "    return x, aux2, aux1\n",
        "  \n",
        "  @torch.jit.unused\n",
        "  def eager_outputs(self, x, aux2, aux1):\n",
        "    # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> GoogLeNetOutputs\n",
        "    if self.training and self.aux_logits:\n",
        "      return _GoogLeNetOutputs(x, aux2, aux1)\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "  def forward(self, x):\n",
        "    # type: (Tensor) -> GoogLeNetOutputs\n",
        "    x = self._transform_input(x)\n",
        "    x, aux1, aux2 = self._forward(x)\n",
        "    aux_defined = self.training and self.aux_logits\n",
        "    if torch.jit.is_scripting():\n",
        "      if not aux_defined:\n",
        "        warnings.warn('Scripted Googlenet alwatd returns GoogleNetOutputs Tuple')\n",
        "      return GoogLeNetOutputs(x, aux2, aux1)\n",
        "    else:\n",
        "      return self.eager_outputs(x, aux2, aux1)\n",
        "\n",
        "class Inception(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj,\n",
        "               conv_block = None):\n",
        "    super(Inception, self).__init__()\n",
        "    if conv_block is None:\n",
        "      conv_block = BasicConv2d\n",
        "    self.branch1 = conv_block(in_channels, ch1x1, kernel_size = 1)\n",
        "\n",
        "    self.branch2 = nn.Sequential(\n",
        "        conv_block(in_channels, ch3x3red, kernel_size = 1),\n",
        "        conv_block(ch3x3red, ch3x3, kernel_size = 3, padding = 1)\n",
        "    )\n",
        "\n",
        "    self.branch3 = nn.Sequential(\n",
        "        conv_block(in_channels, ch5x5red, kernel_size = 1),\n",
        "        conv_block(ch5x5red, ch5x5, kernel_size = 3, padding = 1)\n",
        "    )\n",
        "\n",
        "    self.branch4 = nn.Sequential(\n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1, ceil_mode = True),\n",
        "        conv_block(in_channels, pool_proj, kernel_size = 1)\n",
        "    )\n",
        "\n",
        "  def _forward(self, x):\n",
        "    branch1 = self.branch1(x)\n",
        "    branch2 = self.branch2(x)\n",
        "    branch3 = self.branch3(x)\n",
        "    branch4 = self.branch4(x)\n",
        "\n",
        "    outputs = [branch1, branch2, branch3, branch4]\n",
        "    return outputs\n",
        "\n",
        "  def forward(self, x):\n",
        "    outputs = self._forward(x)\n",
        "    return torch.cat(outputs, 1)\n",
        "\n",
        "class InceptionAux(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, num_classes, conv_block = None):\n",
        "    super(InceptionAux, self).__init__()\n",
        "    if conv_block is None:\n",
        "      conv_block = BasicConv2d\n",
        "    self.conv = conv_block(in_channels, 128, kernel_size = 1)\n",
        "\n",
        "    self.fc1 = nn.Linear(2048, 1024)\n",
        "    self.fc2 = nn.Linear(1024, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n",
        "    x = F.adaptive_avg_pool2d(x, (4, 4))\n",
        "    \n",
        "    #aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n",
        "    x = self.conv(x)\n",
        "\n",
        "    #N x 128 x 4 x 4\n",
        "    x = self.torch.flatten(x, 1)\n",
        "\n",
        "    #N x 2048\n",
        "    x = F.relu(self.fc1(x), inplace = True)\n",
        "\n",
        "    #N x 1024\n",
        "    x = F.dropout(x, 0.7, training = self.training)\n",
        "\n",
        "    #N x 1024\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    # N x 1000 (num_classes)\n",
        "\n",
        "    return x\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, **kwargs):\n",
        "    super(BasicConv2d, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, bias = False, **kwargs)\n",
        "    self.bn = nn.BatchNorm2d(out_channels, eps = 0.001)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.bn(x)\n",
        "    return F.relu(x, inplace = True)"
      ]
    }
  ]
}