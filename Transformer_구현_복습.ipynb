{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAI+7BBqaBGWgNGN/Zmk39",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauss5930/Natural-Language-Processing/blob/main/Transformer_%EA%B5%AC%ED%98%84_%EB%B3%B5%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 간단한 Transformer model 구현\n",
        "\n",
        "Transformer의 기본 동작 원리만을 구현한 것"
      ],
      "metadata": {
        "id": "K1Jq9j1brnno"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "abaFN4aNoU1K",
        "outputId": "2246b0a9-d392-4284-d0a8-1aaaf8c5e8c0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b539498e6498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.nn.function'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.function as F   # old version의 Ptyorch에 있는 함수라 선언 불가가\n",
        "\n",
        "import copy\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "  def decode(self, tgt, encoder_out, tgt_mask, src_tgt_mask):\n",
        "    return self.decoder(self.tgt_embed(tgt), encoder_out, tgt_mask, src_tgt_mask)\n",
        "\n",
        "  def forward(self, src, tgt, src_mask):\n",
        "    # (추가) mask\n",
        "    # (추가) generator\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    tgt_mask = self.make_tgt_mask(tgt)\n",
        "    src_tgt_mask = self.make_src_tgt_mask(src, tgt)\n",
        "    encoder_out = self.encode(src, src_mask)\n",
        "    decoder_out = self.decode(tgt, encoder_out, tgt_mask, src_tgt_mask)\n",
        "    out = self.generator(decoder_out)\n",
        "    out = F.log_softmax(out, dim = -1)\n",
        "    return out, decoder_out\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "    pad_mask = self.make_pad_mask(src, src)\n",
        "    return pad_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder block 구현\n",
        "Encoder는 sentence에서 context를 파악하는 역할. 반복되는 Encoder block을 통해 이전의 Encoder block의 출력을 다음의 Encoder block이 사용하게 되면서, 더욱 세밀하게 context 파악. "
      ],
      "metadata": {
        "id": "fIB_3b-qrtrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder_block, n_layer):   # n_layer: 블록의 갯수\n",
        "    super(Encoder, self).__init__()\n",
        "    self.layers = []\n",
        "    for i in range(n_layer):\n",
        "      self.layers.append(copy.deepcopy(encoder_block))\n",
        "\n",
        "  def forward(self, src, src_mask):   # 이전 Encoder block의 output을 현재 Encoder block의 input으로 사용용\n",
        "    out = src\n",
        "    for layer in self.layers:\n",
        "      out = layer(out, src_mask)\n",
        "    return out"
      ],
      "metadata": {
        "id": "prmFrU_3qvQM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder block은 크게 Multi-Head Attention Layer와 Position-wise Feed-FOrward Layer로 구성된다."
      ],
      "metadata": {
        "id": "QGr1VP5Hs5uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, self_attention, position_ff):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "    self.self_attention = self_attention   # Multi-Head Attention\n",
        "    self.position_ff = position_ff         # Position-wise Feed-Forward\n",
        "    self.residuals = [ResidualConnectionLayer() for _ in range(2)]   # (추가) Reisdual connection\n",
        "\n",
        "  def forward(self, src, src_mask):\n",
        "    out = src\n",
        "    out = self.residuals[0](out, lambda out: self.self_attention(query = out, key = out, value = out, mask = src_mask))\n",
        "    out = self.residuals[1](out, self.position_ff(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "pbSrmG0Js5Hh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention이 무엇일까?\n",
        "\n",
        "Multi-Head Attention은 Scaled DOt-Product Attention을 병렬적으로 수행하는 layer이다. 이렇게만 설명하면 이해를 하기 힘든데, 우선 Attention은 넓은 범위의 전체 data에서 특정한 부분에 집중한다는 의미이다. 그리고 이 Scaled-Dot Product Attention을 줄여서 Attention이라고 부르기도 한다. 이러한 Attention 중에서도 **같은 문장 내**의 두 토큰 사이의 Attention을 계산하는 방법을 *Self-Attention*이라고 한다.\n",
        "\n",
        "### Query, Key, Value\n",
        "\n",
        "이제 Attention이 어떤 방식으로 작동하는지 살펴보자. Attention 계산에는 Query, Key, Value 이렇게 3가지의 벡터가 사용되는데, 각 벡터의 역할은 다음과 같다.\n",
        "\n",
        "1. Query: 현 시점의 token\n",
        "2. Key: attention 값을 구하고자 하는 대상 token\n",
        "3. Value: attention 값을 구하고자 하는 대상 token (Key와 동일한 token)\n",
        "\n",
        "Key와 Value는 완전히 동일한 값을 가지는데, 이 둘은 문장의 처음부터 끝까지 탐색한다. Query는 고정된 하나의 값을 가지는데, Query와 가장 부합하는 token을 찾기 위해 Key, Value를 문장의 처음부터 끝까지 탐색시키는 것이다.\n",
        "\n",
        "### Scaled Dot-Product Attention\n",
        "\n",
        "이 Scaled Dot-Product Attention의 작동 방식에 대한 자세한 내용은 블로그를 참고하길 바란다. https://cartinoe5930.tistory.com/entry/Transformer-Attention-Is-All-You-Need-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2v4k-QDtu_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이러한 Self-Attention을 pytorch code로 구현하면 다음과 같다. "
      ],
      "metadata": {
        "id": "YPghlYOh0_56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_attention(query, key, value, mask):\n",
        "  # query, key, value: (n_batch, seq_len, d_k)\n",
        "  # mask: (n_batch, seq_len, seq_len)\n",
        "  d_k = key.shape[-1]\n",
        "  attention_score = torch.matmul(query, key.transpose(-2, -1)) # Q x K^T, (n_batch, seq_len, seq_len)\n",
        "  attention_score = attention_score / math.sqrt(d_k) # Scale\n",
        "  if mask is not None: # Mask(Opt.)\n",
        "    attention_score = attention_score.masked_fill(mask == 0, -1e9)\n",
        "  attention_prob = F.softmax(attention_score, dim = -1) # SoftMax (n_batch, seq_len, seq_len)\n",
        "  out = torch.matmul(attention_prob, value) # last MatMul\n",
        "  return out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "EVJERV7TtnPu",
        "outputId": "9f620c72-822a-4ae4-c30d-628ffc6c2190"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c139f23ffb1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# query, key, value: (n_batch, seq_len, d_k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.nn.function'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "지금까지의 Self-Attention 개념은 모두 Multi-Head Attention을 이해하기 위한 것이었다. Transformer의 Encoder layer에서 Scaled Dot-Product Attention은 1회씩 수행하는 것이 아니라 병렬적으로 h회 수행한 뒤에, 그 결과를 종합해서 사용한다. 이러한 연산을 수행하는 이유는 다양한 Attention을 잘 반영하기 위해서이다. 논문에서는 이 h를 8로 설정하였다. 자세한 내용은 블로그를 참고하길 바란다.\n",
        "\n",
        "https://cartinoe5930.tistory.com/entry/Transformer-Attention-Is-All-You-Need-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0\n",
        "\n",
        "이 Multi-Head Attention을 pytorch code로 구현하면 다음과 같다."
      ],
      "metadata": {
        "id": "BLVJVgbR3KTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, h, qkv_fc, out_fc):\n",
        "    super(MultiHeadAttentionLayer, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    self.q_fc = copy.deepcopy(qkv_fc)\n",
        "    self.k_fc = copy.deepcopy(qkv_fc)\n",
        "    self.v_fc = copy.deepcopy(qkv_fc)\n",
        "    self.out_fc = out_fc\n",
        "\n",
        "  def forward(self, *arge, query, key, value, mask = None):   # Transformer에서 가장 중요한 부분이니 확실히 이해하자!\n",
        "    n_batch = query.size(0)\n",
        "\n",
        "    def transform(x, fc):   # (n_batch, seq_len, d_embed)\n",
        "      out = fc(x)           # (n_batch, seq_len, d_model)\n",
        "      out = out.view(n_batch, -1, self.h, self.d_model, self.d_model//self.h)   # (n_batch, seq_len, h, d_k)\n",
        "      out = out.transpose(1, 2)   # (n_batch, h, seq_len, d_k)\n",
        "      return out\n",
        "\n",
        "    query = transform(query, self.q_fc)\n",
        "    key = transform(key, self.k_fc)\n",
        "    value = transform(value, self.v_fc)\n",
        "\n",
        "    out = self.calculate_attention(query, key, value, mask)   # Self-Attention (n_batch, h, seq_len, d_k)\n",
        "    out = out.transpose(1, 2)   # (n_batch, seq_len, h, d_k)\n",
        "    out = out.contiguous().view(n_batch, -1, self.d_model)   # (n_batch, seq_len, d_model)\n",
        "    out = self.out_fc(out)   # (n_batch, seq_len, d_model)\n",
        "    return out"
      ],
      "metadata": {
        "id": "eqDrLPPB2wdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "인자로 받고 있는 Query, Key, Value는 아직 실제 Q, K, V 행렬이 아닌, input sentence embedding이다. 이를 3개의 FC layer에 넣어서 각각의 Q, K, V를 구하는 것이다.\n",
        "\n",
        "transform()는 Q, K, V를 구하는 함수이다. 따라서 input shape는 (n_batch x seq_len x d_embed)이고, output shape는 (n_batch x seq_len x d_model)이다. 하지만, 이 중간에 d_model을 h와 d_k로 분리해서, shape는 (n_batch x seq_len x h x d_k)가 된다. 그리고 transpose를 통해 (n_batch x h x seq_len x d_k)로 변형하는데, 이는 calculate_attention()의 입력으로 사용해야 하기 때문이다. \n",
        "\n",
        "다시 forward()로 돌아와서, calculate_attention을 사용해서 attention을 계산하면 그 shape이 (n_batch x h x seq_len x d_k)이다. 하지만, Multi-Head Attention도 shape에 대해 멱등해야 하기 때문에, output shape는 input과 같은 (n_batch x seq_len x d_embed)여야 한다. 이를 위해 transpose를 한 후, d_k와 h를 결합하여 d_model을 만든다. 이후에 FC layer을 거쳐서 d_model을 d_embed로 변환한다.\n",
        "\n",
        "그리고 mask 인자를 받기 위해 Encoder block과 Transformer도 수정해야 한다."
      ],
      "metadata": {
        "id": "MwByosxA6fQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pad Mask\n",
        "\n",
        "그동안 생략했던 과정인 pad masking을 생성하는 make_pad_mask()이다. 여기서 pad의 인덱스를 의미하는 pad_idx와 일치하는 token들은 모두 0, 그 외에는 모두 1인 mask를 생성한다."
      ],
      "metadata": {
        "id": "Lmf9vBAgCZiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_pad_mask(self, query, key, pad_idx = 1):\n",
        "  query_seq_len, key_seq_len = query.size(1), key.size(1)\n",
        "  \n",
        "  key_mask = key.ne(pad_idx).unsqueeze(1).unsqueeze(2)   # (n_batch, 1, 1, key_seq_len)\n",
        "  key_mask = key_mask.repeat(1, 1, query_seq_len, 1)     # (n_batch, 1, query_seq_len, key_seq_len)\n",
        "\n",
        "  query_mask = query.ne(pad_idx).unsqueeze(1).unsqueeze(2)   # (n_batch, 1, 1, key_seq_len)\n",
        "  query_mask = query_mask.repeat(1, 1, 1, key_seq_len)       # (n_batch, 1, query_seq_len, key_seq_len)\n",
        "\n",
        "  mask = key_mask & query_mask\n",
        "  mask.requires_grad = False\n",
        "  return mask"
      ],
      "metadata": {
        "id": "2ber2Pc0CYz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pad mask는 개념적으로 Encoder 내부에서 생성하는 것이 아니기 때문에, Transformer의 method로 위치시킨다."
      ],
      "metadata": {
        "id": "qC0pGo_1Ef8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position-wise Feed Forward\n",
        "\n",
        "단순하게 2개의 FC layer를 갖는 layer이다. 각 FC layer는 (d_embed x d_ff), (d_ff x d_embed)의 가중치 행렬을 갖는다. 즉, Feed Forward Layer 역시 shape에 대해 멱등하다. 다음 Encoder Block에게 shape를 유지할 채 넘겨줘야 하기 때문이다. 정리하면, Feed Forward layer는 Multi-Head Attention layer의 output을 입력으로 받아 연산을 수행하고, 다음 Encoder Block에게 output을 넘겨준다. \n",
        "\n",
        "Position-wise Feed Forward를 pytorch code로 구현하면 다음과 같다."
      ],
      "metadata": {
        "id": "zW77ghLnEPth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForwardLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, fc1, fc2):\n",
        "    super(PositionWiseFeedForwardLayer, self).__init__()\n",
        "    self.fc1 = fc1   # (d_embed, d_ff)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = fc2   # (d_ff, d_embed)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = x\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "A38KFAFXFT2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual Connection Layer\n",
        "\n",
        "Encoder block은 Multi-Head Attention layer와 Position-wise Feed-Forward layer로 구성되어있다. 그러나 사실은 Encoder Block을 구성하는 두 layer는 Residual Connection으로 둘러싸여 있다. \n",
        "\n",
        "이를 pytorch code로 구현하면 다음과 같다. 정말 간단한다!"
      ],
      "metadata": {
        "id": "4NHOsx0QF7R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnectionLayer(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ResidualConnectionLayer, self).__init__()\n",
        "\n",
        "  def forward(self, x, sub_layer):\n",
        "    out = x\n",
        "    out = sub_layer(out)\n",
        "    out = out + x   # f(x) = f(x) + x\n",
        "    return out"
      ],
      "metadata": {
        "id": "siZe6HjqEnrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "따라서 Encoder Block의 code가 변경되게 된다."
      ],
      "metadata": {
        "id": "s54ew28IGre_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder 구현\n",
        "\n",
        "Decoder는 Encdoer의 출력인 context와 추가적인 sentence를 input으로 받아 output sentence를 출력한다. 여기서 의문이 생길 것이다. context를 받는 것 까지는 이해가 가는데, sentence는 무엇일까?\n",
        "\n",
        "#### Teacher forcing\n",
        "\n",
        "Decoder에 추가적으로 들어오는 sentence를 이해하기 위해서는 Teacher Forcing이라는 개념을 알아야 한다. Teacher Forcing은 지도 학습에서, label data를 input으로 사용하는 것이다. 이를 통해서 model이 잘못된 token을 생성하더라도 이후에 제대로 된 token을 생성해내도록 유도할 수 있는 것이다.\n",
        "\n",
        "따라서 이제, input으로 들어오는 sentence가 ground_truth[:-1]의 sentence라는 것을 알 수 있다. 하지만, Transformer는 RNN과 달리 이전 cell의 값을 참고할 수 없다. 왜냐하면 Transformer는 병렬 연산을 하기 때문이다. 따라서, masking을 적용해야 한다. 이러한 masking을 subsequent masking이라 한다. 이를 pytorch code로 구현하면 다음과 같다."
      ],
      "metadata": {
        "id": "OFHv6Q1VHTBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_subsequent_mask(query, key):\n",
        "  query_seq_len, key_seq_len = query.size(1), key.size(1)\n",
        "\n",
        "  tril = np.tril(np.ones((query_seq_len, key_seq_len)), k = 0).astype('uint8')\n",
        "  mask = torch.tensor(tril, dtype = torch.bool, requires_grad = False, device = query.device)\n",
        "  return mask"
      ],
      "metadata": {
        "id": "cwV-Z-5DGwVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "make_subsequent_mask()는 np.tril()을 사용하여 lower triangle을 생성한다. \n",
        "\n",
        "이를 통해 나온 결과는, n 번째 토큰은 0~n-1 번째 토큰을 참고할 수 있다.\n",
        "\n",
        "하지만, 동시에 Encoder와 마찬가지로 pad masking 역시 적용되어야 하기 때문에, make_tgt_mask()는 다음과 같다. make_subsequent_mask(), make_tgt_mask(), make_src_mask()는 Transformer의 method로 작성한다."
      ],
      "metadata": {
        "id": "vW3rg771JaaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tgt_mask(self, tgt):\n",
        "  pad_mask = self.make_pad_mask(tgt, tgt)\n",
        "  seq_mask = self.make_subsequent_mask(tgt, tgt)\n",
        "  mask = pad_mask & seq_mask\n",
        "  return pad_mask & seq_mask"
      ],
      "metadata": {
        "id": "Cj1DvwcdKMr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer로 다시 돌아가서, 기존에는 pad_mask만이 forward()를 구해야 했다면, 이제는 Decoder에서 사용할 subsequent + pad_mask도 구해야 한다. forward() 내부에서 Decoder의 forward()를 호줄할 때 역시 변경되는데, tgt_mask가 추가적으로 인자로 넘어가게 된다."
      ],
      "metadata": {
        "id": "oULxdPW2KhOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Block\n",
        "\n",
        "Decoder 역시 Encoder와 마찬가지로 N개의 Decoder Block이 겹겹이 쌓여있는 구조이다. 이때, Encoder에서 넘어오는 context가 각 Decoder Block마다 input으로 주어진다는 것이다. 그 외에는 Encoder와 차이가 전혀 없다.\n",
        "\n",
        "Decoder의 attention layer는 총 두 가지로 이루어져 있는데, 하나는 Masked-Multi-Head Attention layer이다. 이렇게 불리는 이유는 pad masking 뿐만 아니라 subsequent masking까지 적용되기 때문이다. 다른 하나는 Encoder에서 넘어온 context를 key와 value로 사용하기 때문에, Cross-Multi-Head Attention이다. \n",
        "\n",
        "더욱 자세한 내용은 블로그를 참고하길 바란다.\n",
        "\n",
        "https://cartinoe5930.tistory.com/entry/Transformer-Attention-Is-All-You-Need-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0"
      ],
      "metadata": {
        "id": "dTtCAujKLEYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  \n",
        "  def __init__(self, decoder_block, n_layer):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.n_layer = n_layer\n",
        "    self.layers = nn.ModuleList([copy.deepcopy(decoder_block) for _ in range(self.n_layer)])\n",
        "\n",
        "  def forward(self, tgt, encdoer_out, tgt_mask, src_tgt_mask):\n",
        "    # encoder_out: context로부터 얻어낸 key와 value\n",
        "    # tgt_mask: make_tgt_mask()로 얻어진 mask\n",
        "    out = tgt\n",
        "    for layer in self.layers:\n",
        "      out = layer(out, encdoer_out, tgt_mask, src_tgt_mask)\n",
        "    return out"
      ],
      "metadata": {
        "id": "Xy3C0M2SMyJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder Block은 Encoder Block과 별 차이 없다. forward()에서 self_attention과 달리 cross_attention의 key, value는 encoder_out이라는 것, 각각 mask가 tgt_mask, src_tgt_mask라는 것만 주의하면 된다."
      ],
      "metadata": {
        "id": "C3JMzpNnP5BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, self_attention, cross_attention, position_ff):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    self.self_attention = self_attention\n",
        "    self.cross_attention = cross_attention\n",
        "    self.position_ff = position_ff\n",
        "    self.residuals = [ResidualConnectionLayer() for _ in range(3)]\n",
        "\n",
        "  def forward(self, tgt, encoder_out, tgt_mask, src_tgt_mask):\n",
        "    out = tgt\n",
        "    out = self.residuals[0](out, lambda out: self.self_attention(query = out, key = out, value = out, mask = tgt_mask))\n",
        "    out = self.residuals[1](out, lambda out: self.cross_attention(query = out, key = encoder_out, value = encoder_out, mask = src_tgt_mask))\n",
        "    out = self.residuals[2](out, self.position_ff)\n",
        "    return out"
      ],
      "metadata": {
        "id": "olvoCdfHQHgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이에 따라 Transformer도 src_tgt_mask를 포함해 수정된다."
      ],
      "metadata": {
        "id": "iVv38QTbRia2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Input (Positional Embedding)\n",
        "\n",
        "Transformer의 Embedding은 단순하게 Token Embedding과 Positional Encoding의 sequential로 구성된다."
      ],
      "metadata": {
        "id": "aQFiMljmRypG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "\n",
        "  def __init__(self, otken_embed, pos_embed):\n",
        "    super(TransformerEmbedding, self).__init__()\n",
        "    self.embedding = nn.Sequential(token_embed, pos_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.embedding(x)\n",
        "    return out"
      ],
      "metadata": {
        "id": "d-jPGd3BRhtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token Embedding 또한 역시 단순한다. vocabulary와 d_embed를 사용해서 embedding을 생성한다."
      ],
      "metadata": {
        "id": "W3cmjt6lStag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "  \n",
        "  def __init__(self, d_embed, vocab_size):\n",
        "    super(TokenEmbedding, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, d_embed)\n",
        "    self.d_embed = d_embed\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.embedding(x) * math.sqrt(self.d_embed)\n",
        "    return out"
      ],
      "metadata": {
        "id": "sbOQZEdaSsHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "마지막으로 Positional Encoding을 살펴보자."
      ],
      "metadata": {
        "id": "AY3aCCR4TbVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_embed, max_len = 256, device = torch.device('cpu')):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    encoding = torch.zeros(max_len, d_embed)\n",
        "    encoding.requires_grad = False\n",
        "    position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed))\n",
        "    encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "    encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "    self.encoding = encoding.unsqueeze(0).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    _, seq_len, _ = x.size()\n",
        "    pos_embed = self.encdoing[:, :seq_len, :]\n",
        "    out = x + pos_embed\n",
        "    return out"
      ],
      "metadata": {
        "id": "fhL7jVfsTeUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Encoding의 목적은 positional 정보를 정규화시키기 위한 것이다. positional 정보를 일정한 범위 안의 실수로 제약해두는데, 여기서 sin함수와 cos함수를 사용한다. 짝수 index에는 sin함수를, 홀수 index에는 cos함수를 사용한다. 이를 사용할 경우 항상 -1에서 1 사이의 값만이 positional 정보로 사용되게 된다.\n",
        "\n",
        "이렇게 생성해낸 embedding을 Transformer에 추가해주자."
      ],
      "metadata": {
        "id": "W-8PPhtPU_cQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator\n",
        "\n",
        "Decoder의 output이 그대로 Transformer의 최종 output이 되는 것은 아니다. 즉, Embedding이 아닌 실제 target vocab 에서의 token sequence를 원하는 것이다. 이를 위해 추가적인 FC layer을 거치는데 이를 대게 Generator라고 부른다.\n",
        "\n",
        "Generator가 하는 일은 Decoder output의 마지막 dimension을 d_embed에서 len(vocab)으로 변경하는 것이다. 이를 통해 실제 vocabulary 내 token에 대응시킬 수 있는 shape가 된다. 이후에 softmax()를 사용해 각 covabulary에 대한 확률값으로 변환한다.\n",
        "\n",
        "Generator을 Transformer에 추가시켜 보자."
      ],
      "metadata": {
        "id": "43Ypkm3SWLqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Factory Method\n",
        "\n",
        "Transformer를 생성하는 build_model()은 다음과 같이 작성할 수 있다. 각 module의 submodule을 생성자 내부에서 생성하지 않고, 외부에서 인자로 받는 이유는 더 자유롭게 모델을 변경해 응용할 수 있게 하기 위함이다."
      ],
      "metadata": {
        "id": "yfTf9zLM7OX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(src_vocab_size, tgt_vocab_size, device=torch.device(\"cpu\"), max_len=256, d_embed=512, n_layer=6, d_model=512, h=8, d_ff=2048):\n",
        "    import copy\n",
        "    copy = copy.deepcopy\n",
        "\n",
        "    src_token_embed = TokenEmbedding(\n",
        "                                     d_embed = d_embed,\n",
        "                                     vocab_size = src_vocab_size)\n",
        "    tgt_token_embed = TokenEmbedding(\n",
        "                                     d_embed = d_embed,\n",
        "                                     vocab_size = tgt_vocab_size)\n",
        "    pos_embed = PositionalEncoding(\n",
        "                                   d_embed = d_embed,\n",
        "                                   max_len = max_len,\n",
        "                                   device = device)\n",
        "\n",
        "    src_embed = TransformerEmbedding(\n",
        "                                     token_embed = src_token_embed,\n",
        "                                     pos_embed = copy(pos_embed))\n",
        "    tgt_embed = TransformerEmbedding(\n",
        "                                     token_embed = tgt_token_embed,\n",
        "                                     pos_embed = copy(pos_embed))\n",
        "\n",
        "    attention = MultiHeadAttentionLayer(\n",
        "                                        d_model = d_model,\n",
        "                                        h = h,\n",
        "                                        qkv_fc = nn.Linear(d_embed, d_model),\n",
        "                                        out_fc = nn.Linear(d_model, d_embed))\n",
        "    position_ff = PositionWiseFeedForwardLayer(\n",
        "                                               fc1 = nn.Linear(d_embed, d_ff),\n",
        "                                               fc2 = nn.Linear(d_ff, d_embed))\n",
        "\n",
        "    encoder_block = EncoderBlock(\n",
        "                                 self_attention = copy(attention),\n",
        "                                 position_ff = copy(position_ff))\n",
        "    decoder_block = DecoderBlock(\n",
        "                                 self_attention = copy(attention),\n",
        "                                 cross_attention = copy(attention),\n",
        "                                 position_ff = copy(position_ff))\n",
        "\n",
        "    encoder = Encoder(\n",
        "                      encoder_block = encoder_block,\n",
        "                      n_layer = n_layer)\n",
        "    decoder = Decoder(\n",
        "                      decoder_block = decoder_block,\n",
        "                      n_layer = n_layer)\n",
        "    generator = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    model = Transformer(\n",
        "                        src_embed = src_embed,\n",
        "                        tgt_embed = tgt_embed,\n",
        "                        encoder = encoder,\n",
        "                        decoder = decoder,\n",
        "                        generator = generator).to(device)\n",
        "    model.device = device\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "e-XDobSzD-CZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}