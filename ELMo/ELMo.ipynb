{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNanKVFKMnCVZJm48NJvEOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauss5930/Natural-Language-Processing/blob/main/ELMo/ELMo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAKmz65EfvqQ"
      },
      "outputs": [],
      "source": [
        "from typing import LIst, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from char_cnn import CharEmbedding\n",
        "\n",
        "class ELMo(nn.Module):\n",
        "  def __init__(self, vocab_size, output_dim, emb_dim, hid_dim, prj_dim, kernel_sizes,\n",
        "               seq_len, n_layers, dropout):\n",
        "    #파라미터 설명(몇 개만)\n",
        "    #output_dim: word vocaulary 크기\n",
        "    #n_layers: LSTM의 레이어 수. 기본값은 2\n",
        "\n",
        "    super(ELMo, self).__init__()\n",
        "\n",
        "    self.embedding = CharEmbedding(vocab_size, emb_dim, prj_dim, kernel_sizes, seq_len)\n",
        "    self.bilms = BidirectionalLanguageModel(hid_dim, hid_dim, n_layers, dropout)\n",
        "\n",
        "    self.predict = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #파라미터: x(Sentence)\n",
        "    #차원: x([batch, seq_len])\n",
        "    emb = self.embedding(x)\n",
        "    _, last_output = self.bilms(emb)\n",
        "    y = self.predict(last_output)\n",
        "\n",
        "    return y   #훈련 단계에서는 오직 biLM의 마지막 LSTM의 output만을 사용하여라\n",
        "\n",
        "  def get_embed_layer(self, x):   #torch.Tensor --> List\n",
        "    #순전파와 똑같지만, 모든 레이어의 임베딩을 반환함\n",
        "    #파라미터: x(character로 이루어진 sentence)\n",
        "    #차원: x([batch, seq_len])\n",
        "    emb = self.embedding(x)\n",
        "    first_output, last_output = self.bilms(emb)\n",
        "\n",
        "    return emb, (first_output, last_output)\n",
        "\n",
        "  def init_weights(self):\n",
        "    for p in self.parameters():\n",
        "      if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "    for lstm in self.bilms.lstms:\n",
        "      for names in lstm._all_weights:\n",
        "        for name in filter(lambda n: 'bias' in n, names):\n",
        "          bias = getattr(lstm, name)\n",
        "          n = bias.size(0)\n",
        "          start, end = n // 4, n // 2\n",
        "          bias.data[start:end].fill_(1.)\n",
        "\n",
        "class BidirectionalLanguageModel(nn.Module):\n",
        "  def __init__(self, emb_dim, hid_dim, prj_emb, dropout):\n",
        "    #LSTM 레이어의 이전과 이후 모두에 dropout 사용\n",
        "    super(BidirectionalLanguageModel, self).__init__()\n",
        "    self.lstms = nn.ModuleList([nn.LSTM(emb_dim, hid_dim, bidirectional = True, dropout = dropout,\n",
        "                                        batch_first = True), nn.LSTM(prj_emb, hid_dim, bidirectional = True, dropout = dropout, bacth_first = True)])\n",
        "    self.projection_layer = nn.Linear(2 * hid_dim, prj_emb)\n",
        "\n",
        "  def forward(self, x, hidden = None):\n",
        "    #파라미터: x(임베딩된 sentence tensor), hidden(hidden과 cell의 tuple)\n",
        "    #차원: x([Batch, Seq_len, Emb_size]),\n",
        "    #hidden([num_layers * num_directions, batch, hidden_size], [num_layers * num_directions, batch, hidden_size])\n",
        "    \n",
        "    #LSTM 레이어 사이에 residual connection 추가\n",
        "    first_output, (hidden, cell) = self.lstms[0](x, hidden)\n",
        "\n",
        "    projected = self.projection_layer(first_output)\n",
        "    second_output, (hidden, cell) = self.lstms[1](projected, (hidden, cell))\n",
        "\n",
        "    second_output = second_output.view(second_output.size(0), second_output.size(1), 2, -1)\n",
        "\n",
        "    second_output = second_output[:, :, 0, :] + second_output[:, :, 1, :]\n",
        "\n",
        "    return first_output, second_output"
      ]
    }
  ]
}