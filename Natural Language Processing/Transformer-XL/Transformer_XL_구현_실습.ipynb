{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtb06YYh5iyXi4CRZAWjAR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauss5930/Natural-Language-Processing/blob/main/Transformer-XL/Transformer_XL_%EA%B5%AC%ED%98%84_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70fsbBslZZ7I"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def positional_embedding(pos_seq, inv_freq, bsz = None):\n",
        "  sinusoid_inp = tf.einsum('i,j->ij', pos_seq, inv_freq)\n",
        "  pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n",
        "  if bsz is not None:\n",
        "    return tf.tile(pos_emb[:, None, :], [1, bsz, 1])\n",
        "  else:\n",
        "    return pos_emb[:, None, :]\n",
        "\n",
        "def positionwise_FF(inp, d_model d_inner, dropout, kernel_initializer, scope = 'ff', is_training = True):\n",
        "  output = inp\n",
        "  with tf.variable_scope(scope):\n",
        "    output = tf.layers.dense(inp, d_inner, activation = tf.nn.relu,\n",
        "                             kernel_initializer = kernel_initializer,\n",
        "                             name = 'layer_1')\n",
        "    output = tf.layers.dropout(output, dropout, training = is_training, name = 'drop_1')\n",
        "    output = tf.layers.dense(output, d_model, kernel_initializer = kernel_initializer,\n",
        "                             name = 'layer2')\n",
        "    output = tf.layers.dropout(output, dropout, training = is_training, name = 'drop_2')\n",
        "    output = tf.contrib.layers.layer_norm(output + inp, begin_norm_axis = -1)\n",
        "  return output\n",
        "\n",
        "def rel_shift(x):\n",
        "  x_size = tf.shape(x)\n",
        "\n",
        "  x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
        "  x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])\n",
        "  x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
        "  x = tf.reshape(x, x_size)\n",
        "\n",
        "  return x\n",
        "\n",
        "def rel_multihead_attn(w, r, r_w_bias, r_r_bias, attn_mask, mems, d_model,\n",
        "                       n_head, d_head, dropout, dropatt, is_training,\n",
        "                       kernel_initializer, scope = 'rel_attn'):\n",
        "  scale = 1 / (d_head ** 0.5)\n",
        "  with tf.variable_scope(scope):\n",
        "    qlen = tf.shape(w)[0]\n",
        "    rlen = tf.shape(r)[0]\n",
        "    bsz = tf.shape(w)[1]\n",
        "\n",
        "    cat = tf.concat([mems, w], 0) if mems is not None and mems.shape.ndims > 1 else w\n",
        "    w_heads = tf.layers.dense(cat, 3 * n_head, d_head, use_bias = False, kernel_initializer = kernel_initializer,\n",
        "                              name = 'qkv')\n",
        "    r_head_k = tf.layers.dense(r, n_head * d_head, use_bias = False, kernel_initializer = kernel_initializer,\n",
        "                               name = 'r')\n",
        "    \n",
        "    w_head_q, w_kead_k, w_head_v = tf.split(w_heads, 3, -1)\n",
        "    w_head_q = w_head_q[-qlen:]\n",
        "\n",
        "    klen = tf.shape(w_head_k)[0]\n",
        "\n",
        "    w_head_q = tf.reshape(w_head_q, [qlen, bsz, n_head, d_head])\n",
        "    w_head_k = tf.reshape(w_head_k, [klen, bsz, n_head, d_head])\n",
        "    w_head_v = tf.reshape(w_head_v, [klen, bsz, n_head, d_head])\n",
        "\n",
        "    r_head_k = tf.reshape(r_head_k, [rlen, n_head, d_head])\n",
        "\n",
        "    rw_head_q = w_head_q + r_w_bias\n",
        "    rr_head_q = w_head_q + r_r_bias\n",
        "\n",
        "    AC = tf.einsum('ibnd,jbnd->ijbn', rw_head_q, w_head_k)\n",
        "    BD = tf.einsum('ibnd,jnd->ijbn', rr_head_q, r_head_k)\n",
        "    BD = rel_shift(BD)\n",
        "\n",
        "    attn_score = (AC + BD) * scale\n",
        "    attn_mask_t = attn_mask[:, :, None, None]\n",
        "    attn_score = attn_score * (1 - attn_mask_t) - 1e30 * attn_mask_t\n",
        "\n",
        "    attn_prob = tf.nn.softmax(attn_score, 1)\n",
        "    attn_prob = tf.layers.dropout(attn_prob, dropatt, training = is_training)\n",
        "\n",
        "    attn_vec = tf.einsum('ijbn,jbnd->ibnd', attn_prob, w_head_v)\n",
        "    size_t = tf.shape(attn_vec)\n",
        "    attn_vec = tf.reshape(attn_vec, [size_t[0], size_t[1], n_head * d_head])\n",
        "\n",
        "    attn_out = tf.layers.dense(attn_vec, d_model, use_bias = False,\n",
        "                               kernel_initializer = kernel_initializer, name ='o')\n",
        "    attn_out = tf.layers.dropout(attn_out, dropout, training = is_training)\n",
        "\n",
        "    output = tf.contrib.layers.layer_norm(attn_out + w, begin_norm_axis = -1)\n",
        "\n",
        "  return output\n",
        "\n",
        "def embedding_lookup(lookup_table, x, use_tpu = True):\n",
        "  if use_tpu:\n",
        "    n_token = tf.shape(lookup_table)[0]\n",
        "    one_hot_idx = tf.one_hot(x, n_token)\n",
        "    if one_hot_idx.shape.ndims == 2:\n",
        "      return tf.einsum('nd,in->id', lookup_table, one_hot_idx)\n",
        "    else:\n",
        "      return tf.einsum('nb,ibn->ibd', lookup_table, one_hot_idx)\n",
        "  else:\n",
        "    return tf.nn.embedding_lookup(lookup_table, x)\n",
        "\n",
        "def mask_adaptive_embedding_lookup(x, n_token, d_embed, d_proj, cutoffs, initializer,\n",
        "                                   proj_initializer, div_val = 1,\n",
        "                                   proj_same_dim = True,\n",
        "                                   scope = 'adaptive_embed', **kwargs):\n",
        "  emb_scale = d_proj ** 0.5\n",
        "  with tf.variable_scope(scope):\n",
        "    if div_val == 1:\n",
        "      lookup_table = tf.get_variable('lookup_table', [n_token, d_embed], initializer = initializer)\n",
        "      y = embedding_lookup(lookup_table, x, use_tpu = False)\n",
        "      if d_proj != d_embed:\n",
        "        proj_W = tf.get_variable('proj_W', [d_embed, d_proj], initializer = proj_initializer)\n",
        "        y = tf.einsum('ibe,ed->ibd', y, proj_w)\n",
        "      else:\n",
        "        proj_w = None\n",
        "      ret_params = [lookup_table, proj_W]\n",
        "    else:\n",
        "      tables, projs = [], []\n",
        "      curoff_ends = [0] + cutoffs + [n_token]\n",
        "      x_size = tf.shape(x)\n",
        "      y = tf.zeros([x_size[0], x_size[1], d_proj])\n",
        "      for i in range(len(cutoff_ends) - 1):\n",
        "        with tf.variable_scope('cutoff_{}'.format(i)):\n",
        "          l_idx, r_idx = cutoff_ends[i], cutoff_ends[i+1]\n",
        "          mask = (x >= l_idx) & (x < r_idx)\n",
        "          cur_x = tf.boolean_mask(x, mask) - l_idx\n",
        "          cur_d_embed = d_embed // (div_val ** i)\n",
        "          lookup_table = tf.get_variable('lookup_table', [r_idx - l_idx, cur_d_embed].\n",
        "                                         initializer = initializer)\n",
        "          cur_y = embedding_lookup(lookup_table, cur_x, use_tpu = False)\n",
        "          if d_proj == cur_d_embed and not proj_same_dim:\n",
        "            proj_W = None\n",
        "          else:\n",
        "            proj_W = tf.get_variable('proj_W', [cur_d_embed, d_proj],\n",
        "                                     initializer = proj_initializer)\n",
        "            cur_y = tf.einsum('id,de->ie', cur_y, proj_W)\n",
        "          mask_idx = tf.to_int64(tf.where(mask))\n",
        "          y += tf.scatter_nd(mask_idx, cur_y, tf.to_int64(tf.shape(y)))\n",
        "          tables.append(lookup_table)\n",
        "          projs.append(proj_W)\n",
        "      ret_params = [tables, projs]\n",
        "  \n",
        "  y *= emb_scale\n",
        "  return y, ret_params\n",
        "\n",
        "def mul_adaptive_embedding_lookup(x, n_token, d_embed, d_proj, cutoffs, initializer,\n",
        "                                  proj_initializer, div_val = 1, perms = None,\n",
        "                                  proj_same_dim = True, scope = 'adaptive_embed'):\n",
        "  #만약 perm이 None이라면\n",
        "  #W = W1 X W2와 같이 각각 projection되고, 그 다음에 X x W (embedding lookup)을 계산\n",
        "  #None이 아니라면\n",
        "  #bin-based embedding lookup을 사용\n",
        "\n",
        "  emb_scale = d_proj ** 0.5\n",
        "  with tf.variable_scope(scope):\n",
        "    if div_val == 1:\n",
        "      lookup_table = tf.get_variable('lookup_table', [n_token, d_embed], initializer = initializer)\n",
        "      y = embedding_lookup(lookup_table, x)\n",
        "      if d_proj != d_embed:\n",
        "        proj_W = tf.get_variable('proj_W', [d_embed, d_proj], initializer = proj_initializer)\n",
        "        y = tf.einsum('ibe,ed->ibd', y, proj_W)\n",
        "      else:\n",
        "        proj_W = None\n",
        "      ret_params = [lookup_table, proj_W]\n",
        "    else:\n",
        "      tables, projs = [], []\n",
        "      cutoff_ends = [0] + cutoffs + [n_token]\n",
        "      x_size = tf.shape(x)\n",
        "      if perms is None:\n",
        "        cat_lookup = []\n",
        "      else:\n",
        "        cat_lookup = tf.zeros([x_size[0], x_size[1], d_proj])\n",
        "      for i in range(len(cutoff_ends) - 1):\n",
        "        with tf.variable_scope('cutoff_{}'.format(i)):\n",
        "          l_idx, r_idx = cutoff_ends[i], cutoff_ends[i+1]\n",
        "          cur_d_embed = d_embed // (div_val ** i)\n",
        "          lookup_table = tf.get_variable('lookup_table',\n",
        "                                         [r_idx - l_idx, cur_d_embed],\n",
        "                                         initializer = initializer)\n",
        "          if cur_d_embed == d_proj and not proj_same_dim:\n",
        "            proj_W = None\n",
        "          else:\n",
        "            proj_W = tf.get_variable('proj_W', [cur_d_embed, d_proj],\n",
        "                                     initializer = proj_initializer)\n",
        "          if perms is None:\n",
        "            cat_lookup.append(tf.einsum('ie,ed->id', lookup_table, proj_W))\n",
        "          else:\n",
        "            if i == 0:\n",
        "              cur_y = embedding_lookup(lookup_table, tf.minimum(x, r_idx - 1))\n",
        "              if proj_W is not None:\n",
        "                cur_y = tf.einsum('ibe,ed->ibd', cur_y, proj_W)\n",
        "              cur_y *= perms[i][:, :, None]\n",
        "              cat_lookup += cur_y\n",
        "            else:\n",
        "              cur_x = tf.einsum('ib,ibk->k', tf.to_float(x - l_idx), perms[i])\n",
        "              cur_x = tf.to_int32(cur_x)\n",
        "              cur_y = embedding_lookup(lookup_table, cur_x)\n",
        "              if proj_W is not None:\n",
        "                cur_y = tf.einsum('ke,ed->kd', cur_y, proj_W)\n",
        "              cat_lookup += tf.einsum('kd,idk->ibd', cur_y, perms[i])\n",
        "          tables.append(lookup_table)\n",
        "          projs.append(proj_W)\n",
        "      if perms is None:\n",
        "        cat_lookup = tf.concat(cat_lookup, 0)\n",
        "        y = embedding_lookup(cat_lookup, x)\n",
        "      else:\n",
        "        y = cat_lookup\n",
        "      ret_params = [tables, projs]\n",
        "  \n",
        "  y *= emb_scale\n",
        "  return y, ret_params\n",
        "\n",
        "def mask_adaptive_logsoftmax(hidden, target, n_token, d_embed, d_proj, cutoffs, params,\n",
        "                             tie_projs, initializer = None, proj_initializer = None,\n",
        "                             div_val = 1, scope = 'adaptive_softmax', proj_same_dim = True,\n",
        "                             return_mean = True, **kwargs):\n",
        "  def _logit(x, W, b, proj):\n",
        "    y = x\n",
        "    if proj is not None:\n",
        "      y = tf.einsum('ibd,ed->ibe', y, proj)\n",
        "    return tf.einsum('ibd, nd->ibn', y, W) + b\n",
        "\n",
        "  params_W, params_projs = params[0], params[1]\n",
        "\n",
        "  def _gather_logprob(logprob, target):\n",
        "    lp_size = tf.shape(logprob)\n",
        "    r = tf.range(lp_size[0])\n",
        "    idx = tf.stack([r, target], 1)\n",
        "    return tf.gather_nd(logprob, idx)\n",
        "\n",
        "  with tf.variable_scope(scope):\n",
        "    if len(cutoffs) == 0:\n",
        "      softmax_b = tf.get_variable('bias', [n_token],\n",
        "                                  initializer = tf.zeros_initializer())\n",
        "      output = _logit(hidden, prams_W, softmax_b, params_projs)\n",
        "      nll = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = target, logits = output)\n",
        "    else:\n",
        "      cutoff_ends = [0] + cutoffs + [n_token]\n",
        "      nll = tf.zeros_like(target, dtype = tf.float32)\n",
        "      for i in range(len(cutoff_ends) - 1):\n",
        "        with tf.variable_scope('cutoff_{}'.format(i)):\n",
        "          l_idx, r_idx = cutoff_ends[i], cutoff_ends[i+1]\n",
        "          mask = (target >= l_idx) & (target < r_idx)\n",
        "          mask_idx = tf.where(mask)\n",
        "          cur_target = tf.boolean_mask(target, mask) - l_idx\n",
        "          cur_d_embed = d_embed // (div_val ** i)\n",
        "\n",
        "          if div_val == 1:\n",
        "            cur_W = params_W[l_idx: r_idx]\n",
        "          else:\n",
        "            cur_W = params_W[i]\n",
        "          cur_b = tf.get_variable('b', [r_idx - l_idx], initializer = tf.zeros_initializer())\n",
        "          if tie_projs[i]:\n",
        "            if div_val == 1:\n",
        "              cur_proj = params_projs\n",
        "            else:\n",
        "              cur_proj = params_projs[i]\n",
        "          else:\n",
        "            if (div_val == 1 or not proj_same_dim) and d_proj == cur_d_embed:\n",
        "              cur_proj = None\n",
        "            else:\n",
        "              cur_proj = tf.get_variable('proj', [cur_d_embed, d_proj],\n",
        "                                         initializer = proj_initializer)\n",
        "          if i == 0:\n",
        "            cluster_W = tf.get_variable('cluster_W', [len(cutoffs), d_embed],\n",
        "                                        initializer = tf.zeros_initializer())\n",
        "            cluster_b = tf.get_variable('cluster_b', [len(cutoffs)],\n",
        "                                        initializer = tf.zeros_initializer())\n",
        "            cur_W = tf.concat([cur_W, cluster_W], 0)\n",
        "            cur_b = tf.concat([cur_b, cluster_b], 0)\n",
        "\n",
        "            head_logit = _logit(hidden, cur_W, cur_b, cur_proj)\n",
        "            head_logprob = tf.nn.log_softmax(head_logit)\n",
        "            cur_head_logprob = tf.boolean_mask(head_logprob, mask)\n",
        "            cur_logprob = _gather_logprob(cur_head_logprob, cur_target)\n",
        "          else:\n",
        "            cur_head_logprob = tf.boolean_mask(head_logprob, mask)\n",
        "            cur_hidden = tf.boolean_mask(hidden_mask)\n",
        "            tail_logit = tf.squeeze(_logit(cur_hidden[None], cur_W, cur_b, cur_proj), 0)\n",
        "            tail_logprob = tf.nn.log_softmax(tail_logit)\n",
        "            cur_logprob = (cur_head_logprob[:, cutoff_ends[1]+i-1] + _gather_logprob(tail_logprob, cur_target))\n",
        "          nll += tf.scatter_nd(mask_idx, -cur_logprob, tf.to_int64(tf.shape(nll)))\n",
        "\n",
        "  if return_mean:\n",
        "    nll = tf.reduce_mean(nll)\n",
        "  return nll\n",
        "\n",
        "def mul_adaptive_logsoftmax(hidden, target, n_token, d_embed, d_proj, cutoffs,\n",
        "                            params, tie_projs,\n",
        "                            initializer=None, proj_initializer=None,\n",
        "                            div_val=1, perms=None, proj_same_dim=True,\n",
        "                            scope='adaptive_softmax',\n",
        "                            **kwargs):\n",
        "  def _logit(x, W, b, proj):\n",
        "    y = x\n",
        "    if x.shape.ndims == 3:\n",
        "      if proj is not None:\n",
        "        y = tf.einsum('ibd,ed->ibe', y, proj)\n",
        "      return tf.einsum('ibd,nd->ibn', y, W) + b\n",
        "    else:\n",
        "      if proj is not None:\n",
        "        y = tf.einsum('id,ed->ie', y, proj)\n",
        "      return tf.einsum('id,nd->in', y, W) + b\n",
        "\n",
        "  params_W, params_projs = params[0], params[1]\n",
        "\n",
        "  with tf.variable_scope(scope):\n",
        "    if len(cutoffs) == 0:\n",
        "      softmax_b = tf.get_variable('bias', [n_token], initializer = tf.zeros_initializer())\n",
        "      output = _logit(hidden, params_W, softmax_b, params_projs)\n",
        "      nll = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = target, logits = output)\n",
        "      nll = tf.reduce_mean(nll)\n",
        "    else:\n",
        "      total_loss, total_cnt = 0, 0\n",
        "      cutoff_ends = [0] + cutoffs + [n_token]\n",
        "      for i in range(len(cutoff_ends) - 1):\n",
        "        with tf.variable_scope('cutoff_{}'.format(i)):\n",
        "          l_idx, r_idx = cutoff_ends[i], cutoff_ends[i+1]\n",
        "\n",
        "          cur_d_embed = d_embed // (div_val ** i)\n",
        "\n",
        "          if div_val == 1:\n",
        "            cur_W = params_W[l_idx: r_idx]\n",
        "          else:\n",
        "            cur_W = params_W[i]\n",
        "          cur_b = tf.get_variable('b', [r_idx - l_idx], initializer = tf.zeros_initializer())\n",
        "\n",
        "          if tie_projs[i]:\n",
        "            if div_val == 1:\n",
        "              cur_proj = params_projs\n",
        "            else:\n",
        "              cur_proj = params_projs[i]\n",
        "          else:\n",
        "            if (div_val == 1 of not proj_same_dim) and d_proj == cur_d_embed:\n",
        "              cur_proj = None\n",
        "            else:\n",
        "              cur_proj = tf.get_variable('proj', [cur_d_embed, d_proj], initializer = tf.zeros_initializer())\n",
        "\n",
        "          if i == 0:\n",
        "            cluster_W = tf.get_variable('cluster_W', [len(cutoffs), d_embed],\n",
        "                                        initializer = tf.zeros_initializer())\n",
        "            cluster_b = tf.get_variable('cluster_b', [len(cutoffs)],\n",
        "                                        initializer = tf.zeros_initializer())\n",
        "            cur_W = tf.concat([cur_W, cluster_W], 0)\n",
        "            cur_b = tf.concat([cur_b, cluster_b], 0)\n",
        "\n",
        "            head_logit = _logit(hidden, cur_W, cur_b, cur_proj)\n",
        "\n",
        "            head_target = kwargs.get('head_target')\n",
        "            head_nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                labels = head_target,\n",
        "                logits = head_logit\n",
        "            )\n",
        "\n",
        "            masked_loss = head_nll * perms[i]\n",
        "            total_loss += tf.reduce_sum(masked_loss)\n",
        "            total_cnt += tf.reduce_sum(perms[i])\n",
        "\n",
        "          else:\n",
        "            cur_head_nll = tf.einsum('ib,ibk->k', head_nll, perms[i])\n",
        "\n",
        "            cur_hidden_tf.einsum('ibd,ibk->kd', hidden, perms[i])\n",
        "            tail_logit = _logit(cur_hidden, cur_W, cur_b, cur_proj)\n",
        "\n",
        "            tail_target = tf.einsum('ib,ibk->k', tf.to_float(target - l_idx), perms[i])\n",
        "            tail_nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                labels = tf.to_int43(tail_target), logits = tail_logit\n",
        "            )\n",
        "\n",
        "            sum_nll = cur_head_nll + tail_nll\n",
        "            mask = tf.reduce_sum(perms[i], [0, 1])\n",
        "\n",
        "            masked_loss = sum_nll * mask\n",
        "            total_loss += tf.reduce_sum(masked_loss)\n",
        "            total_cnt += tf.reduce_sum(mask)\n",
        "\n",
        "      nll = total_loss / total_cnt\n",
        "\n",
        "  return nll\n",
        "\n",
        "def _create_mask(qlen, mlen, same_length = False):\n",
        "  attn_mask = tf.ones([qlen, qlen])\n",
        "  mask_u = tf.matrix_band_part(attn_mask, 0, -1)\n",
        "  mask_dia = tf.matrix_band_part(attn_mask, 0, 0)\n",
        "  attn_mask_pad = tf.zeros([qlen, mlen])\n",
        "  ret = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n",
        "  if same_length:\n",
        "    mask_l = tf.matrix_band_part(attn_mask, -1, 0)\n",
        "    ret = tf.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)\n",
        "  return ret\n",
        "\n",
        "def _cache_mem(curr_out, prev_mem, mem_len = None):\n",
        "  if mem_len is None or prev_mem is None:\n",
        "    new_mem = curr_out\n",
        "  elif mem_len == 0:\n",
        "    return prev_mem\n",
        "  else:\n",
        "    new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:]\n",
        "\n",
        "  return tf.stop_gradient(new_mem)\n",
        "\n",
        "def transformer(dec_inp, target, mems, n_token, n_layer, d_model, d_embed,\n",
        "                n_head, d_head, d_inner, dropout, dropatt,\n",
        "                initializer, is_training, proj_initializer=None,\n",
        "                mem_len=None, cutoffs=[], div_val=1, tie_projs=[],\n",
        "                same_length=False, clamp_len=-1, use_tpu=True,\n",
        "                input_perms=None, target_perms=None, head_target=None,\n",
        "                untie_r=False, proj_same_dim=True,\n",
        "                scope='transformer'):\n",
        "  new_mems = []\n",
        "  with tf.variable_scope(scope):\n",
        "    if untie_r:\n",
        "      r_w_bias = tf.get_variable('r_w_bias', [n_layer, n_head, d_head],\n",
        "                                 initializer = initializer)\n",
        "      r_r_bias = tf.get_variable('r_r_bias', [n_layer, n_head, d_head],\n",
        "                                 initializer = initializer)\n",
        "    else:\n",
        "      r_w_bias = tf.get_variable('r_w_bias', [n_head, d_head],\n",
        "                                 initializer = initializer)\n",
        "      r_r_bias = tf.get_variable('r_r_bias', [n_head, d_head],\n",
        "                                 initializer = initializer)\n",
        "      \n",
        "    qlen = tf.shape(dec_inp)[0]\n",
        "    mlen = tf.shape(mems[0])[0] is mems is not None else 0\n",
        "    klen = mlen + qlen\n",
        "\n",
        "    if proj_initializer is None:\n",
        "      proj_initializer = initializer\n",
        "    lookup_fn = (mul_adaptive_embedding_lookup is use_tpu else\n",
        "                 mask_adaptive_embedding_lookup)\n",
        "    embeddings, shared_params = lookup_fn(\n",
        "        x=dec_inp,\n",
        "        n_token=n_token,\n",
        "        d_embed=d_embed,\n",
        "        d_proj=d_model,\n",
        "        cutoffs=cutoffs,\n",
        "        initializer=initializer,\n",
        "        proj_initializer=proj_initializer,\n",
        "        div_val= div_val,\n",
        "        perms=input_perms,\n",
        "        proj_same_dim=proj_same_dim)\n",
        "    \n",
        "    attn_mask = _create_mask(qlen, mlen, same_length)\n",
        "\n",
        "    pos_seq = tf.range(klen - 1, -1, -1.0)\n",
        "    if clasm_len > 0:\n",
        "      pos_seq = tf.minimum(pos_seq, clamp_len)\n",
        "    inv_freq = 1 / (10000 ** (tf.range(0, d_model, 2.0) / d_model))\n",
        "    pos_emb = positional_embedding(pos_seq, inv_freq)\n",
        "\n",
        "    output = tf.layers.dropout(embeddings, dropot, training = is_training)\n",
        "    pos_emb = tf.layers.dropout(pos_emb, dropout, training = is_training)\n",
        "\n",
        "    if mems is None:\n",
        "      mems = [None] * n_layer\n",
        "\n",
        "    for i in range(n_layer):\n",
        "      new_mems.append(_cache_mem(output, mems[i], mem_len))\n",
        "\n",
        "      with tf.variable_scope('layer_{}'.format(i)):\n",
        "        output = rel_multihead_attn(\n",
        "            w=output,\n",
        "            r=pos_emb,\n",
        "            r_w_bias=r_w_bias if not untie_r else r_w_bias[i],\n",
        "            r_r_bias=r_r_bias if not untie_r else r_r_bias[i],\n",
        "            attn_mask=attn_mask,\n",
        "            mems=mems[i],\n",
        "            d_model=d_model,\n",
        "            n_head=n_head,\n",
        "            d_head=d_head,\n",
        "            dropout=dropout,\n",
        "            dropatt=dropatt,\n",
        "            is_training=is_training,\n",
        "            kernel_initializer=initializer\n",
        "        )\n",
        "        output = positionwise_FF(\n",
        "            inp=output,\n",
        "            d_model=d_model,\n",
        "            d_inner=d_inner,\n",
        "            dropout=dropout,\n",
        "            kernel_initializer=initializer,\n",
        "            is_training=is_training\n",
        "        )\n",
        "\n",
        "        output = tf.layers.dropout(output, dropout, training = is_training)\n",
        "\n",
        "        logsoftmax_fn = (mul_adaptive_logsoftmax if use_tpu else\n",
        "                         mask_adaptive_logsoftmax)\n",
        "        loss = logsoftmax_fn(\n",
        "            hidden=output,\n",
        "            target=target,\n",
        "            n_token=n_token,\n",
        "            d_embed=d_embed,\n",
        "            d_proj=d_model,\n",
        "            cutoffs=cutoffs,\n",
        "            params=shared_params,\n",
        "            tie_projs=tie_projs,\n",
        "            initializer=initializer,\n",
        "            proj_initializer=proj_initializer,\n",
        "            div_val=div_val,\n",
        "            perms=target_perms,\n",
        "            head_target=head_target,\n",
        "            proj_same_dim=proj_same_dim\n",
        "        )\n",
        "\n",
        "        return loss, new_mems"
      ]
    }
  ]
}
