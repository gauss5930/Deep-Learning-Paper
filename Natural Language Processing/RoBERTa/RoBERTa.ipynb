{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFMz6/Gok9fwXantVBV8ja",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauss5930/Natural-Language-Processing/blob/main/RoBERTa/RoBERTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfkqXUOcPX1Y"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from fairseq import utils\n",
        "from fairseq.models import (\n",
        "    FairseqEncoder,\n",
        "    FairseqEncoderModel,\n",
        "    register_model,\n",
        "    register_model_architecture,\n",
        ")\n",
        "from fairseq.models.transformer import DEFAULT_MIN_PARAMS_TO_WRAP, TransformerEncoder\n",
        "from fairseq.modules import LayerNorm\n",
        "from fairseq.modules.quant_noise import quant_noise as apply_quant_noise_\n",
        "from fairseq.modules.transformer_sentence_encoder import init_bert_params\n",
        "from fairseq.utils import safe_getattr, safe_hasattr\n",
        "\n",
        "from .hub_interface import RobertaHubInterface\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@register_model('roberta')\n",
        "class RobertaModel(FairseqEncoderModel):\n",
        "  @classmethod\n",
        "  def hub_models(cls):\n",
        "    return {\n",
        "        \"roberta.base\": \"http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz\",\n",
        "        \"roberta.large\": \"http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz\",\n",
        "        \"roberta.large.mnli\": \"http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.mnli.tar.gz\",\n",
        "        \"roberta.large.wsc\": \"http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.wsc.tar.gz\",\n",
        "    }\n",
        "\n",
        "  def __init__(self, args, encoder):\n",
        "    super().__init__(encoder):\n",
        "    self.args = args\n",
        "\n",
        "    # BERT의 랜덤 가중치 초기화를 따름\n",
        "    self.apply(init_bert_params)\n",
        "\n",
        "    self.classification_heads = nn.ModuleDict()\n",
        "\n",
        "  @staticmethod\n",
        "  def add_args(parser):\n",
        "    # parser에 model-specific 인자를 추가\n",
        "    parser.add_argument(\n",
        "        # encoder layer의 수수\n",
        "        '--encoder-layers', type = int, metavat = 'L', help = 'num encoder layers'\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # encoder embedding 차원\n",
        "        '--encoder-embed-dim',\n",
        "        type = int,\n",
        "        metavar = 'H',\n",
        "        help = 'encoder embedding dimension',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # FFN을 위한 encoder embedding 차원\n",
        "        '--encoder-ffn-embed-dim',\n",
        "        type = int,\n",
        "        metavat = 'F',\n",
        "        help = 'encoder embedding dimension for FFN',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # 활성화 함수수\n",
        "        '--activation-fn',\n",
        "        choices = utils.get_available_activation_fns(),\n",
        "        help = 'activation function to use',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # pooler layer을 위한 활성화 함수\n",
        "        '--pooler-activation-fn',\n",
        "        choices = utils.get_available_activation_fns(),\n",
        "        help = 'activation function to use for pooler layer',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # 정규화 이전의 encoder\n",
        "        '--encoder-normalize-before',\n",
        "        action = 'store_true',\n",
        "        help = 'apply layernorm before each encoder block',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # layer normalization embedding\n",
        "        '--layernorm-embedding',\n",
        "        action = 'store_true',\n",
        "        help = 'add layernorm to embedding',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # dropout 확률\n",
        "        '--dropout',\n",
        "        type = float,\n",
        "        metavar = 'D',\n",
        "        help = 'dropout probability'\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # FFN에서 활성화 이후의 dropout 확률\n",
        "        '--activation-dropout',\n",
        "        type = float,\n",
        "        metavat = 'D',\n",
        "        help = 'dropout probability after activation in FFN',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # MLM의 pooler layer에서 dropout 확률\n",
        "        'pooler-activation',\n",
        "        type = float,\n",
        "        metavar = 'D',\n",
        "        help = 'dropout probability in the masked_lm pooler layer',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # 학습해야 하는 positional embedding의 수\n",
        "        '--max-positions',\n",
        "        type = int,\n",
        "        help = 'number of positional embeddings to learn',\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # checkpoint를 불러올 때 head를 등록 및 불러오기\n",
        "        \"--load-checkpoint-heads\",\n",
        "        action=\"store_true\",\n",
        "        help=\"(re-)register and load heads when loading checkpoints\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # RoBERTa에서 embedding과 분류기 간에 가중치를 untie\n",
        "        \"--untie-weights-roberta\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Untie weights between embeddings and classifiers in RoBERTa\",\n",
        "    )\n",
        "    # 구조화된 dropout으로 Transformer 디맨드 온디맨드 감소\n",
        "    parser.add_argument(\n",
        "        # encoder에 대한 LayerDropout 확률률\n",
        "        \"--encoder-layerdrop\",\n",
        "        type=float,\n",
        "        metavar=\"D\",\n",
        "        default=0,\n",
        "        help=\"LayerDrop probability for encoder\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        # \n",
        "        \"--encoder-layers-to-keep\",\n",
        "        default=None,\n",
        "        help=\"which layers to *keep* when pruning as a comma-separated list\",\n",
        "    )\n",
        "    # args for Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)\n",
        "    parser.add_argument(\n",
        "        \"--quant-noise-pq\",\n",
        "        type=float,\n",
        "        metavar=\"D\",\n",
        "        default=0,\n",
        "        help=\"iterative PQ quantization noise at training time\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--quant-noise-pq-block-size\",\n",
        "        type=int,\n",
        "        metavar=\"D\",\n",
        "        default=8,\n",
        "        help=\"block size of quantization noise at training time\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--quant-noise-scalar\",\n",
        "        type=float,\n",
        "        metavar=\"D\",\n",
        "        default=0,\n",
        "        help=\"scalar quantization noise and scalar quantization at training time\",\n",
        "    )\n",
        "    # args for \"Better Fine-Tuning by Reducing Representational Collapse\" (Aghajanyan et al. 2020)\n",
        "    parser.add_argument(\n",
        "        \"--spectral-norm-classification-head\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"Apply spectral normalization on the classification head\",\n",
        "    )\n",
        "    # args for Fully Sharded Data Parallel (FSDP) training\n",
        "    parser.add_argument(\n",
        "        \"--min-params-to-wrap\",\n",
        "        type=int,\n",
        "        metavar=\"D\",\n",
        "        default=DEFAULT_MIN_PARAMS_TO_WRAP,\n",
        "        help=(\n",
        "            \"minimum number of params for a layer to be wrapped with FSDP() when \"\n",
        "            \"training with --ddp-backend=fully_sharded. Smaller values will \"\n",
        "            \"improve memory efficiency, but may make torch.distributed \"\n",
        "            \"communication less efficient due to smaller input sizes. This option \"\n",
        "            \"is set to 0 (i.e., always wrap) when --checkpoint-activations or \"\n",
        "            \"--offload-activations are passed.\"\n",
        "        ),\n",
        "    )\n",
        "    # args for AdaPruning\n",
        "    # In short, it adds regularizarion for the multihead attention module and feed forward neural nets\n",
        "    # For more details, please refer to the paper https://openreview.net/forum?id=_CMSV7FTzGI\n",
        "    parser.add_argument(\n",
        "        \"--mha-reg-scale-factor\",\n",
        "        type=float,\n",
        "        metavar=\"D\",\n",
        "        default=0.0,\n",
        "        help=\"scaling factor for regularization term in adptive pruning, recommendation is 0.000375\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--ffn-reg-scale-factor\",\n",
        "        type=float,\n",
        "        metavar=\"D\",\n",
        "        default=0.0,\n",
        "        help=\"scaling factor for regularization term in adptive pruning, recommendation is 0.000375\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--mha-heads-to-keep\",\n",
        "        type=int,\n",
        "        metavar=\"D\",\n",
        "        default=-1,\n",
        "        help=\"number of heads to keep in each multi-head attention module, -1 means keeping all heads\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--ffn-blocks-to-remove\",\n",
        "        type=int,\n",
        "        metavar=\"D\",\n",
        "        default=-1,\n",
        "        help=\"number of feedforward blocks to remove in each transformer layer, -1 means keeping all ffn blocks\",\n",
        "    )\n",
        "\n",
        "  @classmethod\n",
        "  def build_model(cls, args, task):\n",
        "    # 새로운 모델 instance 짓기\n",
        "    from omegaconf import OmegaConf\n",
        "\n",
        "    if OmegaConf.is_config(args):\n",
        "      OmegaConf.set_struct(args, False)\n",
        "\n",
        "    # 모든 인자들이 존재한다는 것을 확인시켜줌\n",
        "    base_architecture(args)\n",
        "\n",
        "    if not safe_hasattr(args, 'max_positions'):\n",
        "      if not safe_hasattr(args, 'tokens_per_sample'):\n",
        "        args.tokens_per_sample = task.max_positions()\n",
        "      args.max_positions = args.tokens_per_sample\n",
        "\n",
        "    encoder = RobertaEncoder(args, task.source_dictionary)\n",
        "\n",
        "    if OmegaConf.is_config(args):\n",
        "      OmegaConf.set_struct(args, True)\n",
        "\n",
        "    return cls(args, encoder)\n",
        "\n",
        "  def forward(self, src_tokens, features_only = False, return_all_hiddens = False,\n",
        "              classification_head_name = None, **kwargs):\n",
        "    if classification_head_name = None:\n",
        "      features_only = True\n",
        "\n",
        "    x, extra = self.encoder(src_tokens, features_only, return_all_hiddens, **kwargs)\n",
        "\n",
        "    if classification_head_name is not None:\n",
        "      x = self.classification_heads[classification_head_name](x)\n",
        "    return x, extra\n",
        "\n",
        "  def _get_adaptive_head_loss(self):\n",
        "    norm_loss = 0\n",
        "    scaling = float(self.args.mha_reg_scale_factor)\n",
        "    for layer in self.encoder.sentence_encoder.layers:\n",
        "      norm_loss_layer = 0\n",
        "      for i in range(layer.self_attn.num_heads):\n",
        "        start_idx = i * layer.self_attn.head_dim\n",
        "        end_idx = (i + 1) * layer.self_attn.head_dim\n",
        "        norm_loss_layer += scaling * (\n",
        "            torch.sum(torch.abs(layer.self_attn.q_proj.weight[start_idx:end_idx,])) + torch.sum(\n",
        "              torch.abs(layer.self_attn.q_proj.bias[start_idx:end_idx])\n",
        "            )\n",
        "        )\n",
        "        norm_loss_layer += scaling * (\n",
        "            torch.sum(torch.abs(layer.self_attn.k_proj.weight[start_idx:end_idx,])) + torch.sum(\n",
        "                torch.abs(layer.self_attn.v_proj.bias[start_idx:end_idx])\n",
        "            )\n",
        "        )\n",
        "\n",
        "      norm_loss += norm_loss_layer\n",
        "\n",
        "    return norm_loss\n",
        "\n",
        "  def _get_adaptive_ffn_loss(self):\n",
        "    ffn_scale_factor = float(self.args.ffn_reg_scale_factor)\n",
        "    filter_loss = 0\n",
        "    for layer in self.encoder.sentence_encoder.layers:\n",
        "      filter_loss += torch.sum(\n",
        "          torch.abs(layer.fc1.weight * ffn_scale_factor)\n",
        "      ) + torch.sum(torch.abs(layer.fc2.weight * ffn_scale_factor))\n",
        "      filter_loss += torch.sum(\n",
        "          torch.abs(layer.fc1.bias * ffn_scale_factor)\n",
        "      ) + torch.sum(torch.abs(layer.fc2.bias * ffn_scale_factor))\n",
        "\n",
        "    return filter_loss\n",
        "\n",
        "  def get_normalizer_probs(self, net_output, log_probs, sample =  None):\n",
        "    # 네트워크의 출력으로부터 normalized 확률을 얻음\n",
        "    logits = net_output[0].float()\n",
        "    if log_probs:\n",
        "      return F.log_softmax(logits, dim = -1)\n",
        "    else:\n",
        "      return F.softmax(logits, dim = -1)\n",
        "\n",
        "  def register_classification_head(\n",
        "      self, name, num_classes = None, inner_dim = None, **kwargs\n",
        "  ):\n",
        "    # 분류 gead를 등록하기\n",
        "    if name in self.classification_heads:\n",
        "      prev_num_classes = self.classification_heads[name].out_proj.out_features\n",
        "      prev_inner_dim = self.classification_heads[name].dense.out_features\n",
        "      if num_classes != prev_num_classes or inner_dim != prev_inner_dim:\n",
        "        logger.warning(\n",
        "                    're-registering head \"{}\" with num_classes {} (prev: {}) '\n",
        "                    \"and inner_dim {} (prev: {})\".format(\n",
        "                        name, num_classes, prev_num_classes, inner_dim, prev_inner_dim\n",
        "                    )\n",
        "                )\n",
        "      self.classification_heads[name] = RobertaClassificationHead(\n",
        "          input_dim = self.args.encoder_embed_dim,\n",
        "          inner_dim = inner_dim or self.args.encoder_embed_dim,\n",
        "          num_classes = num_classes,\n",
        "          activation_fn = self.args.pooler_activation_fn,\n",
        "          pooler_dropout = self.args.pooler_dropout,\n",
        "          q_noise = self.args.quant_noise_pq,\n",
        "          qn_block_size = self.args.quant_noise_pq_block_size,\n",
        "          do_spectral_norm = self.args.spectral_norm_classification_head,\n",
        "      )\n",
        "\n",
        "    @property\n",
        "    def supported_targets(self):\n",
        "      return {'self'}\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls, model_naem_or_path, check_point_file = 'model.pt', data_name_or_path = '.',\n",
        "        bpe = 'gpt2', **kwargs,\n",
        "    ):\n",
        "      from fairseq import hub_utils\n",
        "      \n",
        "      x = hub_utils.from_pretrained(\n",
        "          model_name_or_path, checkpoint_file, data_name_or_path,\n",
        "          archive_map = cls.hub_models(), bpe = bpe,\n",
        "          load_checkpoint_heads = True, **kwargs,\n",
        "      )\n",
        "\n",
        "      logger.info(x['args'])\n",
        "      return RobertaHubInterface(x['args'], x['task'], x['models'][0])\n",
        "\n",
        "    def upgrade_state_dict_names(self, state_dict, name):\n",
        "      prefix = name + '.' if name != '' else ''\n",
        "\n",
        "      # decoder 재작명 -> children 모듈을 업그레이드하기 전의 encoder\n",
        "      for k in list(state_dict.keys()):\n",
        "        if k.startswith(prefix + 'decoder'):\n",
        "          new_k = prefix + 'encoder' + k[len(prefix + 'decoder') :]\n",
        "          state_dict[new_k] = state_dict[k]\n",
        "          del state_dict[k]\n",
        "\n",
        "      # emb_layer_norm 재작명 -> layernorm_embedding\n",
        "      for k in list(state_dict.keys()):\n",
        "        if '.emb_layer_norm.' in k:\n",
        "          new_k = k.replace('.emb_layer_norm.', '.layernorm_embedding.')\n",
        "          state_dict[new_k] = state_dict[k]\n",
        "          del state_dict[k]\n",
        "\n",
        "      # children module 업그레이드\n",
        "      super().upgrade_state_dict_named(state_dict, name)\n",
        "\n",
        "      # state dict에서 표현된 분류 헤드 다루기\n",
        "      current_head_names = (\n",
        "          [] if not hasattr(self, 'classification_heads')\n",
        "          else self.classification_head.keys()\n",
        "      )\n",
        "      keys_to_delete = []\n",
        "      for k in state_dict.keys():\n",
        "        if not k.startswith(prefix + 'classification_heads.'):\n",
        "          continue\n",
        "\n",
        "        head_name = k[len(prefix + 'classification_heads.') :].split('.')[0]\n",
        "        num_classes = state_dict[\n",
        "            prefix + 'classification_heads.' + head_name + '.out_proj.weight'\n",
        "        ].size(0)\n",
        "        inner_dim = state_dict[\n",
        "            prefix + 'classification_heads.' + head_name + '.dense.weight'\n",
        "        ].size(0)\n",
        "\n",
        "        if getattr(self.args, 'load_checkpoint_heads', False):\n",
        "          if head_name not in current_head_names:\n",
        "            self.register_classification_head(head_name, num_classes, inner_dim)\n",
        "        else:\n",
        "          if head_name not in current_head_names:\n",
        "            logger.warning(\n",
        "                'deleting classification head ({}) from checkpoint not present in current model: {}'.format(head_name, k)\n",
        "            )\n",
        "            keys_to_delete.append(k)\n",
        "          elif (\n",
        "              num_classes != self.classification_head[head_name].out_proj.out_features or\n",
        "              inner_dim != self.classification_head[head_name].dense.out_features\n",
        "          ):\n",
        "            logger.warning(\n",
        "                        \"deleting classification head ({}) from checkpoint \"\n",
        "                        \"with different dimensions than current model: {}\".format(\n",
        "                            head_name, k\n",
        "                        )\n",
        "                    )\n",
        "            keys_to_delete.append(k)\n",
        "\n",
        "      for k in keys_to_delete:\n",
        "        del state_dict[k]\n",
        "\n",
        "      # 새롭게 추가된 classification head를 현재 가중치와 함께 state dict에 복사\n",
        "      if hasattr(self, 'classification_heads'):\n",
        "        cur_state = self.classification_heads.state_dict()\n",
        "        for k, v in cur_state.items():\n",
        "          if prefix + 'classification_heads.' + k not in state_dict:\n",
        "            logger.info('Overwriting ' + prefix + 'classification_heads.' + k)\n",
        "            state_dict[prefix + 'classification_heads.' + k] = v\n",
        "\n",
        "        # data2vec 모델 적용\n",
        "        if (\n",
        "            'encoder._ema' in state_dict and 'encoder.lm_head.weight' not in state_dict\n",
        "        ):\n",
        "          lm_state = self.encoder.lm_head.state_dict()\n",
        "          for k, v in lm_state.items():\n",
        "            state_dict['encoder.lm_head.' + k] = v\n",
        "\n",
        "        for k in list(state_dict.keys()):\n",
        "          if k.startswith('encoder.regression_head') or k == 'encoder._ema':\n",
        "            del state_dict[k]\n",
        "\n",
        "class RobertaLMHead(nn.Module):\n",
        "  # MLM을 위한 Head\n",
        "  def __init__(self, embed_dim, output_dim, activation_fn, weight = None):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(embed_dim, embed_dim)\n",
        "    self.activation_fn = utils.get_activation_fn(activation_fn)\n",
        "    self.layer_norm = LayerNorm(embed_dim)\n",
        "\n",
        "    if weight is None:\n",
        "      weight = nn.Linear(embed_dim, output_dim, bias = False).weight\n",
        "    self.weight - weight\n",
        "    self.bias = nn.Parameter(torch.zeros(output_dim))\n",
        "\n",
        "  def forward(self, features, masked_tokens = None, **kwargs):\n",
        "    # 학습 중에만 masked token을 투영\n",
        "    # 메모리와 계산량을 아낌\n",
        "    if masked_tokens is not None:\n",
        "      features = feature[masked_token, :]\n",
        "\n",
        "    x = self.dense(features)    # dense layer\n",
        "    x = self.activation_fn(x)   # 활성화 함수\n",
        "    x = self.layer_norm(x)      # layer normalization\n",
        "    # bias를 사용하여 voca의 크기로 다시 투영\n",
        "    x = F.Linear(x, self.weight) + self.bias\n",
        "    return x\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "  # sentence-level classification task를 위한 Head\n",
        "\n",
        "  def __init__(self, input_dim, inner_dim, num_classes, activation_fn,\n",
        "               pooler_dropout, q_noise = 0, qn_block_size = 8, do_spectral_norm = False,):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(input_dim, inner_dim)\n",
        "    self.activation_fn = utils.get_actiavtion_fn(activation_fn)\n",
        "    self.dropout = nn.Dropout(p = pooler_dropout)\n",
        "    self.out_proj = apply_quant_noise_(\n",
        "        nn.Linear(inner_dim, num_classes), q_noise, qn_block_size\n",
        "    )\n",
        "    if do_spectral_norm:\n",
        "      if q_noise != 0:\n",
        "        raise NotImplementError(\n",
        "            'Attempting to use Spectral Normalization with Quant Noise. This is not officially supported'\n",
        "        )\n",
        "      self.out_proj = torch.nn.utils.spectral_norm(self.out_proj)\n",
        "\n",
        "  def forward(self, features, **kwargs):\n",
        "    x = features[:, 0, :]   # <s> token을 받아들임 -> [CLS] 토큰과 동등\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense(x)\n",
        "    x = self.activation_fn(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "    return x\n",
        "\n",
        "class RobertaEncoder(FairseqEncoder):\n",
        "  # Roberta Encoder\n",
        "\n",
        "  def __init__(self, args, dictionary):\n",
        "    super().__init__(dictionary)\n",
        "\n",
        "    base_architecture(args)\n",
        "    self.args = args\n",
        "\n",
        "    if args.encoder_layers_to_keep:\n",
        "      args.encoder_layers = len(args.encoder_layers_to_keep.split(','))\n",
        "\n",
        "    embed_tokens = self.build_embedding(\n",
        "        len(dictionary), args.encoder_embed_dim, dictionary.pad()\n",
        "    )\n",
        "\n",
        "    self.sentence_encoder = self.build_encoder(args, dictionary, embed_tokens)\n",
        "\n",
        "    self.lm_head = self.build_lm_head(\n",
        "        embed_dim = args.encoder_embed_dim,\n",
        "        output_dim = len(dictionary),\n",
        "        activation_fn = args.activation_fn,\n",
        "        weight = (\n",
        "            self.sentence_encoder.embed_tokens.weight if not args.untie_weights_roberta else None\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  def build_embedding(self, vocab_size, embedding_dim, padding_idx):\n",
        "    return nn.EMbedding(vocab_size, embedding_dim, embed_tokens)\n",
        "\n",
        "  def build_encoder(self, args, dictionary, embed_tokens):\n",
        "    encoder = TransformerEncoder(args, dictionary, embed_tokens)\n",
        "    encoder.apply(init_ber_params)\n",
        "    return encoder\n",
        "\n",
        "  def build_lm_head(self, embed_dim, output_dim, activation_fn, weight)\n",
        "    return RobertaLMHead(embed_dim, output_dim, activation_fn, weight)\n",
        "\n",
        "  def forward(self, src_tokens, features_only - False, return_all_hiddens = False, masked_tokens = None, **unused,):\n",
        "    '''\n",
        "    Args:\n",
        "      src_tokens: 입력 토큰의 형태 '(batch, src_len)'\n",
        "      features_only(bool, optional): LM Head를 스킵하고 feature를 반환. True면 출력 형태는 '(batch, src_len, embed_dim)'\n",
        "      return_all_hiddens (bool, optional): 중간 hidden state를 모두 반환(기본값은 False)\n",
        "\n",
        "    Returns:\n",
        "     - '(batch, src_len, vocab)' 형태의 LM output\n",
        "     - 추가적 데이터의 사전, 여기서 'inner_states'는 hidden_state의 리스트.\n",
        "       hidden state의 형태는 '(src_len, batch, vocab)'의 형태\n",
        "    '''\n",
        "\n",
        "    x, extra = self.extract_feature(\n",
        "        src_tokens, return_all_hiddens = return_all_hiddens\n",
        "    )\n",
        "    if not feature_only:\n",
        "      x = self.output_layer(x, masked_tokens = masked_tokens)\n",
        "    return x, extra\n",
        "\n",
        "  def extract_features(self, src_tokens, return_all_hiddens = False, **kwargs):\n",
        "    encoder_out = self.sentence_encoder(\n",
        "        src_tokens, return_all_hiddens = return_all_hiddens,\n",
        "        token_embeddings = kwargs.get('token_embedding', None),\n",
        "    )\n",
        "    # T x B x C -> B x T x C\n",
        "    features = encoder_out['encoder_out'][0].transpose(0, 1)\n",
        "    inner_states = encoder_out['encoder_states'] if return_all_hiddens else None\n",
        "    return features, {'inner_states': inner_states}\n",
        "\n",
        "  def output_layer(self, features, masked_tokens = None, **unused):\n",
        "    return self.lm_head(features, masked_tokens)\n",
        "\n",
        "  def max_positions(self):\n",
        "    # encoder가 지원하는 최대 output length\n",
        "    return self.args.max_positions\n",
        "\n",
        "@register_model_architecture('roberta', 'roberta')\n",
        "def base_architecture(args):\n",
        "  args.encoder_layers = safe_getattr(args, 'encoder_layers', 12)\n",
        "  args.encoder_embed_dim = safe_getattr(args, 'encoder_embed_dim', 768)\n",
        "  args.encpoder_ffn_embed_dim = safe_getattr(args, 'encoder_embed_dim', 3072)\n",
        "  args.encoder_attention_head = safe_getattr(args, 'encoder_attention_heads', 12)\n",
        "\n",
        "  args.dropout = safe_getattr(args, 'dropout', 0.1)\n",
        "  args.attention_ropout = safe_getattr(args, 'attention_dropou', 0.1)\n",
        "  args.activation_dropout = safe_getattr(args, 'activation_dropout', 0.0)\n",
        "  args.pooler_dropout = safe_getattr(args, 'pooler_dropout', 0.0)\n",
        "\n",
        "  args.max_source_positions = safe_getattr(args, 'max_positions', 512)\n",
        "  args.no_token_positional_embeddings = safe_getattr(\n",
        "      args, 'no_token_positional_embeddings', False\n",
        "  )\n",
        "\n",
        "  # BERT는 기존의 Transformer에 비해 몇 가지 구조적 차이를 가지고 있다.\n",
        "  args.encoder_learned_pos = safe_getattr(args, 'encoder_learned_pos', True)\n",
        "  args.layernorm_embedding = safe_getattr(args, 'layernorm_embedding', True)\n",
        "  args.no_scale_embedding = safe_getattr(args, 'no_scale_embedding', True)\n",
        "  args.activation_fn = safe_getattr(args, 'activation_fn', 'gelu')\n",
        "  args.encoder_normalize_before = safe_getattr(\n",
        "      args, 'encoder_normalize_before', False\n",
        "  )\n",
        "  args.pooler_activation_fn = safe_getattr(args, 'pooler_activation_fn', 'tanh')\n",
        "  args.untie_weights_roberta = safe_getattr(args, 'untie_weights_roberta', False)\n",
        "\n",
        "  # 입력 설정의 조정\n",
        "  arge.adaptive_input = safe_getattr(args, 'adaptive_input', False)\n",
        "\n",
        "  # LayerDrop 설정\n",
        "  args.encoder_layerdrop = safe_getattr(args, 'encoder_layerdrop', 0.0)\n",
        "  args.encoder_layers_to_keep = safe_getattr(args, 'encoder_layers_to_keep', None)\n",
        "\n",
        "  # 양자화 noise 설정\n",
        "  args.quant_noise_pq = safe_getattr(args, 'quant_noise_pq', 0)\n",
        "  args.quant_noise_pq_block_size = safe_getattr(args, 'quant_noise_pq_block_size', 8)\n",
        "  args.quant_noise_scalar = safe_getattr(args, 'quant_noise_scalar', 0)\n",
        "\n",
        "  # R4F 설정\n",
        "  args.spectral_norm_classification_head = safe_getattr(\n",
        "      args, 'spectral_norm_classification_head', False\n",
        "  )\n",
        "\n",
        "@register_model_architecture('roberta', 'roberta_prenorm')\n",
        "def roberta_prenorm_architecture(args):\n",
        "  args.layernorm_embedding = safe_getattr(args, 'layernorm_embedding', False)\n",
        "  args.encoder_normalize_before = safe_getattr(args, 'encoder_normalize_before', True)\n",
        "  base_architecture(args)\n",
        "\n",
        "@register_model_architecture('roberta', 'roberta_base')\n",
        "def roberta_base_architecture(args):\n",
        "  base_architecture(args)\n",
        "\n",
        "@register_model_architecture('roberta', 'toberta_large')\n",
        "def roberta_large_architecture(args):\n",
        "  args.encoder_layers = safe_getattr(args, 'encoder_layers', 24)\n",
        "  args.encoder_embed_dim = safe_getattr(args, 'encoder_embed_dim', 1024)\n",
        "  args.encoder_ffn_embed_dim = safe_getattr(args, 'encoder_ffn_embed_dim', 4096)\n",
        "  args.encoder_attention_heads = safe_getattr(args, 'encoder_attention_head', 16)\n",
        "  base_architecture(args)\n",
        "\n",
        "@register_model_architecture('roberta', 'xlm')\n",
        "def xlm_architecture(args):\n",
        "  args.encoder_layers = safe_getattr(args, 'encoder_layers', 16)\n",
        "  args.encoder_embed_dim = safe_getattr(args, 'encoder_embed_dim', 1280)\n",
        "  args.encoder_ffn_embed_dim = safe_getattr(args, 1280 * 4)\n",
        "  args.encoder_attention_heads = safe_getattr(args, 'encoder_attention_heads', 16)\n",
        "  base_architecture(args)"
      ]
    }
  ]
}
