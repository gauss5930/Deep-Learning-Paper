{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKbH7m1JZAlSm0wYEpQ5AJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauss5930/Natural-Language-Processing/blob/main/BERT/BERT_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0EfRZ8qrSIz"
      },
      "outputs": [],
      "source": [
        "#Main BERT model\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import prinf_function\n",
        "\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "import six\n",
        "import tensorflow as tf\n",
        "\n",
        "class BertConfig(object):\n",
        "  #BERT의 구성\n",
        "\n",
        "  def __init__(self, vocab_size, hidden_size = 768, num_hidden_layers = 12, num_attention_heads = 12,\n",
        "               intermediate_size = 3072, hidden_act = 'gelu', hidden_dropout_prob = 0.1,\n",
        "               attention_probs_dropout_prob = 0.1, max_position_embedding = 512, type_vocab_size = 16, \n",
        "               initializer_range = 0.02):\n",
        "    \n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.hidden_act = hidden_act\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "    self.max_position_embeddings = max_position_embeddings\n",
        "    self.type_vocab_size = type_vocab_size\n",
        "    self.initializer_range = initializer_range\n",
        "\n",
        "  @classmethod\n",
        "  def from_dict(cls, json_objects):\n",
        "    #Python dictionary에서 BertConfig 구성\n",
        "    config = BertConfig(vocab_size = None)\n",
        "    for (ket, value) in six.iteritems(json_object):\n",
        "      config.__dict__[key] = value\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_json_string(cls, json_object):\n",
        "    #json에서 BertConfig 구성\n",
        "    with tf.gfile.GFile(json_file, 'r') as reader:\n",
        "      text = reaser.read()\n",
        "    return cls.from_dict(json.loads(text))\n",
        "\n",
        "  def to_dict(self):\n",
        "    #instance를 Python dictionary로 시리얼화\n",
        "    output = copy.deepcopy(self.__dict__)\n",
        "    return output\n",
        "\n",
        "  def to_json_string(self):\n",
        "    #instance를 json string으로 시리얼화\n",
        "    return json.dumps(self.to_dict(), indent = 2, sort_keys = True) + '\\n'\n",
        "\n",
        "\n",
        "class BertModel(object):\n",
        "  #BERT Model\n",
        "\n",
        "  #Example usage:\n",
        "  '''python\n",
        "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
        "  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
        "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
        "  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
        "    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "  model = modeling.BertModel(config=config, is_training=True,\n",
        "    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
        "  label_embeddings = tf.get_variable(...)\n",
        "  pooled_output = model.get_pooled_output()\n",
        "  logits = tf.matmul(pooled_output, label_embeddings)'''\n",
        "\n",
        "  def __init__(self, config, is_training, input_ids, input_mask = None, token_type_ids = None,\n",
        "               use_one_hot_embeddings = False, scope = None):\n",
        "    #is_training: training model에 대해서는 True, eval model에 대해서는 False. dropout이 적용될 지 결정\n",
        "    #use_one_hot_embedding: word embedding으로 one-hot word embeddings 또는 tf.embedding_lookup() 중 결정\n",
        "    #scope: variable scope. 기본값은 'bert'\n",
        "\n",
        "    config = copy.deepcopy(config)\n",
        "    if not is_training:\n",
        "      config.hidden_dropout_prob = 0.0\n",
        "      config.attention_probs_dropout_prob = 0.0\n",
        "\n",
        "    input_shape = get_shape_list(input_ids, expected_rank = 2)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_len = input_shape[1]\n",
        "\n",
        "    if input_mask is None:\n",
        "      input_mask = tf.ones(shape = [batch_size, seq_length], dtype = tf.int32)\n",
        "\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = tf.zeros(shape = [batch_size, seq_length], dtype = tf.int32)\n",
        "\n",
        "    with tf.variable_scope(scope, default_name = 'bert'):\n",
        "      with tf.variable_scope('embeddings'):\n",
        "        #word ids에 대해 embedding lookup 수행\n",
        "        (self.embedding_output, self.embedding_table) = embedding_lookup(\n",
        "            input_ids = input_ids, vocab_size = config.vocab_size,\n",
        "            embedding_size = config.hidden_size,\n",
        "            initializer_range = config.initializer_range,\n",
        "            word_embedding_name = 'word_embeddings',\n",
        "            use_one_hot_embeddings = use_one_hot_embeddings\n",
        "        )\n",
        "\n",
        "        #1. positional embedding 추가\n",
        "        #2. token type embedding 진행\n",
        "        #3. layer normalization 진행\n",
        "        #4. dropout 진행\n",
        "        self.embedding_output = embedding_postprocessor(\n",
        "            input_tensor = self.embedding_output,\n",
        "            use_token_type = True,\n",
        "            token_type_ids = token_type_ids,\n",
        "            token_type_vocab_size = config.type_vocab_size,\n",
        "            token_type_embedding_name = 'token_type_embeddings',\n",
        "            use_position_embeddings = True,\n",
        "            position_embedding_name = 'position_embeddings',\n",
        "            initializer_range = config.initializer_range,\n",
        "            max_position_embedding = config.max_position_embeddings,\n",
        "            dropout_prob = config.hidden_dropout_prob\n",
        "        )\n",
        "\n",
        "        with tf.variable_scope('encoder'):\n",
        "          #2D mask [batch_size, seq_length]를 3D mask [batch_size, seq_length, seq_length]로\n",
        "          #변형. attention score를 계산하기 위해\n",
        "          attention_mask = create_attention_mask_from_input_mask(input_ids, input_mask)\n",
        "\n",
        "          #쌓여진 Transformer 실행\n",
        "          self.all_encoder_layers = transformer_model(\n",
        "              input_tensor = self.embedding_output,\n",
        "              attention_mask = attention_mask,\n",
        "              hidden_size = config.num_hidden_layers,\n",
        "              num_hidden_layers = config.num_hidden_layers,\n",
        "              num_attention_heads = config.num_attention_heads,\n",
        "              intermediate_act_fn = get_activation(config.hidden_act),\n",
        "              hidden_dropout_prob = config.hidden_dropout_prob,\n",
        "              attention_probs_dropout_prob = config.attention_probs_dropout_prob,\n",
        "              initializer_range = config.initializer_range,\n",
        "              do_return_all_layers = True\n",
        "          )\n",
        "\n",
        "          self.sequence_output = self.all_encoder_layers[-1]\n",
        "          with tf.variable_scope('pooler'):\n",
        "            #모델을 '풀링'해서 첫 번째 토큰에 해당하는 hidden state를 추출.\n",
        "            #논문에서는 이것이 pre-trained 되어 있을 것이라고 추정\n",
        "            first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis = 1)\n",
        "            self.pooled_output = tf.layers.dense(\n",
        "                first_token_tensor, config.hidden_size,\n",
        "                activation = tf.tanh, \n",
        "                kernel_initializer = create_initializer(config.initializer_range)\n",
        "            )\n",
        "\n",
        "  def get_pooled_output(self):\n",
        "    return self.pooled_output\n",
        "\n",
        "  def get_sequence_output(self):\n",
        "    #encoder의 마지막 hidden layer를 얻음\n",
        "    return self.sequence_output\n",
        "\n",
        "  def get_all_encoder_layers(self):\n",
        "    return self.all_encoder_layers\n",
        "\n",
        "  def get_embedding_output(self):\n",
        "    #embedding lookup의 output을 얻음\n",
        "    return self.embedding_output\n",
        "\n",
        "  def get_embedding_table(self):\n",
        "    return self.embedding_table\n",
        "\n",
        "def gelu(x):\n",
        "  #GeLU 선언\n",
        "  cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "  return x * cdf\n",
        "\n",
        "def get_activation(activation_string):\n",
        "  #string을 파이썬 함수로 변환\n",
        "  if not isinstance(activation_string, six.string_types):\n",
        "    return activation_string\n",
        "\n",
        "  if not activation_string:\n",
        "    return None\n",
        "\n",
        "  act = activation_string.lower()\n",
        "  if act == 'inear':\n",
        "    return None\n",
        "  elif act == 'relu':\n",
        "    return tf.nn.relu\n",
        "  elif act == 'gelu'\n",
        "    return gelu\n",
        "  elif act == 'tanh':\n",
        "    return tf.tanh\n",
        "  else:\n",
        "    raise Valueerror('unsopkiscated activatio: %s' % act)\n",
        "\n",
        "def get_assingment_map_from_chekcpoint(tvars, init_checkpoint):\n",
        "  assingment_map = {}\n",
        "  initializer_variable_names = {}\n",
        "\n",
        "  name_to_variable = collections.OrderedDict()\n",
        "  for var in tvars:\n",
        "    name = var.name\n",
        "    m = re.match(\"^(.*):\\\\d+$\", name)\n",
        "    if m is not None:\n",
        "      name = m.group(1)\n",
        "    name_to_variable[name] = var\n",
        "\n",
        "  init_vars = tf.train.list_variables(init_checkpoint)\n",
        "\n",
        "  assignment_map = collections.OrderedDict()\n",
        "  for x in init_vars:\n",
        "    (name, var) = (x[0], x[1])\n",
        "    if name not in name_to_variable:\n",
        "      continue\n",
        "    assignment_variable_names[name] = 1\n",
        "    initialized_variable_names[name + ':0'] = 1\n",
        "\n",
        "  return (assignment_map, initializer_variable_names)\n",
        "\n",
        "def dropout(input_tensor, dropout_prob):\n",
        "  #dropout 수행\n",
        "  if dropout_prob is None or dropout_prob == 0.0:\n",
        "    return input_tensor\n",
        "  \n",
        "  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
        "  return output\n",
        "\n",
        "def layer_norm(input_tensor, name = None):\n",
        "  #tensor의 마지막 차원에 layer normalization 실행\n",
        "  return tf.contrib.layers.layer_norm(inputs = input_tensor, begin_norm_axis = -1,\n",
        "                                      begin_params_axis = -1, scope = name)\n",
        "  \n",
        "def layer_norm_and)dropout(input_tensor, dropout_prob, name = None):\n",
        "  #layer normalization 수행 후 dropout 수행\n",
        "  output_tensor = layer_norm(input_tensor, name)\n",
        "  output_tensor - dropout(output_tensor, dropout_prob)\n",
        "  return output_tensor\n",
        "\n",
        "def create_initializer(initializer_range = 0.02):\n",
        "  #주어진 범위에서 'truncated_normal_initializer' 수행\n",
        "  return tf.truncated_normal_initializer(stddev = initializer_range)\n",
        "\n",
        "def embedding_lookup(input_ids, vocab_size, embedding_size = 128, initializer_range = 0.02,\n",
        "                     word_embedding_name = 'word_embeddings', use_one_hot_embeddings = False):\n",
        "  #id tensor에 대한 word embedding 찾기\n",
        "  #만약 입력이 2D tensor [batch_size, seq_length]이면,\n",
        "  #[batch_size, seq_length, 1]로 변환\n",
        "  if input_ids.shape.ndims == 2:\n",
        "    input_ids = tf.expand_dims(input_ids, axis = [-1])\n",
        "\n",
        "  embedding_table = tf.get_variable(name = word_embedding_name, shape = [vocab_size, embedding_size],\n",
        "                                    initializer = create_initializer(initializer_range))\n",
        "  \n",
        "  flat_input_ids = tf.reshape(input_ids, [-1])\n",
        "  if use_one_hot_embeddings:\n",
        "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth = vocab_size)\n",
        "    output = tf.matmul(one_hot_input_ids, embedding_table)\n",
        "  else:\n",
        "    output = tf.gather(embedding_table, flat_input_ids)\n",
        "\n",
        "  input_shape = get_shape_list(input_ids)\n",
        "\n",
        "  output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
        "  return (output, embedding_table)\n",
        "\n",
        "def embedding_postprocessor(input_tensor, use_token_type = False, token_type_ids = None,\n",
        "                            token_type_vocab_size = 16, token_type_embedding_name = 'token_type_embedding',\n",
        "                            use_position_embeddings = True, position_mebedding_name = 'position_embeddings',\n",
        "                            initializer_range = 0.02, max_position_embeddings = 512, dropout_prob = 0.1):\n",
        "  #word tensor에 대해 다양한 post-processing 수행\n",
        "\n",
        "  input_shape = get_shaep_list(input_tensor, expected_rank = 3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  width = input_shape[2]\n",
        "\n",
        "  output = input_tensor\n",
        "\n",
        "  if use_token_type:\n",
        "    if token_type_ids is None:\n",
        "      raise ValueError(\"`token_type_ids` must be specified if\"\n",
        "                       \"`use_token_type` is True.\")\n",
        "    token_type_table = tf.get_variable(\n",
        "        name = token_type_embedding_name,\n",
        "        shape = [token_type_vocab_size, width],\n",
        "        initializer = create_initializer(initializer_range)\n",
        "    )\n",
        "    #vocav이 작을 것이기 때문에, one-hot을 항상 여기서 한다.\n",
        "    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
        "    one_hot_ids = tf.one_hot(flat_token_type_ids, depth = token_type_vocab_size)\n",
        "    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
        "    token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width])\n",
        "    output += token_type_embeddings\n",
        "\n",
        "  if use_position_embeddings:\n",
        "    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
        "    with tf.control_dependencies([assert_op]):\n",
        "      full_position_embeddings = tf.get_variable(name = position_embedding_name,\n",
        "                                                 shape = [max_position_embeddings, width],\n",
        "                                                 initializer = create_initializer(initializer_range))\n",
        "      #position embedding은 학습된 값이기 때문에, 긴 길이의 문장을 사용해서 'max_position_length'를 생성\n",
        "      #실제 문장 길이는 빠른 학습을 위해 긴 길이의 시퀀스를 가지면 안 되기 때문에, 이 보다는 짧을 것이다.\n",
        "\n",
        "      #그래서, 'full-position_embedding'이 embedding table로 효과적이다.\n",
        "      posision_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])\n",
        "      num_dims = len(output.shape.as_list())\n",
        "\n",
        "      position_broadcast_shape = []\n",
        "      for _ in range(num_dims - 2):\n",
        "        position_broadcast_shape.append(1)\n",
        "      position_broadcast_shape.extend([seq_length, width])\n",
        "      position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape)\n",
        "      output += position_embeddings\n",
        "\n",
        "  output = layer_norm_and_dropout(output, dropout_prob)\n",
        "  return output\n",
        "\n",
        "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
        "  #2D tensor mask로부터 3D attention mask 생성\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank = [2, 3])\n",
        "  batch_size = from_shape[0]\n",
        "  from_seq_length = from-shape[1]\n",
        "\n",
        "  to_shape = get_shape_list(to_mask, expected_rank = 2)\n",
        "  to_seq_length = to_shape[1]\n",
        "\n",
        "  to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
        "\n",
        "  broadcast_ones = tf.ones(shape = [batch_size, from_seq_length, 1], dtype = tf.float32)\n",
        "\n",
        "  #두 개의 차원과 함께 broadcast해서 mask를 생성\n",
        "  mask = broadcast_ones * to_mask\n",
        "\n",
        "  return mask\n",
        "\n",
        "def attention_layer(from_tensor, to_tensor, attention_mask = None, num_attention_heads = 1,\n",
        "                    size_per_head = 512, query_act = None, key_act = None, value_act = None,\n",
        "                    attention_probs_dropout_prob = 0.0, initializer_range = 0.02,\n",
        "                    do_return_2d_tensor = False, batch_size = None, from_seq_length = None,\n",
        "                    to_seq_length = None):\n",
        "  #from_tensor로부터 to_tensor로 multi-headed attention 수행\n",
        "\n",
        "  #이것은 Transformer에 기반이 되는 multi-headed attention의 응용이다.\n",
        "  #만약, from_tensor와 to_tensor가 똑같다면, self-attention이다.\n",
        "\n",
        "  #이 함수는 처음에 from_tensor을 'query' tensor로 project하고, to_tensor을 'key'와 'value'에 project함.\n",
        "\n",
        "  #그 다음에, query tensor와 key tensor는 dot-product된 이후에 scale됌. 그리고 softmax를 통해 attention probabilties 획득\n",
        "  #이 attention probabilities에 value tensor가 삽입되고, 하나의 tensor로 concatenate되고 반환\n",
        "\n",
        "  def transpose_for_scores(input_tensor, batch_size, num_attention_heads, seq_length, whidth):\n",
        "    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
        "\n",
        "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
        "    return output_tensor\n",
        "\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank = [2, 3])\n",
        "  to_shape = get_shape_list(to_tensor, expected_rank = [2, 3])\n",
        "\n",
        "  if len(from_shape) != len(to_shape):\n",
        "    raise ValueError('The rank of \"from_tensor\" must the rank of \"to_tensor\".')\n",
        "\n",
        "  if len(from_shape) == 3:\n",
        "    batch_size = from_shape[0]\n",
        "    from_seq_length = from_shape[1]\n",
        "    to_seq_length = to_shapep[1]\n",
        "  elif len(from_shape) == 2:\n",
        "    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
        "      raise ValueError(\n",
        "          \"When passing in rank 2 tensors to attention_layer, the values \"\n",
        "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
        "          \"must all be specified.\")\n",
        "      \n",
        "  #스칼라 차원\n",
        "  #B = bathc_size(시퀀스 수)\n",
        "  #F = from_tensor 시퀀스 길이\n",
        "  #T = to_tensor 시퀀스 길이\n",
        "  #N = num_attention_heads\n",
        "  #H = size_per_head\n",
        "\n",
        "  from_tensor_2d = reshape_to_matrix(from_tensor)\n",
        "  to_tensor_2d = reshape_to_matrix(to_tensor)\n",
        "\n",
        "  #query layer -> [B*F, N*H]\n",
        "  query_layer = tf.layers.dense(from_tensor_2d, num_attention_heads * size_per_head,\n",
        "                                activation = query_act, name = 'query',\n",
        "                                kernel_initializer = create_initializer(initializer_range))\n",
        "  \n",
        "  #key layer -> [B*F, N*H]\n",
        "  key_layer = tf.layers.dense(from_tensor_2d, num_attention_heads * size_per_head,\n",
        "                              activation = key_act, name = 'key',\n",
        "                              kernel_initializer = create_initializer(initializer_range))\n",
        "  \n",
        "  #value layer -> [B*T, N*H]\n",
        "  value_layer = tf.layers.dense(to_tensor_2d, num_attention_heads * size_per_head,\n",
        "                                activation = value_act, name = 'value',\n",
        "                                kernel_initializer = create_initializer(initializer_range))\n",
        "  #query layer -> [B, N, F, H]\n",
        "  query_layer = transpose_for_scores(query_layer, batch_size, num_attention_heads,\n",
        "                                     from_seq_length, size_per_head)\n",
        "  \n",
        "  #key layer -> [B, N, T, H]\n",
        "  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n",
        "                                   to_seq_length, size_per_head)\n",
        "  \n",
        "  #query와 key 사이에 dot-product를 진행해서 raw를 얻음\n",
        "  #attention socres = [B, N, F, T]\n",
        "  attention_scores = tf.matmul(query_layer, key_layer, transpose_b = True)\n",
        "  attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head)))\n",
        "\n",
        "  if attention_mask is not None:\n",
        "    #attention_mask = [B, 1, F, T]\n",
        "    attention_mask = tf.expand_dims(attention_mask, axis = [1])\n",
        "\n",
        "    #Attention_mask는 참석하려는 위치에 대해 1.0이고 마스크된 위치에 대해 0.0이므로 \n",
        "    #이 작업은 참석하려는 위치에 대해 0.0, 마스크된 위치에 대해 -10000.0인 텐서를 생성한다.\n",
        "    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * - 10000.0\n",
        "\n",
        "    #논문에서는 softmax 이전에 raw score에 이 값을 더하기 때문에, 모두 없애는 것과 동일하게 효과적이다.\n",
        "    attention_scores += adder\n",
        "\n",
        "  #attention socre를 normalize\n",
        "  #attention_probs = [B, N, F, T]\n",
        "  attention_probs = tf.nn.softmax(attention_scores)\n",
        "\n",
        "  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
        "\n",
        "  #value layer = [B, T, N, H]\n",
        "  value_layer = tf.reshape(value_layer, [batch_size, to_seq_length, num_attention_probs_dropout_prob])\n",
        "\n",
        "  #value_layer = [B, N, T, H]\n",
        "  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
        "\n",
        "  #context_layer = [B, N, F, H]\n",
        "  context_layer = tf.matmul(attention_probs, value_layer)\n",
        "\n",
        "  #context_layer = [B, F, N, H]\n",
        "  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
        "\n",
        "  if do_return_2d_tensor:\n",
        "    #context_layer = [B*F, N*H]\n",
        "    context_layer = tf.reshape(context_layer, [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
        "  else:\n",
        "    #context_layer = [B, F, N*H]\n",
        "    context_layer = tf.reshape(context_layer, [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
        "\n",
        "  return context_layer\n",
        "\n",
        "def transformer_model(input_tensor, \n",
        "                      attention_mask = None,\n",
        "                      hidden_size = 768,\n",
        "                      num_hidden_layers = 12,\n",
        "                      intermediate_size = 3072,\n",
        "                      intermediate_act_fn = gelu,\n",
        "                      hidden_dropout_prob = 0.1,\n",
        "                      attention_probs_dropout_prob = 0.1,\n",
        "                      initializer_range = 0.02,\n",
        "                      do_return_all_layers = False):\n",
        "  #Transformer의 multi-headed attention\n",
        "\n",
        "  if hidden_size % num_attention_heads != 0:\n",
        "    raise ValueError(\n",
        "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "        \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "    \n",
        "  attention_head_size = int(hidden_size / num_attention_heads)\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank = 3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  input_width = input_shape[2]\n",
        "\n",
        "  #Transformer는 residual sum을 모든 레이어에 대해 수행하기 때문에, 입력은 모든 hidden_size와 같아야 함\n",
        "  if input_width != hidden_size:\n",
        "    raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" %\n",
        "                     (input_width, hidden_size))\n",
        "    \n",
        "  prev_output = reshape_to_matrix(input_tensor)\n",
        "\n",
        "  all_layer_outputs = []\n",
        "  for layer_idx in range(num_hidden_layers):\n",
        "    with tf.variable_scope('layer_%d' % layer_idx):\n",
        "      layer_input = prev_output\n",
        "\n",
        "      with tf.variable_scope('attention'):\n",
        "        attention_heads = []\n",
        "        with tf.variable_scope('self'):\n",
        "          attention_head = attention_layer(\n",
        "              from_tensor = layer_input,\n",
        "              to_tensor = layer_input,\n",
        "              attention_mask = attention_mask,\n",
        "              num_attention_mask = attention_mask,\n",
        "              num_attention_heads = num_attention_heads,\n",
        "              size_per_head = attention_head_size,\n",
        "              attention_probs_dropout_prob = attention_probs_dropout_prob,\n",
        "              initializer_range = initializer_range,\n",
        "              do_return_2d_tensor = True,\n",
        "              batch_size = batch_size,\n",
        "              from_seq_length = seq_length,\n",
        "              to_seq_length = seq_length\n",
        "          )\n",
        "          attention_heads.append(attention_head)\n",
        "\n",
        "        attention_output = None\n",
        "        if len(attention_heads) == 1:\n",
        "          attention_output = attention_heads[0]\n",
        "        else:\n",
        "          #또 다른 시퀀스를 가지고 있을 경우에는, 이들을 concatenate해서 self-attention head에 넣어줌\n",
        "          attention_output = tf.concat(attention_heads, axis = -1)\n",
        "\n",
        "        #hidden size에 linear projection하고, residual을 layer_input과 함께 추가함.\n",
        "        with tf.variable_scope('output'):\n",
        "          attention_output = tf.layers.dense(attention_output, hidden_size,\n",
        "                                             kernel_initializer = create_initializer(initializer_range))\n",
        "          \n",
        "          attention_output = dropout(attention_output, hidden_dropout_prob)\n",
        "          attention_output = layer_norm(attention_outpuyt + layer_input)\n",
        "\n",
        "      #활성화 함수는 오직 중간의 히든 레이어에만 적용됌\n",
        "      with tf.variable_scope('intermediate'):\n",
        "        intermediate_output = tf.layer.dense(attention_output, intermediate_size,\n",
        "                                             activation = intermediate_act_fn,\n",
        "                                             kernel_initializer = create_initializer(initializer_range))\n",
        "        \n",
        "      #hidden_size로 다시 down-project하고, residual을 추가함\n",
        "      with tf.variable_scope('output'):\n",
        "        layer_output = tf.layers.dense(intermediate_output, hidden_size,\n",
        "                                       kernel_initializer = create_initializer(initializer_range))\n",
        "        layer_output = dropout(layer_output, hidden_dropout_prob)\n",
        "        layer_output = layer_norm(layer_output + attention_output)\n",
        "        prev_output = layer_output\n",
        "        all_layers_outputs.append(layer_output)\n",
        "\n",
        "  if do_return_all_layers:\n",
        "    final_outputs = []\n",
        "    for layer_output in all_layer_outputs:\n",
        "      final_output = reshape_from_matrix(layer_output, input_shape)\n",
        "      final_outputs.append(final_output)\n",
        "    return final_outputs\n",
        "  else:\n",
        "    final_output = reshape_from_matrix(prev_output, input_shape)\n",
        "    return final_output\n",
        "\n",
        "def get_shape_list(tensor, expected_rank = None, name = None):\n",
        "  #tensor의 shape에 대한 리스트를 반환\n",
        "\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  if expected_rank is not None:\n",
        "    assert_rank(tensor, expected_rank, name)\n",
        "\n",
        "  shape = tensor.shape.as_list()\n",
        "\n",
        "  non_static_indexes = []\n",
        "  for (index, dim) in enumerate(shape):\n",
        "    if dim is None:\n",
        "      non_static_indexes.append(index)\n",
        "\n",
        "  if not non_static_indexes:\n",
        "    return shape\n",
        "\n",
        "  dyn_shape = tf.shape(tensor)\n",
        "  for index in non_static_indexes:\n",
        "    shape[index] = dyn_shape[index]\n",
        "  return shape\n",
        "\n",
        "def reshape_to_matrix(input_tensor):\n",
        "  #랭크가 2 이상인 텐서를 랭크가 2인 텐서로 변환\n",
        "  if len(orig_shape_list) == 2:\n",
        "    return output_tensor\n",
        "\n",
        "  output_shape = get_shape_list(output_tensor)\n",
        "\n",
        "  orig_dims = orig_shape_list[0:-1]\n",
        "  width = output_shape[-1]\n",
        "\n",
        "  return tf.reshape(output_tensor, orig_dims + [width])\n",
        "\n",
        "def assert_rank(tensor, expected_rank, name = None):\n",
        "  #만약 텐서의 랭크가 예상한 랭크가 아니라면 오류 일으키기\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  expected_rank_dict = {}\n",
        "  if isinstance(expected_rank, six.integer_types):\n",
        "    expected_rank_dict[expected_rank] = True\n",
        "  else:\n",
        "    for x in expected_rank:\n",
        "      expected_rank_dict[x] = True\n",
        "\n",
        "  actual_rank = tensor.shape.ndims\n",
        "  if actual_rank not in expected_rank_dict:\n",
        "    scope_name = tf.get_variable_scope().name\n",
        "    raise ValueError(\n",
        "        \"For the tensor `%s` in scope `%s`, the actual rank \"\n",
        "        \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
        "        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))"
      ]
    }
  ]
}
