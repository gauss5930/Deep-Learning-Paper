{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUKl8M5nz7Ruoay8Dd4rCN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauss5930/Natural-Language-Processing/blob/main/ALBERT/ALBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUYl2sz4ryUY"
      },
      "outputs": [],
      "source": [
        "#https://github.com/google-research/albert/blob/master/modeling.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "import six\n",
        "from six.moves import range\n",
        "import tensorflow.compat.v1 as tf\n",
        "from tensorflow.contrib import layers as contrib_layers\n",
        "\n",
        "\n",
        "class AlbertConfig(object):\n",
        "  # ALBERT model의 구성\n",
        "\n",
        "  def __init__(self, vocab_size = 128, hidden_size = 4096, num_hidden_layers = 12,\n",
        "               num_hidden_groups = 1, num_attention_heads = 64, intermediate_size = 16384,\n",
        "               inner_group_num = 1, down_scale_factor = 1, hidden_act = 'gelu',\n",
        "               hidden_dropout_prob = 0, attention_probs_dropout_prob = 0, \n",
        "               max_position_embeddings = 512, type_vocab_size = 2, initializer_range = 0.02):\n",
        "    \n",
        "    '''\n",
        "    vocab_size: ALBERT model에서 input_ids의 vocabulary size\n",
        "    embedding_size: voc embedding의 크기\n",
        "    hidden_size: encoder & pooler layer의 크기\n",
        "    num_hidden_layers: Transformer encoder에서 hidden layer의 수\n",
        "    num_hidden_groups: hidden layer을 위한 그룹의 수, 같은 그룹의 파라미터는 공유됌\n",
        "    num_attention_head: Transformer encoder에서 각 attention layer에 대한 attention head의 수\n",
        "    intermediate_size: Transformer encoder에서 중간 레이어의 크기(ex. feed-forward)\n",
        "    inner_group_num: attention과 ffn의 반복 횟수\n",
        "    down_scale_factor: 적용하기 위한 scale\n",
        "    hidden_act: encoder과 pooler에서 비선형 활성화 함수\n",
        "    hidden_dropout_prob: embedding, encoder, pooler에서 fully connected layer를 위한 dropout 확률\n",
        "    attention_probs_dropout_prob: attention 확률을 위한 dropout 비율\n",
        "    max_position_embeddings: 최대 문장 길이. 경우에 따라 큰 값으로 지정하기도 함.\n",
        "    type_vocab_size: ALBERT model에 들어가는 token_type_ids의 vocabulary 크기\n",
        "    initializer_range: 모든 가중치 행렬을 초기화하기 위한 truncated_normal_initializer의 표준 편차.\n",
        "    '''\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.num_hidden_groups = num_hidden_groups\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.inner_group_num = inner_group_num\n",
        "    self.down_scale_factor = down_scale_factor\n",
        "    self.hidden_act = hidden_act\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "    self.max_position_embeddings = max_position_embeddings\n",
        "    self.type_vocab_size = type_vocab_size\n",
        "    self.initializer_range = initializer_range\n",
        "\n",
        "  @classmethod\n",
        "  def from_dict(cls, json_object):\n",
        "    # 파라미터의 Python dictionary로부터 ALBERTconfig 구성\n",
        "    config = AlbertConfig(vocab_size = None)\n",
        "    for (key, value) in six.iteritems(json_object):\n",
        "      config.__dict__[key] = value\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_json_file(cls, json_file):\n",
        "    # 파라미터의 json file로부터 ALBERTconfig 구성\n",
        "    with tf.gfile.GFile(json_file, 'r') as reader:\n",
        "      text = reader.read()\n",
        "    return csl.from_dict(json.loads(text))\n",
        "\n",
        "  def to_dict(self):\n",
        "    # 이 인스턴스를 Python dictionary로 직렬화\n",
        "    output = copy.deepcopy(self.__dict__)\n",
        "    return output\n",
        "\n",
        "  def to_json_string(self):\n",
        "    # 이 인스턴스를 JSON string으로 직렬화\n",
        "    return json.dumps(self.to_dict(), indent = 2, sort_keys = True) + '\\n'\n",
        "\n",
        "class AlbertModel(object):\n",
        "\n",
        "  def __init__(self, config, is_training, input_ids, input_mask = None, token_type_ids = None,\n",
        "               use_one_hot_embeddings = False, use_einsum = True, scope = None):\n",
        "    '''\n",
        "    ALBERT Model을 위한 생성자\n",
        "\n",
        "    config: AlbertConfig의 인스턴스\n",
        "    is_training: true면 학습 모델, false면 평가 모델. dropout이 적용될 지를 결정\n",
        "    input_ids: [batch_size, seq_length]형태의 Tensor\n",
        "    input_mask: [batch_size, seq_length]형태의 Tensor\n",
        "    token_type_ids: [batch_size, seq_length]형태의 Tensor\n",
        "    use_one_hot_embeddings: one-hot word embedding을 사용할 지 tf.embedding_lookup()을 사용할 지 결정\n",
        "    use_einsum: dense layer를 위해 einsum 또는 reshape+matmul을 사용할 지 결정\n",
        "    scope: 변수 범위\n",
        "    '''\n",
        "\n",
        "    config = copy.deepcopy(config)\n",
        "    if not is_training:   # 평가 모델일 때\n",
        "      config.hidden_dropout_prob = 0.0\n",
        "      config.attention_probs_dropout_prob = 0.0\n",
        "\n",
        "    input_shape = get_shape_list(input_ids, expected_rank = 2)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_length = input_shape[1]\n",
        "\n",
        "    if input_mask is None:\n",
        "      input_mask = tf.ones(shape = [batch_size, seq_length], dtype = tf.int32)\n",
        "\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = tf.zeros(shape = [batch_size, seq_length], dtype = tf.int32)\n",
        "\n",
        "    with tf.variable_scope(scope, default_name = 'bert'):\n",
        "      with tf.variable_scope('embeddings'):\n",
        "        # word ids에 대해 embedding lookup을 수행\n",
        "        (self.word_mebedding_output,\n",
        "         self.output_embedding_table) = embedding_lookup(\n",
        "             input_ids = input_ids, vocab_size = config.vocab_size, \n",
        "             embedding_size = config.embedding_size, initializer_range = config.initializer_range,\n",
        "             word_embedding_name = 'word_embeddings', use_one_hot_embeddings = use_ont_hot_embeddings\n",
        "         )\n",
        "\n",
        "         # positional embedding 추가 & token type embedding 추가 -> layer norm & dropout 수행\n",
        "         self.embedding_output = embedding_postprocessor(\n",
        "             input_tensor = self.word_embedding_output,\n",
        "             use_token_type = True,\n",
        "             token_type_vocab_size = config.type_vocab_size,\n",
        "             token_type_embedding_name = 'token_type_embeddings',\n",
        "             use_position_embeddings = True,\n",
        "             position_embedding_name = 'position_embeddings',\n",
        "             initializer_range = config.initializer_range,\n",
        "             max_position_embeddings = config.max_position_embeddings,\n",
        "             dropout_prob = config.hidden_dropout_prob,\n",
        "             use_one_hot_embeddings = use_one_hot_embeddings\n",
        "         )\n",
        "         \n",
        "      with tf.variable_scope('encoder'):\n",
        "        # 적재된 Transformer 실행\n",
        "        # sequence 출력 형태: [batch_size, seq_length, hidden_size]\n",
        "        self.all_encoder_layers = transformer_model(\n",
        "            input_tensor = self.embedding_output,\n",
        "            attention_mask = input_mask,\n",
        "            hidden_size = config.num_hidden_layers,\n",
        "            num_hidden_layers = config.num_hidden_layers,\n",
        "            num_hidden_groups = config.num_hidden_groups,\n",
        "            num_attention_heads = config.num_attention_heads,\n",
        "            intermediate_size = config.intermediate_size,\n",
        "            inner_group_num = config.inner_group_num,\n",
        "            intermediate_act_fn = get_activation(config.hidden_act),\n",
        "            hidden_dropout_prob = config.hidden_dropout_prob,\n",
        "            attention_probs_dropout_prob = config.attention_probs_dropout_prob,\n",
        "            initializer_range = config.initializer_range,\n",
        "            do_return_all_layers = True,\n",
        "            use_einsum = use_einsum\n",
        "        )\n",
        "\n",
        "        self.sequence_output = self.all_encoder_layer[-1]\n",
        "        # 'pooler'는 encoded sequence tensor의 형태를 [batch_size, seq_length, hidden_size]에서\n",
        "        # [batch_size, hidden_size]로 변환.\n",
        "        # 이 과정은 segment-level classification task에서 필수적임.\n",
        "        # 세그먼트의 고정 차원 representation이 필요\n",
        "        with tf.variable_scope('pooler'):\n",
        "          # 첫 번째 토큰에 해당하는 hidden state를 가져옴으로써 모델을 pooling 함\n",
        "          # pre-trained 되었다고 가정\n",
        "          first_token_tenspr =  tf.squeeze(self.sequence_output[:, 0:1, :], axis = 1)\n",
        "          self.pooled_output = tf.layers.dense(\n",
        "              first_token_tensor,\n",
        "              config.hidden_size,\n",
        "              activation = tf.tanh,\n",
        "              kernel_initializer = create_initializer(config.initializer_range)\n",
        "          )\n",
        "\n",
        "  def get_pooled_output(self):\n",
        "    return self.pooled_out\n",
        "\n",
        "  def get_sequence_output(self):\n",
        "    # encoder의 마지막 hidden layer를 얻음\n",
        "    # Return:\n",
        "    # transformer encoder의 마지막 hidden에 해당하는 [batch_size, seq_length, hidden_size]의 Tensor를 얻음\n",
        "    return self.sequence_output\n",
        "\n",
        "  def get_all_encoder_layers(self):\n",
        "    return self.all_encoder_layers\n",
        "\n",
        "  def get_word_embedding_output(self):\n",
        "    # word embedding lookup의 출력을 얻음\n",
        "    # 아직 positional embedding과 token type embedding이 추가되기 이전\n",
        "    # Return:\n",
        "    # word embedding layer의 출력에 상응하는 [batch_size, seq_length, embedding_size]의 Tensor를 얻음\n",
        "    return self.word_embedding_output\n",
        "\n",
        "  def get_embedding_output(self):\n",
        "    # embedding lookup의 출력을 얻음\n",
        "    # Return:\n",
        "    # word embedding & positional embedding & token type embedding이 합쳐진 embedding layer의 출력\n",
        "    # [batch_size, seq_length, embedding_size]의 Tensor를 얻음. \n",
        "    # 그 다음에 layer normalization을 수행. 이것이 transformer의 입력이 됌.\n",
        "    return self.embedding_output\n",
        "\n",
        "  def get_embedding_table(self):\n",
        "    return self.output_embedding_table\n",
        "\n",
        "def gelu(x):\n",
        "  # gelu 정의\n",
        "  # relu의 더 스무스한 버전\n",
        "  cdf = 0.5 * (1.0 + tf.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "  return x * cdf\n",
        "\n",
        "def get_activation(activation_string):\n",
        "  '''\n",
        "  string을 Python 함수로 매핑\n",
        "\n",
        "  Args:\n",
        "    activation_string: 활성화 함수이 문자열 이름\n",
        "\n",
        "  Returns:\n",
        "    활성화 함수에 상응하는 Python 함수.\n",
        "  '''\n",
        "  if not isinstance(activation_string, six.string_types):\n",
        "    return activation_string\n",
        "\n",
        "  if not activation_string:\n",
        "    return None\n",
        "\n",
        "  act = activation_string.lower()\n",
        "  if act == 'linear':\n",
        "    return None\n",
        "  elif act == 'relu':\n",
        "    return tf.nn.relu\n",
        "  elif act == 'gelu':\n",
        "    return gelu\n",
        "  elif act == 'tanh':\n",
        "    return tf.tanh\n",
        "  else:\n",
        "    raise ValueError('Unsupported activation: %s' % act)\n",
        "\n",
        "def get_assignment_map_from_checkpoint(tvars, init_checkpoint, num_of_group = 0):\n",
        "  # 현재 변수와 checkpoint 변수의 조합을 계산\n",
        "  assignment_map = {}\n",
        "  initializer_variable_names = {}\n",
        "\n",
        "  name_to_variable = collections.OrderDict()\n",
        "  for var in tvats:\n",
        "    name = var.name\n",
        "    m = re.match('^(.*):\\\\d+$', name)\n",
        "    if m is not None:\n",
        "      name = m.group(1)\n",
        "    name_to_variable[name] = var\n",
        "  init_vars = tf.train.list_variable(init_checkpoint)\n",
        "  init_vars_name = [name for (name, _) in init_vars]\n",
        "\n",
        "  if num_of_group > 0:\n",
        "    assignment_map = []\n",
        "    for gid in range(num_of_group):\n",
        "      assignment_map.append(collections.OrderDict())\n",
        "    else:\n",
        "      assignment_map = collections.OrderDict()\n",
        "\n",
        "    for name in name_to_variable:\n",
        "      if name in init_cars_name:\n",
        "        tvar_name = name\n",
        "      elif (re.sub(r\"/group_\\d+/\", \"/group_0/\",\n",
        "                 six.ensure_str(name)) in init_vars_name and\n",
        "          num_of_group > 1):\n",
        "      tvar_name = re.sub(r\"/group_\\d+/\", \"/group_0/\", six.ensure_str(name))\n",
        "      elif (re.sub(r\"/ffn_\\d+/\", \"/ffn_1/\", six.ensure_str(name))\n",
        "            in init_vars_name and num_of_group > 1):\n",
        "        tvar_name = re.sub(r\"/ffn_\\d+/\", \"/ffn_1/\", six.ensure_str(name))\n",
        "      elif (re.sub(r\"/attention_\\d+/\", \"/attention_1/\", six.ensure_str(name))\n",
        "            in init_vars_name and num_of_group > 1):\n",
        "        tvar_name = re.sub(r\"/attention_\\d+/\", \"/attention_1/\",\n",
        "                          six.ensure_str(name))\n",
        "      else:\n",
        "        tf.logging.info(\"name %s does not get matched\", name)\n",
        "        continue\n",
        "      tf.logging.info('name %s match to %s', name, tvar_name)\n",
        "      if num_of_group > 0:\n",
        "        group_mathed = False\n",
        "        for gid in range(1, num_of_group):\n",
        "          if ((\"/group_\" + str(gid) + \"/\" in name) or\n",
        "              (\"/ffn_\" + str(gid) + \"/\" in name) or\n",
        "              (\"/attention_\" + str(gid) + \"/\" in name)):\n",
        "            group_matched = True\n",
        "            tf.logging.info('%s belongs to %d th', name, gid)\n",
        "            assignment_map[gid][tvar_name] = name\n",
        "        if not group_matched:\n",
        "          assignment_map[0][tvar_name] = name\n",
        "      else:\n",
        "        assignment_map[tvar_name] = name\n",
        "      initialized_variable_names[name] = 1\n",
        "      initialized_variable_names[six.ensure_str(name) + ':0'] = 1\n",
        "\n",
        "    return (assignment_map, initialized_variable_names)\n",
        "\n",
        "def dropout(input_tensor, dropout_prob):\n",
        "  '''\n",
        "  Dropout 수행\n",
        "\n",
        "  Args:\n",
        "    input_tensor: Tensor\n",
        "    dropout_prob: 값을 dropout할 확률\n",
        "\n",
        "  Returns:\n",
        "    input_tensor의 \n",
        "  '''\n",
        "  if dropout_prob is None or dropout_prob == 0.0:\n",
        "    return input_tensor\n",
        "\n",
        "  output = tf.nn.dropout(input_tensor, rate = dropout_prob)\n",
        "  return output\n",
        "\n",
        "def layer_norm(input_tensor, name = None):\n",
        "  # Tensor의 마지막 차원에 layer normalization 진행\n",
        "  return contrib_layers.layer_norm(inputs = input_tensor, begin_norm_axis = -1, begin_params_axis = -1, scope = name)\n",
        "\n",
        "def layer_norm_and_dropout(input_tensor, dropout_prob, name = None):\n",
        "  # dropout을 따라서 layer normalization 실행\n",
        "  output_tensor = layer_norm(input_tensor, name)\n",
        "  output_tensor = dropout(output_tenor, dropout_prob)\n",
        "  return output_tensor\n",
        "\n",
        "def create_initializer(initializer_range = 0.02):\n",
        "  # 주어진 범위 내에서 truncated_normal_initializer 생성\n",
        "  return tf.truncated_normal_initializer(stddev = initializer_range)\n",
        "\n",
        "def get_timing_signal_1d_given_position(channels, positions, min_timescale = 1.0,\n",
        "                                        max_timescale = 1.0e4):\n",
        "  '''\n",
        "  주어진 timing position을 사용하여 서로 다른 fequency의 sin함수를 얻음\n",
        "\n",
        "  Args:\n",
        "    channels: 생성해야 하는 timing embedding의 크기. 서로 다른 timescale의 수는 channel / 2와 같음.\n",
        "    position: [batch_size, seq_len]의 형태인 Tensor\n",
        "    min_timescale & max_timescale: float\n",
        "\n",
        "  Return:\n",
        "    [batch, seq_len, channels] 형태의 timing signal의 Tensor\n",
        "  '''\n",
        "  num_timescales = channels // 2\n",
        "  log_timescale_increment = (\n",
        "      math.log(floar(max_timescale) / float(min_timescale)) / \n",
        "      (tf.to_float(num_timescales) - 1))\n",
        "  inv_timescales = min_timescale * tf.exp(\n",
        "      tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
        "  scaled_time = (\n",
        "      tf.expand_dims(tf.to_float(position), 2) * tf.expand_dims(\n",
        "          tf.expand_dims(inv_timescales, 0))\n",
        "  )\n",
        "  signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis = 2)\n",
        "  signal = tf.pad(signal, [[0, 0], [0, 0], [0, tf.mod(channels, 2)]])\n",
        "  return signal\n",
        "\n",
        "def embedding_lookup(input_ids, vocab_size, embedding_size = 128,\n",
        "                     initializer_range = 0.02, word_embedding_name = 'word_embeddings',\n",
        "                     use_one_hot_embeddings = False):\n",
        "  '''\n",
        "  id tensor에 대한 word embedding을 찾음\n",
        "\n",
        "  Args:\n",
        "    input_ids: word id를 포함하는 [batch_size, seq_length] 형태의 Tensor\n",
        "    vocab_size: embedding vocabulary의 크기\n",
        "    embedding_size: word embedding의 폭\n",
        "    initializer_range: Embedding initialization 범위\n",
        "    word_embedding_name: embedding table의 이름\n",
        "    use_one_hot_embeddings: True면 word embedding에 대해 one-hot method를 사용. False면 tf.nn.embedding_lookup() 사용.\n",
        "\n",
        "  Return:\n",
        "    [batch_size, seq_length, embedding_size] 형태의 Tensor를 가짐\n",
        "  '''\n",
        "  # 이 함수는 input의 형태가 [batch_size, seq_length, num_inputs]의 형태를 가짐.\n",
        "  # 만약 input의 형태가 2D tensor [batch_size, seq_length]면 [batch_size, seq_length, 1]로 재형성\n",
        "  if input_ids.shape.ndims == 2:\n",
        "    input_ids = tf.expand_dims(input_ids, axis = [ -1])\n",
        "\n",
        "  embedding_table = tf.get_variable(\n",
        "      name = word_embedding_name, shape = [vocab-size, embedding_size],\n",
        "      initializer = create_initializer(initializer_range)\n",
        "  )\n",
        "\n",
        "  if use_ont_hot_embedding:\n",
        "    flat_input_ids = tf.reshape(input_ids, [-1])\n",
        "    ont_hoe_input_ids = tf.one_hot(flat_input_ids, depth = vocab_size)\n",
        "    output = tf.matmul(one_hot_input_ids, embedding_table)\n",
        "  else:\n",
        "    output = tf.nn.embedding_lookup(embedding_table, input_ids)\n",
        "\n",
        "  input_shape = get_shape_list(input_ids)\n",
        "\n",
        "  output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
        "  return (output, embedding_table)\n",
        "\n",
        "def embedding_postprocessor(input_tensor, use_token_type = False, token_type_ids = None,\n",
        "                            token_type_vocab_size = 16, token_type_embedding_name = 'token_type_embeddings',\n",
        "                            initializer_range = 0.02, max_position_embeddings = 512, dropout_rpob = 0.1,\n",
        "                            use_ont_hot_embeddings = True):\n",
        "  '''\n",
        "  Args:\n",
        "    input_tensor: [batch_size, seq_length, embedding_size] 형태의 Tensor\n",
        "    use_token_type: token_type_ids에 embedding을 추가할 지 말 지 결정\n",
        "    token_type_ids: [batch_size, seq_length] 형태의 Tensor. use_token_type이 True면 무조건 명시되어 있어야 함.\n",
        "    token_type_vocab_size: token_type_ids의 vocabulary 크기\n",
        "    token_type_embedding_name: token type ids에 대한 embedding table 변수의 이름\n",
        "    use_position_embeddings: sequence에서 각각의 토큰의 position에 대해 position embedding을 추가할 지 말 지 결정\n",
        "    position_embedding_name: positional embeddings를 위한 embedding table 변수의 이름\n",
        "    initializer_range: 가중치 초기화의 범위\n",
        "    max_position_embeddings: 모델과 함께 사용될 maximum sequence length. input tensor의 길이보다 길 수는 있으나 짧을 수는 없음\n",
        "    dropout_prob: 최종 output tensor에 적용될 dropout 확률\n",
        "    use_one_hot_embeddings: True면 word embedding에 대해 one-hot method를 사용. False면 tf.nn.embedding_lookup()을 사용\n",
        "\n",
        "  Return:\n",
        "    input_tensor처럼 똑같은 형태\n",
        "  '''\n",
        "  input_shape = get_shape_list(input_tnesor, expected_rank = 3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  width = input_shape[2]\n",
        "\n",
        "  output = input_tensor\n",
        "\n",
        "  if use_token_type:\n",
        "    if token_type_ids is None:\n",
        "      raise ValueError(\"`token_type_ids` must be specified if\"\n",
        "                       \"`use_token_type` is True.\")\n",
        "    token_type_table = tf.get_variable(\n",
        "        name = token_type_embedding_name,\n",
        "        shape = [token_type_vocab_size, width],\n",
        "        initializer = create_initializer(initializer_range)\n",
        "    )\n",
        "    # 이 vocab은 작은 값일 것이기 때문에 항상 one-hot을 한다.\n",
        "    # 왜냐하면 tflite model을 변환하는 것보다는 작은 vocabulary를 변환하는 것이 더 빠를 것이기 때문이다.\n",
        "    if use_ont_hot_embeddings:\n",
        "      flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
        "      one_hot_ids = tf.one_hot(flat_token_type_ids, depth = token_type_vocab_size)\n",
        "      token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
        "      token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width])\n",
        "\n",
        "    else:\n",
        "      token_type_embeddings = tf.nn.embedding_lookup(token_type_table, token_type_ids)\n",
        "\n",
        "    output += toke_type_embeddings\n",
        "\n",
        "  if use_position_embeddings:\n",
        "    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
        "    with tf.control_dependencies([assert_op]):\n",
        "      full_position_embeddings = tf.get_variable(\n",
        "          name = position_embeddings,\n",
        "          shape = [max_position_embeddings, width],\n",
        "          initializer = create_initializer(initializer_range)\n",
        "      )\n",
        "      '''\n",
        "      position embedidng은 학습된 변수이기 때문에, sequence length 'max_position_embeddings'를 사용해서\n",
        "      생성한다. 사실상의 sequence length는 이것보다 짧지만, 빠른 학습을 위해 긴 sequence를 가질 필요는 없다.\n",
        "\n",
        "      그래서 'full_position_embeddings'는 position [0, 1, 2, ..., max_position_embeddings-1]에 대해 효과적인 embedding table이고,\n",
        "      현재 sequence는 [0, 1, 2, ..., seq_length-1]의 position을 가져서 slice를 수행할 수 있다.\n",
        "      '''\n",
        "      position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])\n",
        "      num_gims = len(output.shape.as_list())\n",
        "\n",
        "      # 마지막 두 차원만이 연관성을 가져서 일반적으로 배치 크기인 첫 번째 차원 사이에서 브로드캐스팅한다.\n",
        "      position_broadcast_shape = []\n",
        "      for _ in range(num_dims - 2):\n",
        "        position_broadcast-shape.append(1)\n",
        "      position_broadcast_shape.extend([seq_length, width])\n",
        "      position_embeddings = tf.reshape([position_embeddings, position_broadcase_shape])\n",
        "      output += position_embeddings\n",
        "\n",
        "  output = layer_norm_and_dropout(output, dropout_prob)\n",
        "  return output\n",
        "\n",
        "def einsum_via_matmul(input_tensor, w, num_inner_dims):\n",
        "  '''\n",
        "  matmul을 통한 einsum과 reshape 연산 구현\n",
        "\n",
        "  Args:\n",
        "    input_tensor: [batch_dims, inner_dims] 형태의 Tensor\n",
        "    w: [inner_dims, outer_dims] 형태의 Tensor\n",
        "    num_inner_dims: 내적을 위해 사용해야 하는 차원의 수\n",
        "\n",
        "  Returns:\n",
        "    [batch_dims, outer_dims] 형태의 Tensor\n",
        "  '''\n",
        "  input_shape = get_shape_list(input_tensor)\n",
        "  w_shape = get_shape_list(w)\n",
        "  batch_dims = input_shape[: -num_inner_dims]\n",
        "  inner_dims = input_shape[-num_inner_dims:]\n",
        "  outer_dims = w_shape[num_inner_dims:]\n",
        "  inner_dim = np.prod(inner_dims)\n",
        "  outer_dim = np.prod(outer_dims)\n",
        "  if num_inner_dims > 1:\n",
        "    input_tensor = tf.reshape(input_tensor, batch_dims + [inner_dim])\n",
        "  if len(w_shape) > 2:\n",
        "    w = tf.reshape(w, [inner_dim, outer_dim])\n",
        "  ret = tf.matmul(input_tensor, w)\n",
        "  if len(outer_dims) > 1:\n",
        "    input_tensor = tf.reshape(ret, batch_dims + outer_dims)\n",
        "  return ret\n",
        "\n",
        "def dense_layer_3d(input_tensor, num_attention_heads, head_size, initializer, activation,\n",
        "                   use_einsum, name = None):\n",
        "  '''\n",
        "  3D kernel을 사용하는 dense layer\n",
        "\n",
        "  Args:\n",
        "    input_tensor: [batch, seq_length, hidden_size] 형태의 Tensor\n",
        "    num_attention_heads: attention head의 수\n",
        "    head_size: 각 attention head의 크기\n",
        "    initializer: Kernel Initializer\n",
        "    activation: 활성화 함수\n",
        "    use_einsum: einsum 또는 reshape+matmul 중에 무엇을 dense layer에 사용할 지 결정\n",
        "    name: 이 레이어의 이름 범위\n",
        "\n",
        "  Returns:\n",
        "    Tensor logit\n",
        "  '''\n",
        "\n",
        "  input_shape = get_shape_list(input_tensor)\n",
        "  hidden_size = input_shape[2]\n",
        "\n",
        "  with tf.variable_scope(name):\n",
        "    w = tf.get_variable(\n",
        "        name = 'kernel',\n",
        "        shape = [hidden_size, num_attention_heads * head_size],\n",
        "        initializer = initializer\n",
        "    )\n",
        "    w = tf.reshape(w, [hidden_size, num_attention_heads, head_size])\n",
        "    b = tf.get_variable(\n",
        "        name = 'bias',\n",
        "        shape = [num_attention_heads * head_size],\n",
        "        initializer = tf.zeros_initializer\n",
        "    )\n",
        "    b = tf.reshape(b, [num_attention_heads, head_size])\n",
        "    if use_einsum:\n",
        "      ret = tf.einsum('BFH, HND -> BFND', input_tensor, w)\n",
        "    else:\n",
        "      ret = einsum_via_matmul(input_tensor, w, 1)\n",
        "    ret += b\n",
        "  if activation is not None:\n",
        "    return activation(ret)\n",
        "  else:\n",
        "    return ret\n",
        "\n",
        "def dense_layer_3d_proj(input_tensor, hidden_size, head_size, initializer,\n",
        "                        activation, use_einsum, name = None):\n",
        "  '''\n",
        "  projection을 위한 3D kernel을 사용한 dense layer\n",
        "\n",
        "  Args:\n",
        "    input_tensor: [batch, from_seq_length, num_attention_heads, size_per_head] 형태의 Tensor\n",
        "    hidden_size: hidden layer의 크기\n",
        "    head_size: head의 크기\n",
        "    initializer: Kernel Initializer\n",
        "    activation: 활성화 함수\n",
        "    use_einsum: einsum 또는 reshape+matmul 중에 무엇을 dense layer에 사용할 지 결정\n",
        "    name: 이 레이어의 이름 범위\n",
        "\n",
        "  Returns:\n",
        "    logit Tensor\n",
        "  '''\n",
        "  input_shape = get_shape_list(input_tensor)\n",
        "  num_attention_heads = input_shape[2]\n",
        "  with tf.variable_scope(name):\n",
        "    w = tf.get_variable(\n",
        "        name = 'kernel',\n",
        "        shape = [num_attention_heads * head_size, hidden_size],\n",
        "        initializer = initializer\n",
        "    )\n",
        "    w = tf.reshape(w, [num_attention_heads, head_size, hidden_size])\n",
        "    b = tf.get_variable(\n",
        "        name = 'bias', shape = [hidden_size], initializer = tf.zeros_initializer\n",
        "    )\n",
        "    if use_einsum:\n",
        "      ret = tf.einsum('BFND,NDH->BFH', input_tensor, w)\n",
        "    else:\n",
        "      ret = einsum_via_matmul(input_tensor, w, 2)\n",
        "    ret += b\n",
        "  if activation is not None:\n",
        "    return activation(ret)\n",
        "  else:\n",
        "    return ret\n",
        "\n",
        "def dense_layer_2d(input_tensor, output_size, initializer, activation, use_einsum,\n",
        "                   num_attention_heads = 1, name = None):\n",
        "  '''\n",
        "  2D kernel을 사용하는 dense layer\n",
        "\n",
        "  Args:\n",
        "    input_tensor: rank가 3인 tensor\n",
        "    output_size: 출력 차원의 크기\n",
        "    initializer: Kernel Initializer\n",
        "    activation: 활성화 함수\n",
        "    use_einsum: einsum 또는 reshape+matmul 중에 무엇을 dense layer에 사용할 지 결정\n",
        "    num_attention_heads: attention layer에서 attention head의 수\n",
        "    name: 이 레이어의 이름 범위\n",
        "\n",
        "  Returns:\n",
        "    logit Tensor\n",
        "  '''\n",
        "  del num_attention_heads   # 사용되지 않음\n",
        "  input_shape = get_shape_list(input_tensor)\n",
        "  hidden_size = input_shape[2]\n",
        "  with tf.variable_scope(name):\n",
        "    w = tf.get_variable(\n",
        "        name = 'kernel',\n",
        "        shape = [hidden_size, output_size],\n",
        "        initializer = initializer\n",
        "    )\n",
        "    b = tf.get_variable(\n",
        "        name = 'bias', shape = [output_size], initializer = tf.zeros_initializer\n",
        "    )\n",
        "    if use_einsum:\n",
        "      ret = tf.einsum('BFH,HO->BFO', input_tensor, w)\n",
        "    else:\n",
        "      ret = tf.matmul(input_tensor, w)\n",
        "    ret += b\n",
        "  if activation is not None:\n",
        "    return activation(ret)\n",
        "  else:\n",
        "    return ret\n",
        "\n",
        "def dot_product_attention(q, k, v, bias, dropout_rate = 0.0):\n",
        "  '''\n",
        "  dot-product 연산\n",
        "\n",
        "  Args:\n",
        "    q: [..., length_q, depth_k]\n",
        "    k: [..., length_kv, depth_k]. q와 맞는 차원\n",
        "    v: [..., length_kv, depth_v]. q와 맞는 차원\n",
        "    bias: bias Tensor\n",
        "    dropout_rate: 실수형\n",
        "\n",
        "  Returns:\n",
        "    [..., length_q, depth_v] 형태의 Tensor\n",
        "  '''\n",
        "  logits = tf.matmul(q, k, transpose_b = True)\n",
        "  logits = tf.multiply(logits, 1.0 / math.sqrt(float(get_shape_list(q)[-1])))\n",
        "  if bias is not None:\n",
        "    # attention_mask = [B, T]\n",
        "    from_shape = get_shape_list(q)\n",
        "    if len(from_shape) == 4:\n",
        "      broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], 1], tf.float32)\n",
        "    elif len(from_shape) == 5:\n",
        "      # from_shape = [B, N, Block_num, block_size, depth]\n",
        "      broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], from_shape[3], 1], tf.float32)\n",
        "\n",
        "    bias = tf.matmul(broadcast_ones, tf.cast(bias, tf.float32), transpose_b = True)\n",
        "\n",
        "    # 참조하고 싶은 position에 대해서는 attention_mask가 1.0이고, \n",
        "    # masked position에 대해서는 0.0이다. 이런 연산은 참조할 position에 대해서는\n",
        "    # 0.0이고, masked position에서는 -1000.0을 만들어 준다.\n",
        "    adder = (1.0 - bias) * -1000.0\n",
        "\n",
        "    # 이 값을 softmax 이전의 raw score에 더해주는 것이기 때문에, 완전히 지워버리는 것과 똑같이 효과적임.\n",
        "    logits += adder\n",
        "  else:\n",
        "    adder = 0.0\n",
        "\n",
        "  attention_probs = tf.nn.softmax(logits, name = 'attention_probs')\n",
        "  attention_probs = dropout(attention_probs, dropout_rate)\n",
        "  return tf.matmul(attention_probs, v)\n",
        "\n",
        "def attention_layer(from_tensor, to_tensor, attention_mask = None, num_attention_heads = 1,\n",
        "                    query_act = None, key_act = None, value_act = None,\n",
        "                    attention_probs_dropout_prob = 0.0, initializer_range = 0.02,\n",
        "                    batch_size = None, from_seq_length = None, to_seq_length = None,\n",
        "                    use_einsum = True):\n",
        "  '''\n",
        "  from_tensor로부터 to_tensor로 multi-headed attention을 수행\n",
        "\n",
        "  Args:\n",
        "    from_tensor: [batch_size, from_seq_length, from_width] 형태의 tensor\n",
        "    to_tensor: [batch_size, to_seq_length, to_width] 형태의 tensor\n",
        "    attention_mask: [batch_size, seq_length] 형태의 tensor. 값은 1 또는 0이 됌\n",
        "    num_attention_heads: attention head의 수\n",
        "    query_act: query 변형을 위한 활성화 함수\n",
        "    key_act: key 변형을 위한 활성화 함수\n",
        "    value_act: value 변형을 위한 활성화 함수\n",
        "    attention_probs_dropout_prob: attention 확률을 dropout 확률\n",
        "    initializer_range: 가중치 initializer의 범위\n",
        "    batch_size: 입력이 2D면, from_tensor와 to_tensor의 3D 버전의 배치 크기\n",
        "    from_seq_length: 입력이 2D면, from_tensor의 3D 버전의 seq length\n",
        "    to_seq_length: 입력이 2D면, to_tensor의 3D 버전의 seq length\n",
        "    use_einsum: einsum 또는 reshape+matmul 중에 무엇을 dense layer에 사용할 지 결정\n",
        "\n",
        "  Returns:\n",
        "    [batch_size, from_seq_length, num_attention_heads, size_per_head] 형태의 Tensor\n",
        "  '''\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank = [2, 3])\n",
        "  to_shape = get_shape_list(to_tensor, expected_rank = [2, 3])\n",
        "  size_per_head = int(from_shape[2] / num_attention_heads)\n",
        "  \n",
        "  if len(from_shape) != len(to_shape):\n",
        "    raise ValueError(\n",
        "        \"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
        "\n",
        "  if len(from_shape) == 3:\n",
        "    batch_size = from_shape[0]\n",
        "    from_seq_length = from_shape[1]\n",
        "    to_seq_length = to_shape[1]\n",
        "  elif len(from_shape) == 2:\n",
        "    if (batch_size if None or from_seq_length is None ot to_seq_length is None):\n",
        "      raise ValueError(\n",
        "          \"When passing in rank 2 tensors to attention_layer, the values \"\n",
        "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
        "          \"must all be specified.\")\n",
        "      \n",
        "  # Scalar 차원은 다음과 같다.\n",
        "  # B = batch size(sequence의 수)\n",
        "  # F = from_tensor의 sequence length\n",
        "  # T = to_tensor의 sequence length\n",
        "  # N = num_attention_heads\n",
        "  # H = size_per_head\n",
        "\n",
        "  # query_layer = [B, F, N, H]\n",
        "  q = dense_layer_3d(from_tensor, num_attention_heads, size_per_head,\n",
        "                     create_initializer(initializer_range), query_act,\n",
        "                     use_einsum, 'query')\n",
        "\n",
        "  # key_layer = [B, T, N, H]\n",
        "  k = dense_layer_3d(to_tensor, num_attention_heads, size_per_head,\n",
        "                     create_initializer(initializer_range), key_act,\n",
        "                     use_einsum, 'key')\n",
        "\n",
        "  # value_layer = [B, T, N, H]\n",
        "  v = dense_layer_3d(to_tensor, num_attention_heads, size_per_head,\n",
        "                     create_initializer(initializer_range), value_act,\n",
        "                     use_einsum, 'value')\n",
        "\n",
        "  q = tf.transpose(q, [0, 2, 1, 3])\n",
        "  k = tf.transpose(k, [0, 2, 1, 3])\n",
        "  v = tf.transpose(v, [0, 2, 1, 3])\n",
        "  if attention_mask is not None:\n",
        "    attention_mask = tf.reshape(\n",
        "      attention_mask, [batch_size, 1, to_seq_length, 1])\n",
        "    # new embeddings = [B, N, F, H]\n",
        "  new_embedidngs = dot_product_attention(q, k, v, attention_mask, attention_probs_dropout_prob)\n",
        "  \n",
        "  return tf.transpose(new_embeddings, [0, 2, 1, 3])\n",
        "\n",
        "def attention_ffn_block(layer_input, hidden_size = 768, attention_mask = None, num_attention_heads = 1,\n",
        "                        attention_head_size = 64, attention_probs_dropout_prob = 0.0,\n",
        "                        intermediate_size = 3072, intermediate_act_fn = None,\n",
        "                        initializer_range = 0.02, hidden_dropout_prob = 0.0, use_einsum = True):\n",
        "  '''\n",
        "  Args:\n",
        "    layer_input: [batch_size, seq_length, from_width] 형태의 Tensor\n",
        "    hidden_size: hidden layer의 크기\n",
        "    attention_mask: [batch_size, seq_length] 형태의 Tensor. 값은 항상 1 또는 0이 되어야 한다.\n",
        "    num_attention_heads: attention head의 수\n",
        "    attention_head_size: attention head의 크기\n",
        "    attention_probs_dropout_prob: attention layer에 대한 dropout 확률\n",
        "    intermediate_size: 중가 hidden layer의 크기\n",
        "    intermediate_act_fn: 중간 레이어에 대한 활성화 함수\n",
        "    initializer_range: 가중치 initializer의 범위\n",
        "    hidden_dropout_prob: hidden layer의 dropout 확률\n",
        "    use_einsum: einsum 또는 reshape+matmul 중에 무엇을 dense layer에 사용할 지 결정\n",
        "  '''\n",
        "  with tf.variable_scope('attention_1'):\n",
        "    with tf.variable_scope('self'):\n",
        "      attention_output = attention_layer(\n",
        "          from_tensor = layer_input, to_tensor = layer_input,\n",
        "          attention_mask = attention_mask, num_attention_heads = num_attention_heads,\n",
        "          attention_probs_dropout_prob = attention_probs_dropout_prob,\n",
        "          initializer_range = initializer_range, use_einsum = use_einsum\n",
        "      )\n",
        "\n",
        "    # hidden_size의 선형 projection을 실행\n",
        "    # 그 다음에 layer_input을 사용하여 residual 추가\n",
        "    with tf.variable_scope('output'):\n",
        "      attention_output = dense_layer_3d_proj(\n",
        "          attention_output, hidden_size, attention_head_size,\n",
        "          create_initializer(initializer_range), None, use_eisum = use_einsum,\n",
        "          name = 'dense'\n",
        "      )\n",
        "      attention_output = dropout(attention_output, hidden_dropout_prob)\n",
        "  attention_output = layer_norm(attention_output + layer_input)\n",
        "  with tf.variable_scope('ffn_1'):\n",
        "    with tf.variable_scope('intermediate'):\n",
        "      intermediate_output = dense_layer_2d(\n",
        "          attention_output, intermediate_size, create_initializer(initializer_range),\n",
        "          intermediate_act_fn, use_einsum = use_einsum, num_attention_heads = num_attention_heads,\n",
        "          name = 'dense'\n",
        "      )\n",
        "      with tf.variable_scope('output'):\n",
        "        ffn_output = dense_layer_2d(\n",
        "            intermediate_output, hidden_size, create_initializer(initializer_range),\n",
        "            None, use_einsum = use_einsum, num_attention_heads = num_attention_heads,\n",
        "            name = 'dense'\n",
        "        )\n",
        "      ffn_output = dropout(ffn_output, hidden_dropout_prob)\n",
        "  ffn_output = layer_norm(ffn_output + attention_output)\n",
        "  return ffn_output\n",
        "\n",
        "def transformer_model(input_tensor, attention_mask = None, hidden_size = 768, num_hidden_layers = 12,\n",
        "                      num_hidden_groups = 12, intermediate_size = 3072, inner_group_num = 1,\n",
        "                      intermediate_act_fn = 'gelu', hidden_dropout_prob = 0.1,\n",
        "                      attention_probs_dropout_prob = 0.1, initializer_range = 0.02,\n",
        "                      do_return_all_layers = False, use_einsum = False):\n",
        "  '''\n",
        "  'Attention Is All You Nedd'의 multi-layer Transformer\n",
        "\n",
        "  기존의 Transformer encoder와 거의 비슷한 구조\n",
        "  '''\n",
        "  if hidden_size & num_attention_heads != 0:\n",
        "    raise ValueError(\n",
        "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "        \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "    \n",
        "  attention_head_size = hidden_size // num_attention_heads\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank = 3)\n",
        "  input_width = input_shape[2]\n",
        "\n",
        "  all_layer_outputs = []\n",
        "  if input_wirdth != hidden_size:\n",
        "    prev_output = dense_layer_2d(\n",
        "        input_tensor, hidden_size, create_initializer(initializer_range),\n",
        "        None, use_einsum = use_einsum, name = 'embedding_hidden_mapping_in'\n",
        "    )\n",
        "  else:\n",
        "    prev_output = input_tensor\n",
        "  with tf.variable_scope('transformer', reuse = tf.AUTO_REUSE):\n",
        "    for layer_idx in range(num_hidden_layers):\n",
        "      group_idx = int(layer_idx / num_hidden_layers * num_hidden_groups)\n",
        "      with tf.variable_scope('group_%d' % group_idx):\n",
        "        with tf.name_scope('layer_%d' % layer_idx):\n",
        "          layer_output = prev_output\n",
        "          for inner_group_idx in range(inner_group_num):\n",
        "            with tf.variable_scope('inner_group_%d' % inner_group_idx):\n",
        "              layer_output = attention_ffn)block(\n",
        "                  layer_input = layer_input, hidden_size = hidden_size,\n",
        "                  attention_mask = attention_mask, num_attention_heads = num_attention_heads,\n",
        "                  attention_head_size = attention_head_size, attention_probs_dropout_prob = attention_probs_dropout_prob,\n",
        "                  intermediate_size = intermediate_size, intermediate_act_fn = intermediate_act_fn,\n",
        "                  initializer_range = initializer_range, hidden_dropout_prob = hidden_dropout_prob,\n",
        "                  use_einsum= use_einsum\n",
        "              )\n",
        "              prev_output = layer_output\n",
        "              all_layer_outputs.append(layer_output)\n",
        "  if do_return_all_layers:\n",
        "    return all_layer_outputs\n",
        "  else:\n",
        "    return all_layer_output[-1]\n",
        "\n",
        "def get_shape_list(tensor, expected_rank = None, name = None):\n",
        "  '''\n",
        "  정적 차원을 선호하는 tensor의 모양 목록을 반환한다.\n",
        "\n",
        "  Args:\n",
        "    tensor: 형태를 찾을 tf.Tensor 객체\n",
        "    expected_rank: `tensor`의 예상 순위이다. 이것이 지정되고 `tensor`의 순위가 다른 경우 예외가 발생한다.\n",
        "    name: 오류 메시지에 대한 tensor의 선택적 이름이다.\n",
        "\n",
        "  Returns:\n",
        "    tensor 모양의 차원 목록이다. 모든 정적 차원은 파이썬 정수로 반환되고 동적 차원은 tf.Tensor 스칼라로 반환된다.\n",
        "  '''\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  if expected_rank is not None:\n",
        "    assert_rank(tensor, expected_rank, name)\n",
        "\n",
        "  shape = tensor.shape.as_list()\n",
        "\n",
        "  non_static_indexes = []\n",
        "  for (index, dim) in enumerate(shape):\n",
        "    if dim is None:\n",
        "      non_static_indexes.append(index)\n",
        "\n",
        "  if not non_static_indexes:\n",
        "    return shape\n",
        "\n",
        "  dyn_shape = tf.shape(tensor)\n",
        "  for index in non_static_indexes:\n",
        "    shape[index] = dyn_shape[index]\n",
        "  return shape\n",
        "\n",
        "def reshape_to_matrix(input_tensor):\n",
        "  # rank 2 이상인 tensor a를 rank 2인 tensor로 변환한다.\n",
        "  ndims = input_tensor.shape.ndims\n",
        "  if ndims < 2:\n",
        "    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
        "                     (input_tensor.shape))\n",
        "  if ndims == 2:\n",
        "    return input_tensor\n",
        "\n",
        "  width = input_tensor.shape[-1]\n",
        "  output_tensor = tf.reshape(input_tensor, [-1, width])\n",
        "  return output_tensor\n",
        "\n",
        "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
        "  # rank 2인 tensor를 rank 2 이상인 기존의 tensor로 변환\n",
        "  if len(orig_shape_list) == 2:\n",
        "    return output_tensor\n",
        "\n",
        "  output_shape = get_shape_list(output_tensor)\n",
        "\n",
        "  otig_dims = orig_shape_list[0:-1]\n",
        "  width = output_shape[-1]\n",
        "\n",
        "  return tf.reshape(output_tensor, orig_dims + [width])\n",
        "\n",
        "def assert_rank(tensor, expected_rank, name = None):\n",
        "  # tensor 순위가 예상 순위가 아닌 경우 예외를 발생시킨다.\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  expected_rank_dict = {}\n",
        "  if isinstance(expected_rank, six.integer_types):\n",
        "    expected_rank_dict[expected_rank] = True\n",
        "  else:\n",
        "    for x in expected_rank:\n",
        "      expected_rank_dict[x] = True\n",
        "\n",
        "  actual_rank = tensor.shape.ndims\n",
        "  if actual_rank not in expected_rank_dict:\n",
        "    scope_name = tf.get_variable_scope().name\n",
        "    raise ValueError(\n",
        "        \"For the tensor `%s` in scope `%s`, the actual rank \"\n",
        "        \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
        "        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))"
      ]
    }
  ]
}
