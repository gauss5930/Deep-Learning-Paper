{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyYtgzk8AlQ0iVErJnQqc9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauss5930/Natural-Language-Processing/blob/main/ELECTRA/ELECTRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DOoGPzGpeOl"
      },
      "outputs": [],
      "source": [
        "#https://github.com/google-research/electra/blob/master/model/modeling.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import six\n",
        "import tensorflow.compat.v1 as tf\n",
        "from tensorflow.contrib import layers as contrib_layers\n",
        "\n",
        "class BertConfig(object):\n",
        "  def __init__(self, vocab_size, hidden_size = 768, num_hidden_layers = 12,\n",
        "               num_attention_heads = 12, intermediate_size = 3072, hidden_act = 'gelu',\n",
        "               hidden_dropout_prob = 0.1, attention_probs_dropout_prob = 0.1,\n",
        "               max_position_embeddings = 512, type_vocab_size = 2, initializer_range = 0.02):\n",
        "    '''\n",
        "    BertConfig 구성\n",
        "\n",
        "    Args:\n",
        "      vocab_size: BertModel에서 input_ids의 vocabulary 크기\n",
        "      hidden_size: encoder 레이어와 pooler 레이어의 크기\n",
        "      num_hidden_layers: Transformer encoder의 hidden layer 수\n",
        "      num_attention_heads: Transformer encoder에서 각 attention layer에 대한 attention head의 수\n",
        "      intermediate_size: Transformer encoder에서 중간 레이어의 크기\n",
        "      hidden_act: encoder와 pooler의 비선형 활성화 함수\n",
        "      hidden_dropout_prob: embeddings, encoder, pooler에서 모든 fully connected layer에 대한 dropout 확률\n",
        "      attention_probs_dropout_prob: attention 확률에 대한 dropout 비율\n",
        "      max_position_embeddings: 모델이 사용할 수 있는 최대 sequence length\n",
        "      type_vocab_size: BertModel에 들어가는 token_type_ids의 vocabulary 크기\n",
        "      initializer_range: 모든 가중치 행렬 초기화에 대한 truncated_normal_initializer의 표준편차\n",
        "    '''\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.hidden_act = hidden_act\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "    self.max_position_embeddings = max_position_embeddings\n",
        "    self.type_vocab_size = type_vocab_size\n",
        "    self.initializer_range = initializer_range\n",
        "\n",
        "  @classmethod\n",
        "  def from_dict(cls, json_object):\n",
        "    # 파라미터의 Python dictionary로부터 BertConfig 구성\n",
        "    config = BertConfig(vocab_size = None)\n",
        "    for (key, value) in six.iteritems(json_object):\n",
        "      config.__dict__[key] = value\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_json_file(cls, json_file):\n",
        "    # 파라미터의 json_file로부터 BertConfig 구성\n",
        "    with tf.io.gfile.GFile(json_file, 'r') as reader:\n",
        "      text = reader.read()\n",
        "    return cls.from_dict(json.loads(text))\n",
        "\n",
        "  def to_dict(self):\n",
        "    # Python dictionary로 이 instance들을 나열하기\n",
        "    output = copy.deepcopy(self.__dict__)\n",
        "    return output\n",
        "\n",
        "  def to_json_string(self):\n",
        "    # JSON 문자열로 이 instance들을 나열하기\n",
        "    return json.dumps(self.to_dict(), indent = 2, sort_keys = True) + '\\n'\n",
        "\n",
        "class BertModel(object):\n",
        "  # BERT model이다. 학습 알고리즘은 다르지만 ELECTRA에 사용되는 BERT는 똑같다.\n",
        "  def __init__(self, bert_config, is_training, input_ids, input_mask = None,\n",
        "               token_type_ids = None, use_one_hot_embeddings = True, scope = None,\n",
        "               embedding_size = None, input_embeddings = None, input_reprs = None,\n",
        "               update_embeddings = True, untied_embeddings = False,\n",
        "               ltr = False, rtl = False):\n",
        "    '''\n",
        "    Args:\n",
        "      bert_config: BertConfig 인자\n",
        "      is_training: true면 학습 모델, false면 평가 모델. dropout이 어디에 적용될 지 조종\n",
        "      input_ids: [batch_size, seq_length] 형태의 Tensor\n",
        "      input_mask: [batch_size, seq_length] 형태의 Tensor\n",
        "      token_type_ids: [batch_size, seq_length] 형태의 Tensor\n",
        "      use_one_hot_embeddings: word embedding에 대해 one-hot word embedding을 사용할 지 tf.embedding_lookup()을 사용할 지 결정\n",
        "      scope: 변수 범위. 기본값은 'electra'\n",
        "    '''\n",
        "    bert_config = copy.deepcopy(bert_config)\n",
        "    if not is_training:\n",
        "      bert_config.hidden_dropout_prob = 0.0\n",
        "      bert_config.attention_probs_dropout_prob = 0.0\n",
        "\n",
        "    input_shape = get_shape_list(token_type_ids, expected_rank = 2)\n",
        "    batch_size = input_shaep[0]\n",
        "    seq_length = input_shape[1]\n",
        "\n",
        "    if input_mask is None:\n",
        "      input_mask = tf.ones(shape = [batch_size, seq_length], dtype = tf.int32)\n",
        "\n",
        "    assert token_type_ids is not None\n",
        "\n",
        "    if input_reprs is None:\n",
        "      if input_embeddings is None:\n",
        "        with tf.variable_scope(\n",
        "            (scope if untied_embeddings else 'electra') + '/embeddings',\n",
        "            reuse = tf.AUTO_REUSE):\n",
        "          # word ids에 대해 embedding lookup을 수행\n",
        "          if embedding_size is None:\n",
        "            embedding_size = bert_config.hidden_size\n",
        "          (self.token_embeddings, self.embedding_table) = embeddings_lookup(\n",
        "              input_ids = input_ids,\n",
        "              vocab_size = bert_config.vocab_size,\n",
        "              embedding_size = embedding_size,\n",
        "              initializer_range = bert_config.initializer_range,\n",
        "              word_embedding_name = 'word_embeddings',\n",
        "              use_one_hot_embeddings = use_one_hot_embeddings\n",
        "          )\n",
        "      else:\n",
        "        self.token_embeddings = input_embeddings\n",
        "\n",
        "      with tf.varialbe_scope(\n",
        "          (scope if untied_embeddings else 'electra') + '/embeddings',\n",
        "          reuse = tf.AUTO_REUSE):\n",
        "        # positional embedding과 token_type_embedding 추가 후, layer normalization 진행한 뒤에 dropout 수행\n",
        "        self.embedding_output = embedding_postprocessor(\n",
        "            input_tensor = self.token_embeddings,\n",
        "            use_token_type = True,\n",
        "            token_type_ids = token_type_ids,\n",
        "            token_type_vocab_size = bert_config.type_vocab_size,\n",
        "            token_type_embedding_name = 'token_type_embeddings',\n",
        "            use_position_embeddings = True,\n",
        "            position_embedding_name = 'position_embeddings',\n",
        "            initializer_range = bert_config.max_position_embeddings, dropout_prob = bert_config.hidden_dropout_prob\n",
        "        )\n",
        "    \n",
        "    else:\n",
        "      self.embedding_output = input_reprs\n",
        "    if not update_embeddings:\n",
        "      self.embedding_output = tf.stop_gradient(self.embedding_output)\n",
        "\n",
        "    with tf.variable_scope(scope, default_name = 'electra'):\n",
        "      if self.embedding_output.shape[-1] != bert_config_hidden_size:\n",
        "        self.embedding_output = tf.layers.dense(\n",
        "            self.embedding_output, bert_config.hidden_size,\n",
        "            name = 'embeddings_project'\n",
        "        )\n",
        "\n",
        "      with tf.variable_scope('encoder'):\n",
        "        # [batch_size, seq_length] 2차원 형태의 mask를 3차원 형태의 [batch_size, seq_length, seq_length]로 변환해줌\n",
        "        # 이 mask는 attention score에 사용된다.\n",
        "        attention_mask = creae_attention_mask_from_input_mask(\n",
        "            token_type_ids, input_mask\n",
        "        )\n",
        "\n",
        "        # ltr 또는 rtl Transformer을 실행하기 위해 일반적인 masking을 attention에 추가한다.\n",
        "        if ltr or rtl:\n",
        "          casual_mask = tf.ones((seq_length, seq_length))\n",
        "          if ltr:\n",
        "            casual_mask = tf.matrix_band_part(casual_mask, -1, 0)\n",
        "          else:\n",
        "            casual_mask = tf.matrix_band_part(casual_mask, 0, -1)\n",
        "          attention_mask *= tf.expand_dims(casual_mask, 0)\n",
        "\n",
        "        # 적재된 Transformer를 실행. 출력의 형태는 다음과 같음.\n",
        "        # sequence_output: [batch_size, seq_length, hidden_size]\n",
        "        # pooled_output: [batch_size, hidden_size]\n",
        "        # all_encoder_layers: [n_layers, batch_size, seq_length, hidden_size]\n",
        "        # attn_maps: [n_layers, batch_size, n_heads, seq_length, seq_length]\n",
        "        (self.all_layer_output, self.attn_maps) = transformer_model(\n",
        "            input_tensor = self.embedding_output,\n",
        "            attention_mask = attention_mask,\n",
        "            hidden_size = bert_config.hidden_size,\n",
        "            num_hidden_layers=bert_config.num_hidden_layers,\n",
        "            num_attention_heads=bert_config.num_attention_heads,\n",
        "            intermediate_size=bert_config.intermediate_size,\n",
        "            intermediate_act_fn=get_activation(bert_config.hidden_act),\n",
        "            hidden_dropout_prob=bert_config.hidden_dropout_prob,\n",
        "            attention_probs_dropout_prob=\n",
        "            bert_config.attention_probs_dropout_prob,\n",
        "            initializer_range=bert_config.initializer_range,\n",
        "            do_return_all_layers=True\n",
        "        )\n",
        "        self.sequence_output = self.all_layer_outputs[-1]\n",
        "        self.pooled_output = self.sequence_output[:, 0]\n",
        "\n",
        "  def get_pooled_output(self):\n",
        "    return self.pooled_output\n",
        "\n",
        "  def get_sequence_output(self):\n",
        "    # encoder의 마지막 hidden layer을 얻음\n",
        "    return self.sequence_output\n",
        "\n",
        "  def get_all_encoder_layers(self):\n",
        "    return self.all_layer_outputs\n",
        "  \n",
        "  def get_embedding_output(self):\n",
        "    # embedding lookup의 출력을 얻음\n",
        "    return self.embedding_output\n",
        "\n",
        "  def get_embedding_table(self):\n",
        "    return self.embedding_table\n",
        "\n",
        "def gelu(input_tensor):   # Gaussian Error Linear Unit\n",
        "  cdf = o.0 * (1.0 + tf.math.erf(input_tensor / tf.sqrt(2.0)))\n",
        "  return input_tensor * cdf\n",
        "\n",
        "def get_activation(activation_string):\n",
        "  # 문자열을 Python 함수로 매핑. ex) 'relu' -> 'tf.nn.relu'\n",
        "  if not isinstance(activation_string, six.string_types):\n",
        "    return activation_string\n",
        "\n",
        "  if not activation_string:\n",
        "    return None\n",
        "\n",
        "  act = activation_string.lower()\n",
        "  if act == 'linear':\n",
        "    return None\n",
        "  elif act == 'relu':\n",
        "    return tf.nn.relu\n",
        "  elif act == 'gelu':\n",
        "    return gelu\n",
        "  elif act == 'tanh':\n",
        "    return tf.tanh\n",
        "  else:\n",
        "    raise ValueError('Unsupported activation: %s' % act)\n",
        "\n",
        "def get_assingment_map_from_checkpoint(tvars, init_checkpoint, prefic = \"\"):\n",
        "  # 현재 변수와 checkpoint 변수의 조합을 계산\n",
        "  name_to_variable = collections.OrderDict()\n",
        "  for var in tvars:\n",
        "    name = var.name\n",
        "    m = re.match(\"^(.*):\\\\d+$\", name)\n",
        "    if m is not None:\n",
        "      name = m.group(1)\n",
        "      name_to_variable[name] = var\n",
        "\n",
        "    initializer_varialbe_names = {}\n",
        "    assingment_map = collections.OrderDict()\n",
        "    for x in tf.train.list_variable(init_checkpoint):\n",
        "      (name, var) = (x[0], x[1])\n",
        "      if prefix + name not in name_tovariable:\n",
        "        continue\n",
        "      assignment_map[name] = prefix + name\n",
        "      initialized_variable_names[name] = 1\n",
        "      initialized_variable_names[name + ':0'] = 1\n",
        "\n",
        "    return assignment_map, initialized_variable_names\n",
        "\n",
        "def dropout(input_tensor, dropout_prob):\n",
        "  # Dropout 수행\n",
        "  if dropout_prob is None or dropout_prob == 0.0:\n",
        "    return input_tensor\n",
        "\n",
        "  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
        "  return output\n",
        "\n",
        "def layer_norm(input_tensor, name = None):\n",
        "  # tensor의 마지막 차원에 layer normalization 실행\n",
        "  return contrib_layers.layer_norm(\n",
        "      inputs = input_tensor, begin_norm_axis = -1, begin_params_axis = -1, scope = name\n",
        "  )\n",
        "\n",
        "def layer_norm_and_dropout(input_tensor, dropout_prob, name = None):\n",
        "  # layer normalization 실행 후 dropout 실행\n",
        "  output_tensor = layer_norm(input_tensor, name)\n",
        "  output_tensor = dropout(output_tensor, dropout_prob)\n",
        "  return output_tensor\n",
        "\n",
        "def create_initializer(initializer_range = 0.02):\n",
        "  # 주어진 범위를 사용하여 truncated_normal_initializer 생성\n",
        "  return tf.truncated_normal_initializer(stddev = initializer_range)\n",
        "\n",
        "def embedding_lookup(input_ids, vocab_size, embedding_size = 128, initializer_range = 0.02,\n",
        "                     word_embedding_name = 'word_embeddings', use_one_hot_embeddings = False):\n",
        "  '''\n",
        "  id tensor에 대한 word embedding을 lookup\n",
        "\n",
        "  Args:\n",
        "    input_ids: word ids를 포함하는 [batch_size, seq_length] 형태의 Tensor\n",
        "    vocab_size: embedding vocabulary의 크기\n",
        "    initializer_range: embedding 초기화 범위\n",
        "    word_embedding_name: embedding table의 이름\n",
        "    use_one_hot_embeddings: True면 word embedding에 대해 one-hot method를 사용. False면 tf.nn.embedding_lookup()을 사용\n",
        "  '''\n",
        "\n",
        "  # 이 함수는 입력이 [batch_size, seq_length, num_inputs] 형태라고 생각\n",
        "  # 입력이 [batch_size, seq_length]의 2차원 형태면 [batch_size, seq_length, 1]로 형태 변환\n",
        "  original_dims = input_ids.shape.ndims\n",
        "  if original_dims == 2:\n",
        "    input_ids = tf.expand_dims(input_ids, axis = [-1])\n",
        "\n",
        "  embedding_table = tf.get_variable(name = word_embedding_name, shape = [vocab_size, embedding_size],\n",
        "                                    initializer = vreate_initializer(initializer_range))\n",
        "  \n",
        "  if original_dims == 3:\n",
        "    input_shape = get_shape_list(input_ids)\n",
        "    tf.reshape(input_ids, [-1, input_shape[-1]])\n",
        "    output = tf.matmul(input_ids, embedding_table)\n",
        "    output = tf.reshape(output, [input_shape[0], input_shape[1], embedding_size])\n",
        "  else:\n",
        "    if use_one_hot_embeddings:\n",
        "      flat_input_ids = tf.reshape(input_ids, [-1])\n",
        "      one_hot_input_ids = tf.one_hot(flat_input_ids, depth = vocab_size)\n",
        "      output = tf.matmul(one_hot_input_ids, embedding_table)\n",
        "    else:\n",
        "      output = tf.nn.embedding_lookup(embedding_table, input_ids)\n",
        "\n",
        "    input_shape = get_shape_list(input_ids)\n",
        "\n",
        "    output = tf.reshape(output, \n",
        "                        input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
        "  return output, embedding_table\n",
        "\n",
        "def embedding_postprocessor(input_tensor, use_token_type = False, token_type_ids = None,\n",
        "                            token_type_vocab_size = 16, token_type_embedding_name = 'token_type_embeddings',\n",
        "                            use_position_embedding = True, position_embedding_name = 'position_embeddings',\n",
        "                            initializer_range = 0.02, max_position_embeddings = 512, dropout_prob = 0.1):\n",
        "  '''\n",
        "  Args:\n",
        "    input_tensor: [batch_size, seq_length, embedding_size] 형태의 Tensor\n",
        "    use_token_type: token_type_ids를 위해 임베딩을 추가할 지 말 지 결정\n",
        "    token_type_ids: [batch_size, seq_length] 형태의 Tensor. use_token_type이 true면 무조건 명시되어야 함\n",
        "    token_type_vocab_size: token_type_ids의 vocabulary 크기\n",
        "    token_type_embedding_name: token_type_ids에 대한 embedding table 변수의 이름\n",
        "    use_position_embeddings: sequence에서 각 토큰의 위치에 대해 position embedding을 추가할 지 말 지 결정\n",
        "    position_embedding_name: positional embedding을 위한 embedding table 변수의 이름\n",
        "    initializer_range: 가중치 초기화의 범위\n",
        "    max_position_embeddings: 모델이 사용할 수 있는 maximum sequence length\n",
        "    dropout_prob: 마지막 output tensor에 적용되는 dropout 확률\n",
        "  '''\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank = 3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  width = input_shape[2]\n",
        "\n",
        "  output = input_tensor\n",
        "\n",
        "  if use_token_type:\n",
        "    if token_type_ids is None:\n",
        "      raise ValueError(\"`token_type_ids` must be specified if\"\n",
        "                       \"`use_token_type` is True.\")\n",
        "    token_type_table = tf.get_variable(\n",
        "        name = token_type_embedding_name,\n",
        "        shape = [token_type_vocab_size, width],\n",
        "        initializer = create_initializer(initializer_range)\n",
        "    )\n",
        "    # 이 vocab은 작을 것이기 때문에 항상 one-hot encoding을 한다. 이것이 작은 vocab에 대해서 항상 더 빠른 속도를 보여줌\n",
        "    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
        "    one_hot_ids = tf.one_hot(falt_token_type_ids, depth = token_type_vocab_size)\n",
        "    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
        "    token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width])\n",
        "    output += token_type_embeddings\n",
        "\n",
        "  if use_position_embeddings:\n",
        "    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
        "    with tf.control_dependencies([assert_op]):\n",
        "      full_position_embeddings = tf.get_variable(\n",
        "          name = position_embedding_name,\n",
        "          shape = [max_position_embeddings, width],\n",
        "          initializer = create_initializer(initializer_range)\n",
        "      )\n",
        "      '''\n",
        "      position embedding은 학습된 변수이기 때문에, 'max_position_embeddings'의 sequecen length를 사용해서 생성한다.\n",
        "      실제 sequence length는 이것보다 짧다. 왜냐하면 빠른 학습을 위해 긴 sequence를 가지면 안 되기 때문이다.\n",
        "      \n",
        "      그래서 'full_position_embedding'은 사실상 position [0, 1, 2, ..., max_position_embeddings-1]에 대한\n",
        "      embedding table이고 현재 시퀀스의 position은 [0, 1, 2, ..., seq_length-1]이다.\n",
        "      따라서 슬라이스만을 수행할 수 있다.\n",
        "      '''\n",
        "      position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])\n",
        "      num_dims = len(output.shape.as_list())\n",
        "\n",
        "      '''\n",
        "      마지막 두 개의 차원만이 연관되어 있다(seq_length & width). 그래서 주로 배치 크기인 첫 번째 차원 사이에서 방송한다.\n",
        "      '''\n",
        "      position_broadcast_shape = []\n",
        "      for _ in range(num_dims - 2):\n",
        "        position_broadcast_shape.append(1)\n",
        "      position_broadcast_shape.extend([seq_length, width])\n",
        "      position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape)\n",
        "      output += position_embeddings\n",
        "\n",
        "  output = layer_norm_and_dropout(output, dropout_prob)\n",
        "  return output\n",
        "\n",
        "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
        "  '''\n",
        "  2D tensor mask로부터 3D attention mask 생성\n",
        "\n",
        "  Args:\n",
        "    from_tensor: [batch_size, from_seq_length] 형태의 2D 또는 3D Tensor\n",
        "    to_mask: [batch_size, to_seq_length] 형태의 Tensor\n",
        "\n",
        "  Returns:\n",
        "    [batch_size, from_seq_length, to_seq_length] 형태의 Tensor\n",
        "  '''\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank = [2, 3])\n",
        "  batch_size = from_shape[0]\n",
        "  from_seq_length = from_shape[1]\n",
        "\n",
        "  to_shape = get_shape_list(to_mask, expected_rank = 2)\n",
        "  to_seq_length = to_shape[1]\n",
        "\n",
        "  to_mask = tf.cast(\n",
        "      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n",
        "  )\n",
        "\n",
        "  '''\n",
        "  from_tensor가 마스크일 것이라고 생각하지 않는다. \n",
        "  *from* 패딩 토큰(오직 *to* 패딩)을 참조하는지 여부는 실제로 신경쓰지 않으므로 모든 토큰의 텐서를 생성한다.\n",
        "  \n",
        "  broadcast_ones = [batch_size, from_seq_length, 1]\n",
        "  '''\n",
        "  broadcast_ones = tf.ones(\n",
        "      shape = [batch_size, from_seq_length, 1], dtype = tf.float32\n",
        "  )\n",
        "\n",
        "  # mask를 생성하기 위해 두 개의 차원과 함께 broadcast\n",
        "  mask = broadcast_ones * to_mask\n",
        "\n",
        "  return mask\n",
        "\n",
        "def attention_layer(from_tensor, to_tensor, attention_mask = None, num_attention_heads = 1,\n",
        "                    size_per_head = 512, query_act = None, key_act = None, value_act = None,\n",
        "                    attention_probs_dropout_prob = 0.0, initializer_range = 0.02,\n",
        "                    do_return_2d_tensor = False, batch_size = None, from_seq_length = None,\n",
        "                    to_seq_length = None):\n",
        "  '''\n",
        "  from_tensor로부터 to_tensor로 multi-head attention을 수행\n",
        "  '''\n",
        "  def transpose_for_scores(input_tensor, batch_size, num_attention_heads, seq_length, width):\n",
        "    output_tensor = tf.reshape(\n",
        "        input_tensor, [batch_size, seq_length, num_attention_heads, width]\n",
        "    )\n",
        "\n",
        "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
        "    return output_tensor\n",
        "\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank = [2, 3])\n",
        "  to_shape = get_shape_list(to_tensor, expected_rank = [2, 3])\n",
        "\n",
        "  if len(from_shape) != len(to_shape):\n",
        "    raise ValueError(\n",
        "        \"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
        "    \n",
        "  if len(from_shape) == 3:\n",
        "    batch_size = from_shape[0]\n",
        "    from_seq_length = from_shape[1]\n",
        "    to_seq_length = to_shape[1]\n",
        "  elif len(from_shape) == 2:\n",
        "    if batch_size is None or from_seq_length is None or to_seq_length is None:\n",
        "      raise ValueError(\n",
        "          \"When passing in rank 2 tensors to attention_layer, the values \"\n",
        "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
        "          \"must all be specified.\")\n",
        "      \n",
        "  '''\n",
        "  스칼라 차원\n",
        "  B = batch size(sequence의 수)\n",
        "  F = from_tensor의 sequence length\n",
        "  T = to_tensor의 sequence length\n",
        "  N = num_attnetion_heads\n",
        "  H = size_per_head\n",
        "  '''\n",
        "\n",
        "  from_tensor_2d = reshape_to_matrix(from_tensor)\n",
        "  to_tensor_2d = reshape_to_matrix(to_tensor)\n",
        "\n",
        "  # query layer = [B*F, N*H]\n",
        "  query_layer = tf.layers.dense(\n",
        "      from_tensor_2d, num_attention_heads * size_per_head,\n",
        "      activation = query_act,\n",
        "      name = 'query',\n",
        "      kernel_initializer = create_initializer(initializer_range)\n",
        "  )\n",
        "\n",
        "  # key layer = [B*T, N*H]\n",
        "  key_layer = tf.layers.dense(\n",
        "      to_tensor_2d, num_attention_heads * size_per_head,\n",
        "      activation = key_act, name = 'key',\n",
        "      kernel_initializer = create_initializer(initializer_range)\n",
        "  )\n",
        "\n",
        "  # value layer = [B*T, N*H]\n",
        "  value_layer = tf.layers.dense(\n",
        "      to_tensor_2d, num_attention_heads * size_per_head,\n",
        "      activation = value_act, name = 'value',\n",
        "      kernel_initializer = create_initializer(initializer_range)\n",
        "  )\n",
        "\n",
        "  # query layer = [B, N, F, H]\n",
        "  query_layer = transpose_for_scores(query_layer, batch_size, num_attention_heads,\n",
        "                                     from_seq_length, size_per_head)\n",
        "  \n",
        "  # key layer = [B, N, T, H]\n",
        "  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n",
        "                                   to_seq_length, size_per_head)\n",
        "  \n",
        "  # query와 key의 내적으로 attention score를 얻음\n",
        "  # attention_score = [B, N, F, T]\n",
        "  attention_scores = tf.matmul(query_layer, key_layer, transpose_b = True)\n",
        "  attention_scores = tf.multiply(attention_scores,\n",
        "                                 1.0 / math.sqrt(float(size_per_head)))\n",
        "  \n",
        "  if attention_mask is not None:\n",
        "    # attention_mask = [B, 1, F, T]\n",
        "    attention_mask = tf.expand_dims(attention_mask, axis = [1])\n",
        "\n",
        "    # attention_mask는 참조하길 원하는 position에 대해서는 1.0을 masked position에 대해서는 0.0을 가짐\n",
        "    # 이 연산은 참조하길 원하는 position에 대해서는 0.0의 tensor를, masked position에 대해서는 -10000.0의 tensor를 가짐\n",
        "    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
        "\n",
        "    # 이 값을 softmax 이전의 raw score에 더하기 때문에, 이것은 완전히 지우는 것과 똑같이 효과적이다.\n",
        "    attention_scores += adder\n",
        "\n",
        "  # attention score를 확률로 정규화\n",
        "  # attentipn_probs = [B, N, F, T]\n",
        "  attention_rpobs = tf.nn.softmax(attention_scores)\n",
        "\n",
        "  # 이것은 실제로 참조할 전체 토큰을 삭제함\n",
        "  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
        "\n",
        "  # value_layer = [B, T, N, H]\n",
        "  value_layer = tf.reshape(value_layer,\n",
        "                           [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
        "  \n",
        "  # value_layer = [B, N, T, H]\n",
        "  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
        "\n",
        "  # context_layer = [B, N, F, H]\n",
        "  context_layer = tf.matmul(attention_probs, value_layer)\n",
        "\n",
        "  # context_layer = [B, F, N, H]\n",
        "  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
        "\n",
        "  if do_return_2d_tensor:\n",
        "    # context_layer = [B*F, N*H]\n",
        "    context_layer = tf.reshape(\n",
        "        context_layer, [batch_size * from_seq_length, num_attention_heads * size_per_head]\n",
        "    )\n",
        "  else:\n",
        "    # context_layer = [B, F, N*H]\n",
        "    context_layer = tf.reshape(\n",
        "        context_layer, [batch_size, from_seq_length, num_attention_heads * size_per_head]\n",
        "    )\n",
        "\n",
        "  return context_layer, attention_probs\n",
        "\n",
        "def transformer_model(input_tensor, attention_mask = None, hidden_size = 768,\n",
        "                      num_hidden_layers = 12, num_attention_heads = 12,\n",
        "                      intermediate_size = 3072, intermediate_act_fn = gelu,\n",
        "                      hidden_dropout_prob = 0.1, attention_probs_dropout_prob = 0.1,\n",
        "                      initializer_range = 0.02, do_return_all_layers = False):\n",
        "  '''\n",
        "  Multi-headed, multi-layer Transformer\n",
        "  '''\n",
        "  if hidden_size % num_attention_head != 0:\n",
        "    raise ValueError(\n",
        "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "        \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "    \n",
        "  # re-shaping과 3D tensor를 2D tensor로 바꾸기 위한 노력을 피하려고\n",
        "  # representation을 2D tensor로 유지시킴. re-shapes는 GPU/CPU 에서는 free지만, TPU에서는 그렇지 않음.\n",
        "  # 그래서 optimizer을 돕기 위해 최소화하려고 함.\n",
        "  prev_output = reshape_to_matrix(input_tensor)\n",
        "\n",
        "  attn_maps = []\n",
        "  all_layer_outputs = []\n",
        "  for layer_idx in range(num_hidden_layers):\n",
        "    with tf.variable_scope('layer_%d' % layer_idx):\n",
        "      with tf.variavle_scope('attention'):\n",
        "        attention_heads = []\n",
        "        with tf.variable_scope('self'):\n",
        "          attention_head, probs = attention_layer(\n",
        "              from_tensor = prev_output, to_tensor = prev_output,\n",
        "              attention_mask = attention_mask,\n",
        "              num_attention_heads =  num_attention_heads,\n",
        "              size_per_head = attention_head_size,\n",
        "              attention_probs_dropout_prob = attention_probs_dropout_prob,\n",
        "              initializer_range = initializer_range,\n",
        "              do_return_2d_tensor = True,\n",
        "              batch_size = batch_size,\n",
        "              from_seq_length = seq_length,\n",
        "              to_seq_length = seq_length\n",
        "          )\n",
        "          attention_heads.append(attention_head)\n",
        "          attn_maps.append(probs)\n",
        "\n",
        "        attention_output = None\n",
        "        if len(attention_heads) == 1:\n",
        "          attention_output = attention_heads[0]\n",
        "        else:\n",
        "          # 다른 sequence를 가지는 경우에, projection 이전에 self-attention head로 이들을 합침\n",
        "          attention_output = tf.concat(attention_heads, axis = -1)\n",
        "\n",
        "        # hidden_size의 선형 projection 실행하고 layer_input과 함께께 residual\n",
        "        with tf.variable_scope('output'):\n",
        "          attention_output = tf.layers.dense(\n",
        "              attention_output, hidden_size, kernel_initializer = create_initializer(initializer_range)\n",
        "          )\n",
        "          attention_output = dropout(attention_output, hidden_dropout_prob)\n",
        "          attention_output = layer_norm(attention_output + prev_output)\n",
        "\n",
        "      # 활성화는 오직 중간 hidden layer에 적용됨\n",
        "      with tf.variable_scope('intermediate'):\n",
        "        intermediate_output = tf.layers.dense(\n",
        "            attention_output, intermediate_size, activation = intermediate_act_fn,\n",
        "            kernel_initializer = create_initializer(initializer_range)\n",
        "        )\n",
        "\n",
        "      # hidden_size로 다시 하향 투영한 뒤 residual을 추가\n",
        "      with tf.variable_scope('output'):\n",
        "        prev_output = tf.layers.dense(\n",
        "            intermediate_output, hidden_size, kernel_initializer = create_initializer(initializer_range)\n",
        "        )\n",
        "        prev_output = dropout(prev_output, hidden_dropout_prob)\n",
        "        prev_output = layer_norm(prev_output + attention_output)\n",
        "        all_layer_outputs.append(prev_output)\n",
        "\n",
        "  attn_maps = tf.stack(attn_maps, 0)\n",
        "  if do_return_all_layers:\n",
        "    return tf.stack([reshape_from_matrix(layer, input_shape) for layer in all_layer_outputs], 0), attn_maps\n",
        "  else:\n",
        "    return reshape_from_matrix(prev_output, input_shape), attn_maps\n",
        "\n",
        "def get_shape_list(tensor, expected_rank = None, name = None):\n",
        "  # tensor의 형태 리스트를 반환. static 차원을 더 선호\n",
        "  if isinstance(tensor, np.ndarray) or isinstance(tensor, list):\n",
        "    shape = np.array(tensor).shape\n",
        "    if isinstance(expected_rank, six.integer_types)\n",
        "      assert len(shape) == expected_rank\n",
        "    elif expected_rank is not NOne:\n",
        "      assert len(shape) in expected_rank\n",
        "    return shape\n",
        "\n",
        "  if name is NOne:\n",
        "    name = tensor.name\n",
        "\n",
        "  if expected_rank is not None:\n",
        "    assert_rank(tensor, expected_rank, name)\n",
        "\n",
        "  shape = tensor.shape.as_list()\n",
        "\n",
        "  non_static_indexes = []\n",
        "  for (index, dim) in enumerate(shape):\n",
        "    if dim is None:\n",
        "      non_static_indexes.append(index)\n",
        "\n",
        "  if not non-static_indexes:\n",
        "    return shape\n",
        "\n",
        "  dyn_shape = tf.shape(tensor)\n",
        "  for index in none_static_indexes:\n",
        "    shape[index] = dyn_shape[index]\n",
        "  return shape\n",
        "\n",
        "def reshape_to_matrix(input_tensor):\n",
        "  # a >= rank 2 tensor를 rank 2 tensor로 변형\n",
        "  ndims = input_tensor.shape.ndims\n",
        "  if ndims < 2:\n",
        "    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
        "                     (input_tensor.shape))\n",
        "    \n",
        "  if ndims == 2:\n",
        "    return input_tensor\n",
        "\n",
        "  width = input_tensor.shape[-1]\n",
        "  output_tensor = tf.reshape(input_tensor, [-1, width])\n",
        "  return output_tensor\n",
        "\n",
        "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
        "  # rank 2 tensor를 기존의 rank >= 2인 tensor로 변형\n",
        "  if len(orig_shape_list) == 2:\n",
        "    return output_tensor\n",
        "\n",
        "  output_shape = get_shape_list(output_tensor)\n",
        "\n",
        "  orig_dims = orig_shape_list[0:-1]\n",
        "  width = output_shape[-1]\n",
        "\n",
        "  return tf.reshape(output_tensor, orig_dims + [width])\n",
        "\n",
        "def assert_rank(tensor, expected_rank, name = None):\n",
        "  # tensor rank가 expected rank가 아니면 예외 발생시킴\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  expected_rank_dict = {}\n",
        "  if isinstance(expected_rank, sic.integer_types):\n",
        "    expected_rank_dict[expected_rank] = True\n",
        "  else:\n",
        "    for x in expected_rank:\n",
        "      expected_rank_dict[x] = True\n",
        "\n",
        "  actual_rank = tensor.shape.ndims\n",
        "  if actual_rank not in expectted_rank_dict:\n",
        "    scope_name = tf.get_variable_scope().name\n",
        "    raise ValueError(\n",
        "        \"For the tensor `%s` in scope `%s`, the actual rank \"\n",
        "        \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
        "        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))"
      ]
    }
  ]
}
