{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7PtJswjOPtdrbUQcICrcp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauss5930/Natural-Language-Processing/blob/main/XLNet/XLNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgYufJCgpn7Z"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import modeling\n",
        "\n",
        "def _get_initializer(FLAGS):\n",
        "  # 변수 초기화\n",
        "  if FLAGS.init == 'uniform':\n",
        "    initializer = tf.initializers.random_uniform(\n",
        "        minval = -FLAGS.init_range,\n",
        "        maxval = FLAGS.init_range,\n",
        "        seed = None\n",
        "    )\n",
        "\n",
        "  elif FLAGS.init == 'normal':\n",
        "    initializer = tf.initializers.random_normal(\n",
        "        stddev = FLAGS.init_std,\n",
        "        seed = None\n",
        "    )\n",
        "\n",
        "  else:\n",
        "    raise ValueError('Initializer {} not supported'.format(FALGS.init))\n",
        "  return initializer\n",
        "\n",
        "class XLNetConfig(object):\n",
        "  ''' XLNetConfig는 model checkpoint에 특정된 하이퍼 파라미터를 포함하고 있음\n",
        "  이 하이퍼 파라미터들은 pre-training 시와 fine-tuning 시에 모두 같아야 함\n",
        "\n",
        "  n_layer: 레이어의 수\n",
        "  d_model: hidden size\n",
        "  n_head: attention head의 수\n",
        "  d_head: 각 attention head의 차원 크기\n",
        "  d_inner: feed-forward layer에서 hidden size\n",
        "  ff_activation: 'relu' 또는 'gelu'\n",
        "  untie_r: attention에서 bias들을 untie할 지 말지 결정\n",
        "  n_token: vocab_size\n",
        "  '''\n",
        "\n",
        "  def __init__(self, FLAGS = None, json_path = None):\n",
        "    '''\n",
        "    XLNetConfig 구조\n",
        "    하나의 FLAGS 또는 json_path는 제공되어야 한다.\n",
        "    '''\n",
        "\n",
        "    assert FLAGS is not None or json_path is not None\n",
        "\n",
        "    self.keys = ['n_layer', 'd_model', 'n_head', 'd_head', 'd_inner', 'ff_activation', \n",
        "                 'untie_r', 'n_token']\n",
        "\n",
        "    if FLAGS is not None:\n",
        "      self.init_from_flags(FLAGS)\n",
        "\n",
        "    if json_path is not None:\n",
        "      self.init_from_json(json_path)\n",
        "\n",
        "  def init_from_flags(self, FLAGS):\n",
        "    for key in self.keys:\n",
        "      setattr(self, key, getattr(FLAGS, key))\n",
        "\n",
        "  def init_from_json(self, FLAGS):\n",
        "    with tf.gfile.Open(json_path) as f:\n",
        "      json_data = json.load(f)\n",
        "      for key in self.keys:\n",
        "        setattr(self, key, json_data[key])\n",
        "\n",
        "  def to_json(self, json_path):\n",
        "    # XLNetConfig를 json 파일로 저장\n",
        "    json_data = {}\n",
        "    for key in self.keys:\n",
        "      json_data[key] = getattr(self, key)\n",
        "\n",
        "    json_dir = os.path.dirname(json_path)\n",
        "    if not tf.gfile.Exists(json_dir):\n",
        "      tf.gfile.MakeDirs(json_dir)\n",
        "    with tf.gfile.Open(json_path, 'w') as f:\n",
        "      json.dump(json_data, f, indent = 4, sort_keys = True)\n",
        "\n",
        "def create_run_config(is_training, is_finetune, FLAGS):\n",
        "  kwargs = dict(\n",
        "      is_training=is_training,\n",
        "      use_tpu=FLAGS.use_tpu,\n",
        "      use_bfloat16=FLAGS.use_bfloat16,\n",
        "      dropout=FLAGS.dropout,\n",
        "      dropatt=FLAGS.dropatt,\n",
        "      init=FLAGS.init,\n",
        "      init_range=FLAGS.init_range,\n",
        "      init_std=FLAGS.init_std,\n",
        "      clamp_len=FLAGS.clamp_len\n",
        "  )\n",
        "\n",
        "  if not is_finetune:\n",
        "    kwargs.update(dict(\n",
        "        mem_len=FLAGS.mem_len,\n",
        "        reuse_len=FLAGS.reuse_len,\n",
        "        bi_data=FLAGS.bi_data,\n",
        "        clamp_len=FLAGS.clamp_len,\n",
        "        same_length=FLAGS.same_length\n",
        "    ))\n",
        "\n",
        "  return RunConfig(**kwargs)\n",
        "\n",
        "class RunConfig(object):\n",
        "  '''\n",
        "  RunConfig는 pre-training과 fine-tuning에서 서로 다른 하이퍼 파라미터를 가져야 함.\n",
        "  이 하이퍼 파라미터들은 실행할 때마다 변경할 수 있다.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, is_training, use_tpu, use_bfloat16, dropout, dropatt,\n",
        "               init = 'normal', init_range = 0.1, init_std = 0.02, mem_len = None,\n",
        "               reuse_len = None, bi_data = False, clamp_len = -1, same_length = False):\n",
        "    '''\n",
        "    is_training: 학습 모드인지 아닌지 확인\n",
        "    use_tpu: TPU를 사용할 지 말 지 확인\n",
        "    use_bfloat16: float32 대신에 bfloat16 사용\n",
        "    dropout: dropout 비율\n",
        "    dropatt: attention 확률에 dropout 비율\n",
        "    init: 초기화 scheme. 'normal' 또는 'uniform' 둘 중 하나\n",
        "    init_range: [-init_range, init_range]에서 균일한 분포를 사용해서 파라미터를 초기화\n",
        "      init='uniform'일 때 가장 효과적임\n",
        "    mem_len: 캐시해둘 토큰의 수\n",
        "    reuse_len: 캐시되고 향후 재사용될 현재 배치의 토큰 수이다.\n",
        "    bi_data: 양방향성 입력 파이프라인을 사용할 지 말 지 정함. \n",
        "      pre-training 중에는 True를 사용, fine-tuning 중에는 False를 사용\n",
        "    clamp_len: clamp_len보다 큰 모든 상대 거리를 고정한다다. -1은 클램핑이 없음을 의미한다.\n",
        "    same_length: 각 토큰에 대해 똑같은 attention length를 사용할 지 말 지 결정\n",
        "    '''\n",
        "\n",
        "    self.init = init\n",
        "    self.init_range = init_range\n",
        "    self.init_std = init_std\n",
        "    self.is_training = is_training\n",
        "    self.dropout = dropout\n",
        "    self.dropatt = dropatt\n",
        "    self.use_tpu = use_tpu\n",
        "    self.use_bfloat16 = use_bfloat16\n",
        "    self.mem_len = mem_len\n",
        "    self.reuse_len = reuse_len\n",
        "    self.bi_data = bi_data\n",
        "    self.clamp_len = clamp_len\n",
        "    self.same_length = same_length\n",
        "\n",
        "class XLNetModel(object):\n",
        "  # pre-training 및 fine-tuning 중에 사용되는 XLNet 모델의 wrapper이다.\n",
        "\n",
        "  def __init__(self, xlnet_config, run_config, input_ids, seg_ids, input_mask,\n",
        "               memes = None, perm_mask = None, target_mapping = None, inp_q = None,\n",
        "               **kwargs):\n",
        "    \n",
        "    initializer = _get_initializer(run_config)\n",
        "\n",
        "    tfm_args = dict(\n",
        "        n_token=xlnet_config.n_token,\n",
        "        initializer=initializer,\n",
        "        attn_type=\"bi\",\n",
        "        n_layer=xlnet_config.n_layer,\n",
        "        d_model=xlnet_config.d_model,\n",
        "        n_head=xlnet_config.n_head,\n",
        "        d_head=xlnet_config.d_head,\n",
        "        d_inner=xlnet_config.d_inner,\n",
        "        ff_activation=xlnet_config.ff_activation,\n",
        "        untie_r=xlnet_config.untie_r,\n",
        "\n",
        "        is_training=run_config.is_training,\n",
        "        use_bfloat16=run_config.use_bfloat16,\n",
        "        use_tpu=run_config.use_tpu,\n",
        "        dropout=run_config.dropout,\n",
        "        dropatt=run_config.dropatt,\n",
        "\n",
        "        mem_len=run_config.mem_len,\n",
        "        reuse_len=run_config.reuse_len,\n",
        "        bi_data=run_config.bi_data,\n",
        "        clamp_len=run_config.clamp_len,\n",
        "        same_length=run_config.same_length\n",
        "    )\n",
        "\n",
        "    input_args = dict(\n",
        "        inp_k=input_ids,\n",
        "        seg_id=seg_ids,\n",
        "        input_mask=input_mask,\n",
        "        mems=mems,\n",
        "        perm_mask=perm_mask,\n",
        "        target_mapping=target_mapping,\n",
        "        inp_q=inp_q\n",
        "    )\n",
        "\n",
        "    with tf.variable_scope('model', reuse = tf.AUTO_REUSE):\n",
        "      (self.output, self.new_mems, self.lookup_table) = modeling.transformer_xl(**tfm_args)\n",
        "\n",
        "    self.input_mask = input_mask\n",
        "    self.initializer = initializer\n",
        "    self.clnet_config = clnet_config\n",
        "    self.run_config = run_config\n",
        "\n",
        "  def get_pooled_out(self, summary_type, use_summ_proj = True):\n",
        "    xlnet_config = self.xlnet_config\n",
        "    run_config = self.run_config\n",
        "\n",
        "    with tf.variable_scope('model', reuse = tf.AUTO_REUSE):\n",
        "      summary = modeling.summarize_sequence(\n",
        "          summary_type=summary_type,\n",
        "          hidden=self.output,\n",
        "          d_model=xlnet_config.d_model,\n",
        "          n_head=xlnet_config.n_head,\n",
        "          d_head=xlnet_config.d_head,\n",
        "          dropout=run_config.dropout,\n",
        "          dropatt=run_config.dropatt,\n",
        "          is_training=run_config.is_training,\n",
        "          input_mask=self.input_mask,\n",
        "          initializer=self.initializer,\n",
        "          use_proj=use_summ_proj\n",
        "      )\n",
        "\n",
        "    return summary\n",
        "\n",
        "  def get_sequence_output(self):\n",
        "    # XLNet의 마지막 레이어의 hidden representation\n",
        "    \n",
        "    return self.output\n",
        "\n",
        "  def get_new_memory(self):\n",
        "    # 이전 메모리와 현재 input representation을 합친 new memory\n",
        "    # list의 길이는 n_layer와 같음\n",
        "    return self.new_mems\n",
        "\n",
        "  def get_embedding_table(self):\n",
        "    # embedding lookup table\n",
        "    # input 레이어와 output 레이어 간의 embedding tie\n",
        "    return self.lookup_table\n",
        "\n",
        "  def get_initializer(self):\n",
        "    # tf initilizer\n",
        "    # XLNet의 top layer에서 변수들을 초기화하기 위해 사용\n",
        "    return self.initializer"
      ]
    }
  ]
}
