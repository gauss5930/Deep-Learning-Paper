{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMCURwdSd6LE/DF4oH8QYA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauss5930/Natural-Language-Processing/blob/main/GPT-1/GPT_1%20Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-1 Implementation\n",
        "\n",
        "GPT-1 구현 코드는 [GPT 구현하기](https://paul-hyun.github.io/gpt-01/)를 참고하여 작성되었다.\n",
        "\n",
        "우선 GPT를 구현하기 전에 GPT에 대해 간략하게 설명하면 GPT는 Transformer의 Decoder만을 사용한 Pre-trained LM이다.\n",
        "\n",
        "### 1. Config\n",
        "\n",
        "Transformer와 파라미터를 동일하게 설정하였다. GPT는 Transformer의 Decoder만을 사용하므로 Encoder 부분은 제거하고 사용하였다."
      ],
      "metadata": {
        "id": "hMbeV9Y6mqNK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpRTodgTmhUR"
      },
      "outputs": [],
      "source": [
        "config = Config({\n",
        "    'n_dec_vocab': len(vocab),\n",
        "    'n_dec_seq': 256,\n",
        "    'n_layer': 6,\n",
        "    'd_hidn': 256,\n",
        "    'i_pad': 0,\n",
        "    'd_ff': 1024,\n",
        "    'n_head': 4,\n",
        "    'd_head': 64,\n",
        "    'dropout': 0.1,\n",
        "    'layer_norm_epsilon': 1e-12\n",
        "})\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Decoder\n",
        "\n",
        "GPT는 Transformer의 Encoder는 사용하지 않고 Decoder만 사용하므로 Decoder에서 Encoder의 출력과 Attention을 하는 부분인 Encoder-Decoder-Multi-Head Attention 부분은 제거하고 사용하였다. 그 외에 나머지 부분은 Transformer와 동일하다."
      ],
      "metadata": {
        "id": "0u1Nu0LroUzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.self_attn = MultiHeadAttention(self.config)\n",
        "    self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps = self.config.layer_norm_epsilon)\n",
        "    self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
        "    self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps = self.config.layer_norm_epsilon)\n",
        "\n",
        "  def forward(self, dec_inputs, self_attn_mask):\n",
        "    # (batch_size, n_dec_seq, d_hidn), (batch_size, n_head, n_dec_seq, n_dec_seq)\n",
        "    self_att_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)\n",
        "    self_att_outputs = self.layer_norm1(dec_inputs + self_att_outputs)\n",
        "    # (batch_size, n_dec_seq, d_hidn)\n",
        "    ffn_outputs = self.po_ffn(self_att_outputs)\n",
        "    ffn_outputs = self.layer_norm3(self_att_outputs + ffn_outputs)\n",
        "    # (batch_size, n_dec_seq, d_hidn), (batch_size, n_head, n_dec_seq, n_dec_seq), (batch_size, n_head, n_dec_seq, n_enc_seq)\n",
        "    return ffn_outputs, self_attn_prob"
      ],
      "metadata": {
        "id": "ZbvA3ofcom9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)\n",
        "    sinusoid_table = torch.FloatTensor(det_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_hidn))\n",
        "    self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze = True)\n",
        "\n",
        "    self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
        "\n",
        "  def forward(self, dec_inputs):\n",
        "    positions = torch.arange(dec_inputs.size(1), device = dec_inputs.device, dtype = dec_inputs.dtype).expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous() + 1\n",
        "    pos_mask = dec_inputs.eq(self.config.i_pad)\n",
        "    positions.masked_fill_(pos_mask, 0)\n",
        "\n",
        "    # (batch_size, n_dec_seq, d_hidn)\n",
        "    dec_outputs = self.dec_emb(dec_inputs) + self.pos_emb(positions)\n",
        "\n",
        "    # (batch_size, n_dec_seq, n_dec_seq)\n",
        "    dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config.i_pad)\n",
        "    # (batch_size, n_dec_seq, n_dec_seq)\n",
        "    dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs)\n",
        "    # (batch_size, n_dec_seq, n_dec_seq)\n",
        "    dec_self_attn_mask = torch.gt((dec_attn_mask + dec_attn_decoder_mask), 0)\n",
        "\n",
        "    self_attn_probs = []\n",
        "    for layer in self.layers:\n",
        "      # (batch_size, n_dec_seq, d_hidn), (batch_size, n_dec_seq, n_dec_seq)\n",
        "      dec_outputs, self_attn_prob = layer(dec_outputs, dec_self_attn_mask)\n",
        "      self_attn_probs.append(self_attn_prob)\n",
        "    # (batch_size, n_dec_seq, d_hidn), [(batch_size, n_dec_seq, n_dec_seq)]\n",
        "    return dec_outputs, self_attn_probs"
      ],
      "metadata": {
        "id": "Z89dmNpSqq9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. GPT\n",
        "\n",
        "GPT는 단순히 Transformer Decoder를 실행\n",
        "Pre-traing 모델을 저장하기 위한 save, 저장된 모델을 읽기 위한 load 함수가 추가로 정의의"
      ],
      "metadata": {
        "id": "x59jUSdgF2FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.decoder = Decoder(self.config)\n",
        "\n",
        "  def forward(self, dec_inputs):\n",
        "    # (batch_size, n_seq, d_hidn), [(batch_size, n_head, n_dec_seq, n_dec_seq)]\n",
        "    dec_outputs, dec_self_attn_probs = self.decoder(dec_inputs)\n",
        "    # (batch_size, n_dec_seq, n_dec_vocab), [(batch_size, n_head, n_dec_seq, n_dec_seq)]\n",
        "    return dec_outputs, dec_self_attn_probs\n",
        "\n",
        "  def save(self, epoch, loss, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch, \n",
        "        'loss': loss, \n",
        "        'state_dict': self.state_dict()\n",
        "    }, path)\n",
        "\n",
        "  def load(self, path):\n",
        "    save = torch.load(path)\n",
        "    self.load_state_dict(save['state_dict'])\n",
        "    return save['epoch'], save['loss']"
      ],
      "metadata": {
        "id": "uEbD9YQ8GEd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Pre-traing Model\n",
        "\n",
        "GPT를 pre-train 하기 위한 클래스. GPT pre-train 클래스의 목적은 입력 단어에 대한 다음 단어를 예측하는 것이다."
      ],
      "metadata": {
        "id": "T5X1B3EzHaQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTPretraing(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.gpt = GPT(self.config)\n",
        "    # 단어를 예측하기 위한 projection_lm을 선언\n",
        "    self.projection_lm = nn.Linear(self.config.d_hidn, self.config.n_dec_vocab, bias = False)\n",
        "    # Decoder의 Embedding & weight를 공유\n",
        "    self.projection_lm.weight = self.gpt.decoder.dec_emb.weight\n",
        "\n",
        "  def forward(self, dec_inputs):\n",
        "    # (batch_size, n_dec_seq, d_hidn), [(batch_size, n_head, n_dec_seq, n_dec_seq)]\n",
        "    dec_outputs, dec_self_attn_probs = self.gpt(dec_inputs)\n",
        "    # (batch_size, n_dec_seq, n_dec_vocab)\n",
        "    # GPT 실행 결과를 입력으로 projection_lm을 실행해서 단어를 예측측\n",
        "    logits_lm = self.projection_lm(dec_outputs)\n",
        "    # (batch_size, n_dec_seq - 1, n_dec_vocab), (batch_size, n_output), [(batch_size, n_head, n_dec_seq, n_dec_seq)]\n",
        "    # 결과의 마지막을 제외한 나머지를 리턴\n",
        "    return logits_lm[:, :-1, :].contiguous(), dec_self_attn_probs"
      ],
      "metadata": {
        "id": "tdUtbgF7HuRi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
