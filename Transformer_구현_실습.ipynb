{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEjZ5/XN13lrmM3kUVgIFW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauss5930/Natural-Language-Processing/blob/main/Transformer_%EA%B5%AC%ED%98%84_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYoZgseydKyf"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.function as F\n",
        "\n",
        "from utils import utils\n",
        "\n",
        "def initialize_weight(x):\n",
        "  nn.init.xavier_uniform_(x.weight)\n",
        "  if x.bias is not None:\n",
        "    nn.init.constant_(x.bias, 0)\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "  def __init__(self, hidden_size, filter_size, dropout_rate):\n",
        "    super(FeedForwardNetwork, self).__init__()\n",
        "\n",
        "    self.layer1 = nn.Linear(hidden_size, filter_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.layer2 = nn.Linear(filter_size, hidden_size)\n",
        "\n",
        "    initialize_weight(self.layer1)\n",
        "    initialize_weight(self.layer2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer2(x)\n",
        "    return x\n",
        "\n",
        "class MultiHeadAttention(nn.Moculde):\n",
        "  def __init__(self, hidden_size, dropout_rate, head_size = 8):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    self.head_size = head_size\n",
        "\n",
        "    self.att_size = att_size = hidden_size // head_size\n",
        "    self.scale = arr_size ** -0.5\n",
        "\n",
        "    self.linear_q = nn.Linear(hidden_size, head_size * att_size, bias = False)\n",
        "    self.linear_k = nn.Linear(hidden_size, head_size * att_size, bias = False)\n",
        "    self.linear_v = nn.Linear(hidden_size, head_size * att_size, bias = False)\n",
        "    initialize_weight(self.linear_q)\n",
        "    initialize_weight(self.linear_k)\n",
        "    initialize_weight(self.linear_v)\n",
        "\n",
        "    self.att_dropout = nn.Dropout(dropout_rate)\n",
        "    \n",
        "    self.output_layer = nn.Linear(head_size * att_size, hidden_size, bias = False)\n",
        "    initialize_weight(self.output_layer)\n",
        "\n",
        "  def forward(self, q, k, v, mask, cache = None):\n",
        "    orig_q_size = q.size()\n",
        "\n",
        "    d_k = self.att_size\n",
        "    d_v = self.att_size\n",
        "    batch_size = q.size(0)\n",
        "\n",
        "    #head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)\n",
        "    q = self.linear_q(q).view(batch_size, -1, self.head_size, d_k)\n",
        "    if cache is not None and 'endec_k' in cache:\n",
        "      k, v = cache['endec_k'], cahce['endec_v']\n",
        "    else:\n",
        "      k = self.linear_k(k).view(bacth_size, -1, self.head_size, d_k)\n",
        "      v = self.linear_v(v).view(batch_size, -1, self.head_size, d_v)\n",
        "\n",
        "      if cache is not None:\n",
        "        cache['endec_k'], cache['endec_v'] = k, v\n",
        "\n",
        "    q = q.transpose(1, 2)                   # [b, h, q_len, d_k]\n",
        "    v = v.transpose(1, 2)                   # [b, h, v_len, d_v]\n",
        "    k = k.transpose(1, 2).transpose(2, 3)   # [b, h, d_k, k_len]\n",
        "\n",
        "    #Scaled Dot-Product Attention\n",
        "    #Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V\n",
        "    q.mul_(self.scale)\n",
        "    x = torch.matmul(q, k)   # [b, h, q_len, k_len]\n",
        "    x.masked_fill_(mask.unsqueeze(1), -1e9)\n",
        "    x = torch.softmax(x, dim = 3)\n",
        "    x = self.att_dropout(x)\n",
        "    x = x.matmul(v)   # [b, h, q_len, attn]\n",
        "\n",
        "    x = x.transpose(1, 2).contiguous()   # [b, q_len, h, attn]\n",
        "    x = x.view(batch_size, -1, self.head_size * d_v)\n",
        "\n",
        "    x = self.output_layer(x)\n",
        "\n",
        "    assert x.size() == orig_q_size\n",
        "    return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_size, filter_size, dropout_rate):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.self_attention_norm = nn.LayerNorm(hidden_size, eps = 1e-6)\n",
        "    self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
        "    self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    self.enc_dec_attention_norm = nn.LayerNorm(hidden_size, eps = 1e-6)\n",
        "    self.enc_dec_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
        "    self.enc_dec_attention_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    self.ffn_norm = nn.LayerNorm(hidden_size, eps = 1e-6)\n",
        "    self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
        "    self.ffn_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x, enc_output, self_mask, i_mask, cache):\n",
        "    y = self.self_attention_norm(x)\n",
        "    y = self.self_attention(y, y, y, self_mask)   #(q, k, v, mask)\n",
        "    y = self.self_attention_dropout(y)\n",
        "    x =  x + y   #skip connection\n",
        "\n",
        "    y = self.ffn_norm(x)\n",
        "    y = ffn(y)\n",
        "    y = self.ffn_dropout(y)\n",
        "    x = x + y   #skip connection\n",
        "    return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, hidden_size, filter_size, dropout_rate):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.self_attention_norm = nn.LayerNorm(hidden_size, eps = 1e-6)\n",
        "    self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
        "    self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    self.enc_dec_attention_norm = nn.LayerNorm(hidden_size, eps = 1e-6)\n",
        "    self.enc_dec_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
        "    self.enc_dec_attention_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    self.ffn_norm = nn.LayerNorm(hidden_size, eps = 1e-6)\n",
        "    self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
        "    self.ffn_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x, enc_output, self_mask, i_mask, cache):\n",
        "    y = self.self_attention_norm(x)\n",
        "    y = self.self_attention(y, y, y, self_mask)\n",
        "    y = self.self_attention_dropout(y)\n",
        "    x = x + y\n",
        "\n",
        "    if enc_output is not None:\n",
        "      y = self.self_attention_norm(x)\n",
        "      y = self.self_attention(y, enc_output, enc_output, i_mask, cache)\n",
        "      y = self.enc_dec_attention_dropout(y)\n",
        "      x = x + y\n",
        "\n",
        "    y = self.ffn_norm(x)\n",
        "    y = self.ffn(y)\n",
        "    y = self.ffn_dropout(y)\n",
        "    x = x + y\n",
        "    return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    encoders = [EncoderLayer(hidden_size, filter_size, dropout_rate) for _ in range(n_layers)]\n",
        "    self.layers = nn.ModuleList(encoders)\n",
        "\n",
        "    self.last_norm = nn.LayerNorm(gidden_size, eps = 1e-6)\n",
        "\n",
        "  def forward(self, inputs, mask):\n",
        "    encoder_output = inputs\n",
        "    for enc_layer in self.layers:\n",
        "      encoder_output = enc_layer(encoder_output, mask)\n",
        "    return self.last_norm(encoder_output)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    decoders = [DecoderLayer(hidden_size, filter_size, dropout_rate) for _ in range(n_layers)]\n",
        "    self.layers = nn.ModuleList(decoders)\n",
        "\n",
        "    self.last_norm = nn.LayerNorm(hidden_size, eps = 1e-6)\n",
        "\n",
        "  def forward(self, targets, enc_output, i_mask, t_self_mask, cache):\n",
        "    decoder_output = targets\n",
        "    for i, dec_layer in enumerate(self.layers):\n",
        "      layer_cache = None\n",
        "      if cache is not None:\n",
        "        if i not in cache:\n",
        "          cache[i] = {}\n",
        "        layer_cache = cache[i]\n",
        "      decoder_output = dec_layer(decoder_output, enc_output, t_self_mask, i_mask, layer_cache)\n",
        "\n",
        "    return self.last_norm(decoder_output)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, i_vocab_size, t_vocab_size, n_layers = 6, hidden_size = 512, \n",
        "               filter_size = 2048, dropout_rate = 0.1, share_target_embedding = True,\n",
        "               has_inputs = True, src_pad_idx = None, trg_pad_idx = None):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.emb_scale = hidden_size ** 0.5\n",
        "    self.has_inputs = has_inputs\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "\n",
        "    self.t_vocab_embedding = nn.Embedding(t_vocab_size, hidden_size)\n",
        "    nn.init.normal_(self.t_vocab_embedding.weight, mead = 0, std = hidden_size ** -0.5)\n",
        "    self.t_emb_dropout = nn.Dropout(dropout_rate)\n",
        "    self.decoder = Decoder(hidden_size, filter_size, dropout_rate, n_layers)\n",
        "\n",
        "    if has_inputs:\n",
        "      if not share_target_embedding:\n",
        "        self.i_vocab_embedding = nn.Embedding(i_vocab_size, hidden_size)\n",
        "        nn.init.normal_(self.i_vocab_embedding.weight, mean = 0, std = hidden_size ** -0.5)\n",
        "      else:\n",
        "        self.i_vocab_embedding = self.t_vocab_embedding\n",
        "\n",
        "      self.i_emb_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "      self.encoder = Encoder(hidden_size, filter_size, dropout_rate, n_layers)\n",
        "\n",
        "    #Positional Encoding\n",
        "    num_timescales = self.hidden_size // 2\n",
        "    max_timescale = 10000.0\n",
        "    min_timescale = 1.0\n",
        "    log_timescale_increment = (\n",
        "        math.log(floast(max_timescale) / float(min_timescale)) / \n",
        "        max(num_timescale - 1, 1))\n",
        "    inv_timescales = min_timescale * torch.exp(\n",
        "        torch.arange(num_timescales, dtype = torch.float32) * \n",
        "        -log_timescale_increment)\n",
        "    self.register_buffer('inv_timescales', inv_timescales)\n",
        "\n",
        "  def forward(self, inputs, targets):\n",
        "    enc_output, i_mask = None, None\n",
        "    if self.has_inputs:\n",
        "      i_mask = utils.create_pad_mask(inputs, self.src_pad_idx)\n",
        "      enc_output = self.encode(inputs, i_mask)\n",
        "\n",
        "    t_mask = utils.create_pad_mask(targets, self.trg_pad_idx)\n",
        "    target_size = targets.size()[1]\n",
        "    t_self_mask = utils.create_trg_self_mask(target_size, device = targets.device)\n",
        "\n",
        "    return self.decode(targets, enc_output, i_mask, t_self_mask, t_mask)\n",
        "\n",
        "  def encode(self, inputs, i_mask):\n",
        "    #Input Embedding\n",
        "    input_embedded = self.i_vocab_embedding(inputs)\n",
        "    input_embedded.masked_fill_(i_mask.squeeze(1).unaqueeze(-1), 0)\n",
        "    input_embedded *= self.emb_scale\n",
        "    input_embedded += self.get_position_encoding(inputs)\n",
        "    input_embedded = self.i_emb_dropout(input_embedded)\n",
        "\n",
        "    return self.encoder(input_embedded, i_mask)\n",
        "\n",
        "  def decoder(self, targets, enc_output, i_mask, t_self_mask, t_mask, cache = None):\n",
        "    #target embedding\n",
        "    target_embedded = self.t_vocab_embedding(targets)\n",
        "    target_embedded.masked_fill(t_mask.squeeze(1).unsqueeze(-1), 0)\n",
        "\n",
        "    #Shfting\n",
        "    target_embedded = target_embedded[:, :-1]\n",
        "    target_embedded = F.pad(target_embedded, (0, 0, 1, 0))\n",
        "\n",
        "    target_embedded *= self.emb_scale\n",
        "    target_embedded += self.get_position_encoding(targets)\n",
        "    target_embedded = self.t_emb_dropout(target_embedded)\n",
        "\n",
        "    #decoder\n",
        "    decoder_output = self.decoder(target_embedded, enc_output, i_mask, t_self_mask, cache)\n",
        "\n",
        "    #linear\n",
        "    output = torch.matmul(decoder_output, self.t_vocab_embedding.weight.transpose(0, 1))\n",
        "\n",
        "    return output\n",
        "\n",
        "  def get_position_encoding(self, x):\n",
        "    max_length = x.size()[1]\n",
        "    position = torch.arange(max_length, dtype = torch.float32, device = x.device)\n",
        "    scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)\n",
        "    signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim = 1)\n",
        "    signal = F.pad(signal, (0, 0, 0, self.hidden_size % 2))\n",
        "    signal = signal.view(1, max_length, self.hidden_size)\n",
        "    return signal"
      ]
    }
  ]
}